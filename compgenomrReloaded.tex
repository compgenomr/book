\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[12pt,]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmonofont[Mapping=tex-ansi,Scale=0.7]{Source Code Pro}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Computational Genomics With R},
            pdfauthor={Altuna Akalin},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\setmainfont[UprightFeatures={SmallCapsFont=AlegreyaSC-Regular}]{Alegreya}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Computational Genomics With R}
\author{Altuna Akalin}
\date{2019-02-19}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


The aim of this book is to provide the fundamentals for data analysis
for genomics. We developed this book based on the computational genomics
courses we are giving every year. We have had invariably an
interdisicplineary audience with backgrounds from physics, biology
medicine, math, computer science or other quantitative fields. We want
this book to be a starting point for computational genomics students and
a guide for further data analysis in more specific topics in genomics.
This is why we tried to cover a large variety of topics from programming
to basic genome biology. As the field is interdisciplineary, it requires
different starting points for people with different backgrounds. A
biologist might skip sections on basic genome biology and start with R
programming whereas a computer scientist might want to start with genome
biology. In the same manner, a more experienced person might want to
refer to this book when s/he needs a certain type of analysis where s/he
does not have prior experience.

\includegraphics{images/by-nc-sa.png}\\
The online version of this book is licensed under the
\href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative
Commons Attribution-NonCommercial-ShareAlike 4.0 International License}.
You can purchase a hardcopy from
\href{https://www.crcpress.com/product/isbn/}{Chapman \& Hall} or
Amazon.

\hypertarget{who-is-this-book-for}{%
\section*{Who is this book for?}\label{who-is-this-book-for}}


The book contains practical and theoretical aspects for computational
genomics. Biology and medicine generate more data than ever before and
we need to educate more people with data analysis skills and
understanding of computational genomics. Since computational genomics is
interdisciplinary; this book aims to be accessible for biologists,
medical scientists, computer scientists and people from other
quantitative backgrounds. We wrote this book for the following
audiences:

\begin{itemize}
\tightlist
\item
  Biologists and medical scientists who generate the data and are keen
  on analyzing it themselves.
\item
  Students and researchers who are formally starting to do research on
  or using computational genomics but do not have extensive domain
  specific knowledge but has at least a beginner level in a quantitative
  field: math, stats
\item
  Experienced researchers looking for recipes or quick how-tos to get
  started in specific data analysis tasks relating to computational
  genomics.
\end{itemize}

\hypertarget{what-will-you-get-out-of-this}{%
\subsection*{What will you get out of
this?}\label{what-will-you-get-out-of-this}}


This resource describes the skills and provides how-tos that will help
readers analyze their own genomics data.

After reading:

\begin{itemize}
\tightlist
\item
  If you are not familiar with R, you will get the basics of R and dive
  right in to specialized uses of R for computational genomics.
\item
  you will understand genomic intervals and operations on them, such as
  overlap
\item
  You will be able to use R and its vast package library to do sequence
  analysis: Such as calculating GC content for given segments of a
  genome or find transcription factor binding sites
\item
  You will be familiar with visualization techniques used in genomics,
  such as heatmaps,meta-gene plots and genomic track visualization
\item
  You will be familiar with supervised and unsupervised learning
  techniques which are important in data modelling and exploratory
  analysis of high-dimensional data
\item
  You will be familiar with analysis of different high-throughput
  sequencing data sets mostly using R based tools.
\end{itemize}

\hypertarget{structure-of-the-book}{%
\section*{Structure of the book}\label{structure-of-the-book}}


The book is designed with the idea that practical and conceptual
understanding of data analysis methods is as important, if not more
important, than the theoretical understanding, such as detailed
derivation of equations in statistics or machine-learning. That is why
we first try to give a conceptual explanation of the concepts then we
try to give essential parts of the mathematical formulas for more
detailed understanding. In this sprit, we always show the code and
explain the code for a particular data analysis task. In addition, we
give additional references such as books, websites and scientific papers
for readers who desire to gain deeper theoretical understanding of data
analysis related methods or concepts.

Introduction chapter introduce data analysis paradigms and what
computational genomics looks like in practice. ``Essentials for
genomics'' chapter introduces the basic concepts in genome biology and
genomics. Understanding these concepts are important for computational
genomics.

Chapters ``Introduction to R'' and ``Statistics and Exploratory Data
Analysis for Genomics'' introduce the necessary programming and
practical data analysis skillls for computational genomics. Related to
these, ``Operations on genomic intervals and genome arithmetic''
introduces fundemental tools for dealing with genomic intervals and
their relationship to eachother over the genome. These three chapters
composes the necessary technical toolkit for analysis of specific
high-throughput sequencing techniques.

Chapters ``RNA-seq analysis'', ``ChIP-seq analysis'' and ``BS-seq
analysis'' deals with the analysis techniques for popular
high-throughput sequencing techniques. The last chapter ``multi-omics
data integration'' deals with methods for integrating multiple omics
data sets.

To sum it up, this book is a comprehensive guide for computational
genomics. Some sections are there for the sake of the wide
interdisciplineary audience and completeness, and not all sections will
equally useful to all readers from this broad audience.

\hypertarget{software-information-and-conventions}{%
\section*{Software information and
conventions}\label{software-information-and-conventions}}


This book is primarily about the using R packages to analyze genomics
data, therefore if you want to reproduce the analysis in this book you
need to install the relevant packages in each chapter using
\texttt{install.packages} or \texttt{BiocManager::install} functions. We
rely on data from different R and Bioconductor packages through out the
book. For the datasets that do not ship with those packages, we created
our own package \textbf{compGenomRData}.

Package names are in bold text (e.g., \textbf{methylKit}), and inline
code and filenames are formatted in a typewriter font. Function names
are followed by parentheses (e.g., \texttt{genomation::ScoreMatrix()}).
The double-colon operator \texttt{::} means accessing an object from a
package.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}


We wish to thank R and Bioconductor community for developing and
maintaining libraries for genomic data analysis. Without their constant
work and dedication, writing such a book will not be possible.

\hypertarget{about-the-authors}{%
\chapter*{About the Authors}\label{about-the-authors}}


The authors have decades of combined experience in data analysis for
genomics. They are developers of Bioconductor packages such as
\textbf{methylKit}, \textbf{genomation}, \textbf{RCAS} and
\textbf{netSmooth}. In addition, they have played key roles in
developing end-to-end genomics data analysis pipelines for RNA-seq,
ChIP-seq, Bisulfite-seq, and single cell RNA-seq called PiGx.

\mainmatter

\hypertarget{intro}{%
\chapter{Introduction to Genomics}\label{intro}}

The aim of this chapter is to provide the reader with some of the
fundamentals required for understanding genome biology. By no means,
this is a complete overview of the subject but just a summary that will
help the non-biologist reader understand the recurring biological
concepts in computational genomics. Readers that are well-versed in
genome biology and modern genome-wide quantitative assays should feel
free to skip this chapter or skim it through.

\hypertarget{genes-dna-and-central-dogma}{%
\section{Genes, DNA and central
dogma}\label{genes-dna-and-central-dogma}}

A central concept that will come up again and again is ``the gene''.
Before we can explain that we need to introduce a few other concepts
that are important to understand the gene concept. Human body is made up
of billions of cells. These cells specialize in different tasks. For
example, in the liver there are cells that help produce enzymes to break
toxins. In the heart, there are specialized muscle cells that make the
heart beat. Yet, all these different kinds of cells come from a single
celled embryo. All the instructions to make different kinds of cells are
contained within that single cell and with every division of that cell,
those instructions are transmitted to new cells. These instructions can
be coded into a string - a molecule of DNA, a polymer made of recurring
units called nucleotides. The four nucleotides in DNA molecules,
Adenine, Guanine, Cytosine and Thymine (coded as four letters: A, C, G,
and T) in a specific sequence, store the information for life. DNA is
organized in a double-helix form where two complementary polymers
interlace with each other and twist into the familiar helical shape.

\hypertarget{what-is-a-genome}{%
\subsection{What is a genome?}\label{what-is-a-genome}}

The full DNA sequence of an organism, which contains all the hereditary
information, is called a genome. The genome contains all the information
to build and maintain an organism. Genomes come in different sizes and
structures. Our genome is not only a naked strech of DNA. In eukaryotic
cells, DNA is wrapped around proteins (histones) forming higher-order
structures like nucleosomes which make up chromatins and chromosomes
(see Figure \ref{fig:chromatinChr}).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/chromatinChr} 

}

\caption{Chromosome structure in animals}\label{fig:chromatinChr}
\end{figure}

There might be several chromosomes depending on the organism. However,
in some species (such as most prokaryotes) DNA is stored in a circular
form. The size of genome between species differs too. Human genome has
46 chromosomes and over 3 billion base-pairs, whereas wheat genome has
42 chromosomes and 17 billion base-pairs, both genome size and
chromosome numbers are variable between different organisms. Genome
sequences of organisms are obtained using sequencing technology. With
this technology, fragments of the DNA sequence from the genome, called
reads, are obtained. Larger chunks of the genome sequence is later
obtained by stitching the initial fragments to larger ones by using the
overlapping reads. Latest, sequencing technologies made genome
sequencing cheaper and faster. These technologies output more reads,
longer reads and more accurate reads.

Estimated cost of the first human genome is \$300 million in 1999-2000,
today a high-quality human genome can be obtained for \$1500. Since the
costs are going down, researchers and clinicians can generate more data.
This drives up to costs for data storage and also drives up the demand
for qualified people to analyze genomic data. This was one of the
motivations behind writing this book.

\hypertarget{what-is-a-gene}{%
\subsection{What is a gene?}\label{what-is-a-gene}}

In the genome, there are specific regions containing the precise
information that encodes for physical products of genetic information. A
region in the genome with this information is traditionally called a
``gene''. However, the precise definition of the gene is still
developing. According to the classical textbooks in molecular biology, a
gene is a segment of a DNA sequence corresponding to a single protein or
to a single catalytic and structural RNA molecule {[}1{]}. A modern
definition is: ``A region (or regions) that includes all of the sequence
elements necessary to encode a functional transcript'' {[}2{]}. No
matter how variable the definitions are, all agree on the fact that
genes are basic units of heredity in all living organisms.

All cells use their hereditary information in the same way most of the
time; the DNA is replicated to transfer the information to new cells. If
activated, the genes are transcribed into messenger RNAs (mRNAs) in
nucleus (in eukaryotes), followed by mRNAs (if the gene is protein
coding) getting translated into proteins in the cytoplasm. This is
essentially a process of information transfer between information
carrying polymers; DNA, RNA and proteins, known as the ``central dogma''
of molecular biology (see Figure \ref{fig:CentDog} for a summary).
Proteins are essential elements for life. The growth and repair,
functioning and structure of all living cells depends on them. This is
why the gene is a central concept in genome biology, because a gene can
encode information for proteins and other functional molecules. How
genes are controled and activated dictates everything about an organism.
From the identity of a cell to response to an infection, how cells
develop and behave against certain stimuli is governed by activity of
the genes and functional molecules they encode. The liver cell becomes a
liver cell because certain genes are activated and their functional
products are produced to help liver cell achieve its tasks.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/CentDogma} 

}

\caption{Central Dogma: replication, transcription, translation}\label{fig:CentDog}
\end{figure}

\hypertarget{how-genes-are-controlled-the-transcriptional-and-the-post-transcriptional-regulation}{%
\subsection{How genes are controlled ? The transcriptional and the
post-transcriptional
regulation}\label{how-genes-are-controlled-the-transcriptional-and-the-post-transcriptional-regulation}}

In order to answer this question, we have to dig a little deeper on the
transcription concept we introduced via the central dogma. The first
step in a process of information transfer - a production of an RNA copy
of a part of the DNA sequence - is called transcription. This task is
carried out by the RNA polymerase enzyme. RNA polymerase-dependent
initiation of transcription is enabled by the existence of a specific
region in the sequence of DNA - a core promoter. Core promoters are
regions of DNA that promote transcription and are found upstream from
the start site of transcription. In eukaryotes, several proteins, called
general transcription factors recognize and bind to core promoters and
form a pre-initiation complex. RNA polymerases recognize these complexes
and initiate synthesis of RNAs, the polymerase travels along the
template DNA and making an RNA copy{[}3{]}. After mRNA is produced it is
often spliced by splicesosome. The sections called `introns' are removed
and sections called `exons' left in. Then, the remaining mRNA translated
into proteins. Which exons will be part of the final mature transcript
can also be regulated and creates diversity in protein structure and
function (See Figure \ref{fig:TransSplice}`).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/TransSplice} 

}

\caption{Transcription could be followed by splicing, which creates different transcript isoforms. This will in return create different protein isoforms since the information required to produce the protein is encoded in the transcripts. Differences in transcript of the same gene can give rise to different protein isoforms}\label{fig:TransSplice}
\end{figure}

On the contrary to protein coding genes, non-coding RNA (ncRNAs) genes
are processed and assume their functional structures after transcription
and without going into translation, hence the name: non-coding RNAs.
certain ncRNAs can also be spliced but still not translated. ncRNAs and
other RNAs in general can form complementary base-pairs within the RNA
molecule which gives them additional complexity. This
self-complementarity based structure, termed RNA secondary structure, is
often necessary for functions of many ncRNA species.

In summary, the set of processes, from transcription initiation to
production of the functional product, is referred to as gene expression.
Gene expression quantification and regulation is a fundamental topic in
genome biology.

\hypertarget{what-does-a-gene-look-like}{%
\subsection{What does a gene look
like?}\label{what-does-a-gene-look-like}}

Before we move forward, it will be good to discuss how we can visualize
genes. As someone interested in computational genomics, you will
frequently encounter a gene on a computer screen, and how it is
represented on the computer will be equivalent to what you imagine when
you hear the word ``gene''. In the online databases, the genes will
appear as a sequence of letters or as a series of connected boxes
showing exon-intron structure which may include the direction of
transcription as well (see Figure \ref{fig:RealGene}). direction. You
will encounter more with the latter so this is likely what will pop into
your mind when you think of genes.

As we have mentioned DNA has two strands, and a gene can be located on
either of them, and direction of transcription will depend on that. In
the Figure you can see arrows on introns (lines connecting boxes)
indicating the direction of the gene.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/RealGene} 

}

\caption{A) Representation of a gene at UCSC browser. Boxes indicate exons, and lines indicate introns. B) Partial sequence of FATE1 gene as shown in NCBI GenBank database.}\label{fig:RealGene}
\end{figure}

\hypertarget{elements-of-gene-regulation}{%
\section{Elements of gene
regulation}\label{elements-of-gene-regulation}}

The mechanisms regulating gene expression are essential for all living
organisms as they dictate where and how much of a gene product (may it
be protein or ncRNA) should be manufactured. This regulation could occur
at the pre- and co-transcriptional level by controlling how many
transcripts should be produced and/or which version of the transcript
should be produced by regulating splicing. Different versions of the
same gene could encode for proteins by regulating splicing the process
can decide which parts will go into the final mRNA that will code for
the protein. In addition, gene products can be regulated
post-transcriptionally where certain molecules bind to RNA and mark them
for degradation even before they can be used in protein production.

Gene regulation drives cellular differentiation; a process during which
different tissues and cell types are produced. It also helps cells
maintain differentiated states of cells/tissues. As a product of this
process, at the final stage of differentiation, different kinds of cells
maintain different expression profiles although they contain the same
genetic material. As mentioned above there are two main types of
regulation and next we will provide information on those.

\hypertarget{transcriptional-regulation}{%
\subsection{Transcriptional
regulation}\label{transcriptional-regulation}}

The rate of transcription initiation is the primary regulatory element
in gene expression regulation. The rate is controlled by core promoter
elements as well as distant-acting regulatory elements such as
enhancers. On top of that, processes like histone modifications and/or
DNA methylation have a crucial regulatory impact on transcription. If a
region is not accessible for the transcriptional machinery, e.g.~in the
case when chromatin structure is compacted due to the presence of
specific histone modifications, or if the promoter DNA is methylated,
transcription may not start at all. Last but the not least, gene
activity is also controlled post-transcriptionally by ncRNAs such as
microRNAs (miRNAs), as well as by cell signaling resulting in protein
modification or altered protein-protein interactions.

\hypertarget{regulation-by-transcription-factors-through-regulatory-regions}{%
\subsubsection{Regulation by transcription factors through regulatory
regions}\label{regulation-by-transcription-factors-through-regulatory-regions}}

Transcripton factors are proteins that recognize a specific DNA motif to
bind on a regulatory region and regulate the transcription rate of the
gene associated with that regulatory region (See Figure
\ref{fig:regSummary})` for an illustration). These factors bind to a
variety of regulatory regions summarized in Figure \ref{fig:regSummary},
and their concerted action controls the transcription rate. Apart from
their binding preference, their concentration, the availability of
synergistic or competing transcription factors will also affect the
transcription rate.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/regulationSummary} 

}

\caption{Representation of regulatory regions in animal genomes}\label{fig:regSummary}
\end{figure}

\hypertarget{core-and-proximal-promoters}{%
\paragraph{Core and proximal
promoters}\label{core-and-proximal-promoters}}

Core promoters are the immediate neighboring regions around the
transcription start site (TSS) that serves as a docking site for the
transcriptional machinery and pre-initiation complex (PIC) assembly. The
textbook model for transcription initiation is as follows: The core
promoter has a TATA motif (referred as TATA-box) 30 bp upstream of an
initiator sequence (Inr), which also contains TSS. Firstly,
transcription factor TFIID binds to the TATA-box. Next, general
transcription factors are recruited and transcription is initiated on
the initiator sequence. Apart from the TATA-box and Inr, there are a
number of sequence elements on the animal core promoters that are
associated with transcription initiation and PIC assembly, such as
downstream promoter elements (DPEs), the BRE elements and CpG islands.
DPEs are found 28-32 bp downstream of the TSS in TATA-less promoters of
Drosophila melanogaster, it generally co-occurs with the Inr element,
and is thought to have a similar function to the TATA-box. The BRE
element is recognized by TFIIB protein and lies upstream of the
TATA-box. CpG islands are CG dinucleotide-enriched segments of
vertebrate genomes, despite the general depletion of CG dinucleotides in
those genomes. 50-70\% of promoters in human genome are associated with
CpG islands.

Proximal promoter elements are typically right upstream of the core
promoters and usually contain binding sites for activator transcription
factors and they provide additional control over gene expression.

\hypertarget{enhancers}{%
\paragraph{Enhancers:}\label{enhancers}}

Proximal regulation is not the only, nor the most important mode of gene
regulation. Most of the transcription factor binding sites in the human
genome are found in intergenic regions or in introns . This indicates
the widespread usage of distal regulatory elements in animal genomes. On
a molecular function level, enhancers are similar to proximal promoters;
they contain binding sites for the same transcriptional activators and
they basically enhance the gene expression. However, they are often
highly modular and several of them can affect the same promoter at the
same time or in different time-points or tissues. In addition, their
activity is independent of their orientation and their distance to the
promoter they interact with. A number of studies showed that enhancers
can act upon their target genes over several kilobases away. According
to a popular model, enhancers achieve this by looping the DNA and coming
to contact with their target genes.

\hypertarget{silencers}{%
\paragraph{Silencers:}\label{silencers}}

Silencers are similar to enhancers; however their effect is opposite of
enhancers on the transcription of the target gene, and results in
decreasing their level of transcription. They contain binding sites for
repressive transcription factors. Repressor transcription factors can
either block the binding of an activator , directly compete for the same
binding site, or induce a repressive chromatin state in which no
activator binding is possible. Silencer effects, similar to those of
enhancers, are independent of orientation and distance to target genes.
In contradiction to this general view, in Drosophila there are two types
of silencers, long-range and short-range. Short-range silencers are
close to promoters and long-range silencers can silence multiple
promoters or enhancers over kilobases away. Like enhancers, silencers
bound by repressors may also induce changes in DNA structure by looping
and creating higher order structures. One class of such repressor
proteins, which is thought to initiate higher-order structures by
looping, is Polycomb group proteins (PcGs).

\hypertarget{insulators}{%
\paragraph{Insulators:}\label{insulators}}

Insulator regions limit the effect of other regulatory elements to
certain chromosomal boundaries; in other words, they create regulatory
domains untainted by the regulatory elements in regions outside that
domain. Insulators can block enhancer-promoter communication and/or
prevent spreading of repressive chromatin domains. In vertebrates and
insects, some of the well-studied insulators are bound by CTCF
(CCCTC-binding factor). Genome-wide studies from different mammalian
tissues confirm that CTCF binding is largely invariant of cell type, and
CTCF motif locations are conserved in vertebrates. At present, there are
two models of explaining the insulator function; the most prevalent
model claims insulators create physically separate domains by modifying
chromosome structure. This is thought to be achieved by CTCF-driven
chromatin looping and recent evidence shows that CTCF can induce a
higher-order chromosome structure through creating loops of chromatins.
According to the second model, an insulator-bound activator cannot bind
an enhancer; thus enhancer-blocking activity is achieved and insulators
can also recruit active histone domain, creating an active domain for
enhancers to function.

\hypertarget{locus-control-regions}{%
\paragraph{Locus control regions:}\label{locus-control-regions}}

Locus control regions (LCRs) are clusters of different regulatory
elements that control entire set of genes on a locus. LCRs help genes
achieve their temporal and/or tissue-specific expression programs. LCRs
may be composed of multiple cis-regulatory elements, such as insulators,
enhancers and they act upon their targets even from long distances.
However LCRs function with an orientation dependent manner, for example
the activity of beta-globin LCR is lost if inverted. The mechanism of
LCR function otherwise seems similar to other long-range regulators
described above. The evidence is mounting in the direction of a model
where DNA-looping creates a chromosomal structure in which target genes
are clustered together, which seems to be essential for maintaining open
chromatin domain.

\hypertarget{epigenetic-regulation}{%
\subsubsection{Epigenetic regulation}\label{epigenetic-regulation}}

Epigenetics in biology usually refers to constructions (chromatin
structure, DNA methylation etc.) other than DNA sequence that influence
gene regulation. In essence, epigenetic regulation is the regulation of
DNA packing and structure, the consequence of which is gene expression
regulation. A typical example is that DNA packing inside the nucleus can
directly influence gene expression by creating accessible regions for
transcription factors to bind. There are two main mechanisms in
epigenetic regulation: i) DNA modifications ii) histone modifications.
Below, we will introduce these two mechanisms.

\hypertarget{dna-modifications-such-as-methylation}{%
\paragraph{DNA modifications such as
methylation:}\label{dna-modifications-such-as-methylation}}

DNA methylation is usually associated with gene silencing. DNA
methyltransferase enzyme catalyzes the addition of a methyl group to
cytosine of CpG dinucleotides (while in mammals the addition of methyl
group is largely restricted to CpG dinucleotides, methylation can occur
in other bases as well) . This covalent modification either interferes
with transcription factor binding on the region, or methyl-CpG binding
proteins induce the spread of repressive chromatin domains, thus the
gene is silenced if its promoter has methylated CG dinucleotides. DNA
methylation usually occurs in repeat sequences to repress transposable
elements, these elements when active can jump around and insert them to
random parts of the genome, potentially distrupting the genomic
functions.

.DNA methylation is also related to a key core and proximal promoter
element: CpG islands. CpG islands are usually unmethylated, however for
some genes CpG island methylation accompanies their silenced expression.
For example, during X-chromosome inactivation many CpG islands are
heavily methylated and the associated genes are silenced. In addition,
in embryonic stem cell differentiation pluripotency-associated genes are
silenced due to DNA methylation. Apart from methylation, there are other
kinds of DNA modifications present in mamalian genomes, such as
hydroxy-methylation and formylcytosine. These are other modifications
under current research that are either intermediate or stable
modifications with distinct functional associations. There are as many
as 12 distinct DNA modifications observed when we look accross all
studied species.

\hypertarget{histone-modifications}{%
\paragraph{Histone Modifications:}\label{histone-modifications}}

Histones are proteins that constitute nucleosome. In eukaryotes, eight
histones nucleosomes are wrapped around by DNA and build nucleosome.
They help super-coiling of DNA and inducing high-order structure called
chromatin. In chromatin, DNA is either densely packed (called
heterochromatin or closed chromatin), or it is loosely packed (called
euchromatin or open chromatin) {[}60, 61{]}. Heterochromatin is thought
to harbor inactive genes since DNA is densely packed and transcriptional
machinery cannot access it. On the other hand, euchromatin is more
accessible for transcriptional machinery and might therefore harbor
active genes. Histones have long and unstructured N-terminal tails which
can be covalently modified. The most studied modifications include
acetylation, methylation and phosphorylation {[}60{]}. Using their
tails, histones interact with neighboring nucleosomes and the
modifications on the tail affect the nucleosomes affinity to bind DNA
and therefore influence DNA packaging around nucleosomes. Different
modifications on histones are used in different combinations to program
the activity of the genes during differentiation. Histone modifications
have a distinct nomenclature, for example: H3K4me3 means the lysine (K)
on the 4th position of histone H3 is tri-methylated.

\begin{longtable}[]{@{}ll@{}}
\caption{Table 1 Histone modifications and their effects. If more than
one histone modification has the same effect, they are separated by
commas.}\tabularnewline
\toprule
Modifications & Effect\tabularnewline
\midrule
\endfirsthead
\toprule
Modifications & Effect\tabularnewline
\midrule
\endhead
H3K9ac & Active promoters and enhancers\tabularnewline
H3K14ac & Active transcription\tabularnewline
H3K4me3/me2/me1 Active p & romoters and enhancers, H3K4me1 and H3K27ac
is enhancer-specific\tabularnewline
H3K27ac & H3K27ac is enhancer-specific\tabularnewline
H3K36me3 & Active transcribed regions\tabularnewline
H3K27me3/me2/me1 & Silent promoters\tabularnewline
H3K9me3/me2/me1 & Silent promoters\tabularnewline
\bottomrule
\end{longtable}

Histone modifications are associated with a number of different
transcription-related conditions; some of them are summarized in Table
1. Histone modifications can indicate where the regulatory regions are
and they can also indicate activity of the genes. From a gene regulatory
perspective, maybe the most important modifications are the ones
associated with enhancers and promoters. Certain genes in mouse
embryonic stem cells have both active H3K4me3 and inactive H3K27me3
modifications in their promoters. Surprisingly, most of these genes have
high CpG content, are important for development and are shown to have
paused RNA polymerase II {[}66, 67{]}. In addition, Heintzman et
al.~showed that H3K4me1 could predict tissue-specific active enhancers
in human cells {[}68, 69{]}. The examples above demonstrate the
capability of histone modifications in predicting regulatory potential.

Furthermore, certain proteins can influence chromatin structure by
interacting with histones. Some of these proteins, like those of the
Polycomb Group (PcG) and CTCF, are discussed above in the insulators and
silencer sections. In vertebrates and insects, PcGs are responsible for
maintaining the silent state of developmental genes, and trithorax group
proteins (trxG) for maintaining their active state {[}70, 71{]}. PcGs
and trxGs induce repressed or active states by catalyzing histone
modifications or DNA methylation. Both the proteins bind PREs that can
be on promoters or several kilobases away {[}30, 31, 71{]}. Another
protein that induces histone modifications is CTCF. In b-globin locus,
CTCF binding is shown to be associated with repressive H3K9/K27me2
modifications {[}72{]}.

\BeginKnitrBlock{rmdtip}
\textbf{Want to know more ?}

\begin{itemize}
\item
  Transcriptional regulatory elements in the human genome:
  \url{http://www.ncbi.nlm.nih.gov/pubmed/16719718}
\item
  On metazoan promoters: types and transcriptional properties:
  \url{http://www.ncbi.nlm.nih.gov/pubmed/22392219}
\item
  General principles of regulatory sequence function
  \url{http://www.nature.com/nrg/journal/v15/n7/abs/nrg3684.html}
\item
  DNA methylation: roles in mammalian development
  \url{http://www.nature.com/doifinder/10.1038/nrg3354}
\item
  Histone modifications and organization of the genome
  \url{http://www.nature.com/nrg/journal/v12/n1/full/nrg2905.html}
\item
  DNA methylation and histone modifications are linked
  \url{http://www.nature.com/nrg/journal/v10/n5/abs/nrg2540.html}
\end{itemize}
\EndKnitrBlock{rmdtip}

\hypertarget{post-transcriptional-regulation}{%
\subsection{Post-transcriptional
regulation}\label{post-transcriptional-regulation}}

\hypertarget{regulation-by-non-coding-rnas}{%
\subsubsection{Regulation by non-coding
RNAs}\label{regulation-by-non-coding-rnas}}

Recent years have witnessed an explosion in noncoding RNA
(ncRNA)-related research. Many publications implicated ncRNAs as
important regulatory elements. Plants and animals produce many different
types of ncRNAs such as long non-coding RNAs (lncRNAs),
small-interferring RNAs (siRNAs), microRNAs (miRNAs),
promoter-associated RNAs (PARs) and small nucleolar RNAs (snoRNAs).
lncRNAs are typically \textgreater{}200 bp long, they are involved in
epigenetic regulation by interacting with chromatin remodeling factors
and they function in gene regulation. siRNAs are short double-stranded
RNAs which are involved in gene-regulation and transposon control, they
silence their target genes by cooperating with Argonaute proteins .
miRNAs are short single-stranded RNA molecules that interact with their
target genes by using their complementary sequence and mark them for
quicker degradation. PARs may regulate gene expression as well: they are
\textasciitilde{}18-200bp long ncRNAs originating from promoters of
coding genes. snoRNAs also shown to play roles in gene regulation,
although they are mostly believed to guide ribosomal RNA modifications.

\hypertarget{splicing-regulation}{%
\subsubsection{Splicing regulation}\label{splicing-regulation}}

Splicing is regulated by regulatory elements on the pre-mRNA and
proteins binding to those elements . Regulatory elements are categorized
as splicing enhancers and repressors. They can be located either in
exons or introns. Depending of their activity and their locations there
are four types of regulatory elements: - exonic splicing enhancers
(ESEs) - exonic splicing silencers (ESSs) - intronic splicing enhancers
(ISEs) - intronic splicing silencers (ISSs).

The majority of splicing repressors are heterogeneous nuclear
ribonucleoproteins (hnRNPs). If splicing repressor protein bind silencer
elements they reduce the change of nearby site to be used as splice
junction. On the contrary, splicing enhancers are sites to which
splicing activator proteins bind and binding on that region increases
the probability that a nearby site will be used as a splice junction.
Most of the activator proteins that bind to splicing enhancers are
members of the SR protein family. Such proteins can recognize specific
RNA recognition motifs. By regulating splicing exons can be skipped or
included which creates protein diversity.

\BeginKnitrBlock{rmdtip}
\textbf{Want to know more ?}

\begin{itemize}
\item
  On miRNAs: Their genesis and modes of regulation
  \url{http://www.sciencedirect.com/science/article/pii/S0092867404000455}
\item
  Functions of small RNAs
  \url{http://www.nature.com/nrg/journal/v15/n9/abs/nrg3765.html}
\item
  Functions of non coding RNAs
  \url{http://www.nature.com/nrg/journal/v15/n6/abs/nrg3722.html}
\item
  on splicing: Wang, Zefeng; Christopher B. Burge (May 2008). ``Splicing
  regulation: From a parts list of regulatory elements to an integrated
  splicing code''. RNA 14 (5): 802--813. \url{doi:10.1261/rna.876308}.
  ISSN 1355-8382. PMC 2327353. PMID 18369186. Retrieved 2013-08-15.
\end{itemize}
\EndKnitrBlock{rmdtip}

\hypertarget{shaping-the-genome-dna-mutation}{%
\section{Shaping the genome: DNA
mutation}\label{shaping-the-genome-dna-mutation}}

Human and chimpanzee genomes are 98.8\% similar. The 1.2\% difference is
what separetes us from chimpanzees. The further you move away from human
in terms of evolutionary distance the higher the difference gets.
However, even between the members of the same species differences in
genome sequences exists. These differences are due to a process called
mutation which drives differences between individuals but also the
provides the fuel for evolution as the source of the genetic variation.
Individuals with beneficial mutations can adapt to their surroundings
better than others and in time these mutations which are beneficial for
survival spreads in the population due to a process called ``natural
selection''. Selection acts upon individuals with beneficial features
which gives them an edge for survival in a given environment. Genetic
variation created by the mutations in individuals provide the material
on which selection can act upon. If the selection process goes for a
long time in a relatively isolated environment that requires adaptation,
this population can evolve into a different species given enough
time.This is the basic idea behind evolution in a nutshell, and without
mutations providing the genetic variation there will be no evolution.

Mutations in the genome occur due to multiple reasons. First, DNA
replication is not an error-free process. Before a cell division, the
DNA is replicated with 1 mistake per 10\^{}8 to 10\^{}10 base-pairs.
Second, mutagens such as UV light can induce mutations on the genome.
Third factor that contributes to mutation is imperfect DNA repair. Every
day any human cell suffers multiple instances DNA damage. DNA repair
enzymes are there to cope with this damage but they are also not
error-free, depending on which DNA repair mechanism is used (there are
multiple) mistakes will be made at varying rates.

Mutations are classified by how many bases they effect, their effect on
DNA structure and gene function. By their effect on DNA structure the
mutations are classified as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Base substitution}: A base is changed with another.
\item
  \textbf{Deletion}: One or more bases is deleted.
\item
  \textbf{Insertion}: New base or bases inserted into the genome.
\item
  \textbf{Microsatellite mutation}: Small insertions or deletions of
  small tandemly repeating DNA segments.
\item
  \textbf{Inversion}: A DNA fragment changes its orientation 180
  degrees.
\item
  \textbf{Translocation}: A DNA fragment moves to another location in
  the genome.
\end{itemize}

Mutations can also be classified by their size as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Point mutations}: mutations that involve one base.
  Substitutions, deletions and insertions are point mutations. They are
  also termed as single nucleotide polymorphisms (\textbf{SNPs}).
\item
  \textbf{small-scale mutations}: mutations that involve several bases.
\item
  \textbf{Large-scale mutations}: mutations which involve larger
  chromosomal regions. Transposable element insertions (where a segment
  of the genome jumps to another region in the genome) and segmental
  duplications ( a large region is copied multiple times in tandem) are
  typical large scale mutations.
\item
  \textbf{Aneuploidies}: Insertions or deletions of whole chromosomes.
\item
  \textbf{Whole-genome polyploidies}: duplications involving whole
  genome.
\end{itemize}

Mutations by their effect on gene function can be classified as follows:

\begin{itemize}
\tightlist
\item
  \textbf{gain-of-function mutations}: A type of mutation in which the
  altered gene product possesses a new molecular function or a new
  pattern of gene expression.
\item
  \textbf{loss-of-function mutations}: A mutation that results in
  reduced or abolished protein function. This is the more common type of
  mutation.
\end{itemize}

\hypertarget{high-throughput-experimental-methods-in-genomics}{%
\section{High-throughput experimental methods in
genomics}\label{high-throughput-experimental-methods-in-genomics}}

Most of the biological phenomena described above relating to
transcription , gene regulation or DNA mutation can be measured over the
entire genome using high-throughput experimental techniques, which are
quickly becoming the standard for studying genome biology. In addition,
their applications in the clinic are also gaining momemntum: there are
already diagnostic tests that are based on these techniques.

Some of the things that can be measured by high-throughput assays are as
follows:

\begin{itemize}
\tightlist
\item
  \emph{Which genes are expressed and how much ?}
\item
  \emph{Where does a transcription factor bind ?}
\item
  \emph{Which bases are methylated in the genome ?}
\item
  \emph{Which transcripts are translated ?}
\item
  \emph{Where does RNA-binding proteins bind ?}
\item
  \emph{Which microRNAs are expressed ?}
\item
  \emph{Which parts of the genome are in contact with each other ?}
\item
  \emph{Where are the mutations in the genome located ?}
\item
  \emph{Which parts of the genome are nucleosome-free ?}
\end{itemize}

There are many more questions one can answer using modern genome-wide
techniques and every other day a new variant of the existing techniques
comes along to answer a new question. However, One has to keep in mind
that these methods are at varying degrees of maturity and they all come
with technical limitations and are not noise-free. Despite this, they
are extremely useful for research and clinical purposes. And, thanks to
these methods we are able to sequence and annotate genomes at a massive
scale.

\hypertarget{the-general-idea-behind-high-throughput-techniques}{%
\subsection{The general idea behind high-throughput
techniques}\label{the-general-idea-behind-high-throughput-techniques}}

High-throughput methods aim to quantify or locate all or most of the
genome that harbours the biological feature (expressed genes, binding
sites, etc.) of interest. Most of the methods rely on some sort of
enrichment of the the targeted biological feature. For example, if you
want to measure expression of protein coding genes you need to be able
to extract mRNA molecules with special post-transcriptional alterations
that protein-coding genes acquire. If you are looking for transcription
factor binding, you need to enrich for the DNA fragments that are bound
by the protein if interest.This parts depends on available molecular
biology and chemistry technques, and the final product of this part is
RNA or DNA fragments.

Next, you need to be able to tell where these fragments are coming from
in the genome and how many of them are there. Microarrays were the
standard tool for the quantification step until spread of sequencing
techniques. In microarrays, one had to design complementary bases
,called ``oligos'' or ``probes'', to the genetic material enriched via
the experimental protocol. If the enriched material is complementary to
the genetic material, a light signal will be produced and the intensity
of the signal will be proportional to the amount of the genetic material
pairing with that oligo. There will be more probes available for
hybrdization (process of complementary bases forming bonds ), so the
more fragments available stronger the signal. For this to be able to
work, you need to know at least part of your genome sequence, and design
probes. If you want to measure gene expression, your probes should
overlap with genes and should be unique enough to not to bind sequences
from other genes. This technology is now being replaced with sequencing
technology, where you directly sequence your genetic material. If you
have the sequence of your fragments, you can align them back to genome,
see where they are coming from and count them. This is a better
technology where the quantification is based on the real identiy of
fragments rather than based on hybridization to designed probes.

In summary HT techniques has the following steps, and this also
summarized in Figure \ref{fig:HTassays}:

\begin{itemize}
\tightlist
\item
  Extraction: This is the step where you extract the genetic material of
  interest, RNA or DNA. and nrichment
\item
  Enrichment: In this step, you enrich for the event you are interested
  in. For example, protein binding sites. In some cases such as
  whole-genome DNA sequencing there is no need for enrichment step. You
  just get fragments of genomic DNA and sequence them.
\item
  Quantification: This is where you quantify your enriched material.
  Depending on the protocol you may need to quantify a control set as
  well, where you should see no enrichment or only background
  enrichment.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/HTassays} 

}

\caption{Common steps of High-throughput assays in genome biology}\label{fig:HTassays}
\end{figure}

\hypertarget{high-throughput-sequencing}{%
\subsection{High-throughput
sequencing}\label{high-throughput-sequencing}}

High-throughput sequencing, or massively parallel sequencing, is a
collection of methods and technologies that can sequence DNA
thousands/millons of fragments at a time. This is in contrast to older
technologies that can produce a limited number of fragments at a time.
Here, throughput refers to number of sequenced bases per hour. The older
low-throughput sequencing methods have \textasciitilde{} 100 times less
throughput compared to modern high-throughput methods. The increased
throughput gives the ability to measure biological features on a
genome-wide scale in a shorter time frame.

Similar to other high-throughput methods, sequencing based methods also
require an enrichment step. This step enriches for the features we are
interested in. The main difference of the sequencing based methods is
the quantification step. In high-throughput sequencing, enriched
fragments are put through the sequencer which outputs the sequences for
the fragments. Due to limitations in current leading technologies, only
limited number of bases can be sequenced using from the input fragments.
However, the length is usually enough to uniquely map the reads to the
genome and quantify the input fragments.

\hypertarget{high-throughput-sequencing-data}{%
\subsubsection{High-throughput sequencing
data}\label{high-throughput-sequencing-data}}

If there is a genome available, the reads are aligned to the genome and
based on the sequencing protocol different strategies are applied for
analysis. Some of the potential analysis strategies and processed output
of read alignments are depicted in Figure \ref{fig:HTseq}. For example,
we maybe interested to quantify the gene expression. The experimental
protocol, called RNA sequencing- RNA-seq, enriches for fragments of RNA
that are coming from protein coding genes. Upon alignment, we can
calculate the coverage profile which gives us a read count per base
along the genome. This information can be stored in a text file or
specialized file formats to be used in subsequent analysis or
visualization. We can also just count how many reads overlap with exons
of each gene and record read counts per gene for further analysis. This
essentially produces a table with gene names and read counts for
different samples. As we will see in later chapters, this is an
essential information for statistical models that model RNA-seq data.
Furthermore, we can stack up the reads and count how many times we see a
base position in a read mismatches the base in the genome. Read aligners
allow for mismatches, and for this reason we can see reads with
mismatches. This information can be used to identify SNPs, and can be
stored again in a tabular format with the information of position and
mismatch type and number of reads supporting the mismatch. The original
algorithms are a bit more complicated than just counting mismatches but
the general idea is the same, what they are doing differently is trying
to minimize false positive rates by using filters, so that not every
mismatch is recorded as SNP.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/HTseq} 

}

\caption{High-throughput sequencing summary}\label{fig:HTseq}
\end{figure}

\hypertarget{future-of-high-throughput-sequencing}{%
\subsubsection{Future of high-throughput
sequencing}\label{future-of-high-throughput-sequencing}}

The sequencing tech is still evolving. Next frontier is the obtaining
longer single-molecule reads and preferably being able to call base
modifications on the fly. With longer reads, the genome asssembly will
be easier for the regions that have high repeat content. With
single-molecule sequencing, we will be able tell how many transcripts
are present in a given cell population without relying on fragment
amplification methods which can introduce biases.

Another recent development is single-cell sequencing. Current
technologies usually work on genetic material from thousands to millions
of cells. This means that the results you receive represents the
population of cells that were used in the experiment. However, there is
a lot variation between the same type of cells, this variation is not
observed at all. Newer sequecing techniques can work on single cells and
give quantititave information on each cell.

\BeginKnitrBlock{rmdtip}
\textbf{Want to know more ?}

\begin{itemize}
\item
  Current and the future high-throughput sequencing technologies
  \url{http://www.sciencedirect.com/science/article/pii/S1097276515003408}
\item
  Illumina repository for different library preperation protocols for
  sequencing
  \url{http://www.illumina.com/techniques/sequencing/ngs-library-prep/library-prep-methods.html}
\end{itemize}
\EndKnitrBlock{rmdtip}

\hypertarget{visualization-and-data-repositories-for-genomics}{%
\section{Visualization and data repositories for
genomics}\label{visualization-and-data-repositories-for-genomics}}

There are \textasciitilde{}100 animal genomes sequenced as of 2016. On
top these, there are many research projects from either individual labs
or consortiums that produce petabytes of auxiliary genomics data, such
as ChIP-seq, RNA-seq, etc.

There are two requirements for to be able to visualize genomes and its
associated data, 1) you need to be able to work with a species that has
a sequenced genome and 2) you want to have annotation on that genome,
meaning at the very least you want to know where the genes are. Most
genomes after sequencing quickly annotated with gene-predictions or know
gene sequences are mapped on to it, you can also have conservation to
other species to filter functional elements. If you are working with a
model organism or human you will also have a lot of auxiliary
information to help demarcate the functional regions such as regulatory
regions, ncRNA, SNPs that are common in the population. Or you might
have disease or tissue specific data available. . The more the organism
is worked on the more auxiliary data you will have.

\hypertarget{accessing-genome-sequences-and-annotations-via-genome-browsers}{%
\subsubsection{Accessing genome sequences and annotations via genome
browsers}\label{accessing-genome-sequences-and-annotations-via-genome-browsers}}

As someone intends to work with genomics, you will need to visualize a
large amount of data to make biological inferences or simply check
regions of interest in the genome visually. Looking at the genome case
by case with all the additional datasets is a necessary step to develop
hypothesis and understand the data.

Many genomes and associated data is available through genome browsers. A
genome browser is a website or an app that helps you visualize the
genome and all the available data associated with it. Via genome
browsers, you will be able to see where genes are in relation to each
other and other functional elements. You will be able to see gene
structure. You will be able to see auxiliary data such as conservation,
repeat content and SNPs. Here we review some of the popular browsers.

\textbf{UCSC genome browser:} This is an online browser hosted by
University of California, Santa Cruz at \url{http://genome.ucsc.edu/}.
This is an interactive website that contains genomes and annotations for
many species. You can search for genes or genome coordinates for the
species of your interest. It is usually very responsive and allows you
to visualize large amounts of data. In addition, it has multiple other
tools that can be used in connection with the browser. One of the most
useful tool is \emph{UCSC Table Browser}, which lets you download the
all the data you see on the browser, including sequence data, in
multiple formats . Users can upload data or provide links to the data to
visualize user specific data.

\textbf{Ensembl:} This is another online browser maintained by\\
European Bioinformatics Institute and the Wellcome Trust Sanger
Institute in the UK, \url{http://www.ensembl.org}. Similar to UCSC
browser, users can visualize genes or genomic coordinates from multiple
species and it also comes with auxiliary data. Ensemlb is associated
with \emph{Biomart} tool which is similar to UCSC Table browser, can
download genome data including all the auxiliary data set in multiple
formats.

\textbf{IGV:} Integrated genomics viewer (IGV) is a desktop application
developed by Broad institute
(\url{https://www.broadinstitute.org/igv/}). It is developed to deal
with large amounts of high-throughput sequencing data which is harder to
view in online browsers. IGV can integrate your local sequencing results
with online annotation on your desktop machine. This is useful when
viewing sequencing data, especially alignments. Other browsers mentioned
above have similar functionalities however you will need to make your
large sequencing data available online somewhere before it can be viewed
by browsers.

\hypertarget{data-repositories-for-high-throughput-assays}{%
\subsubsection{Data repositories for high-throughput
assays}\label{data-repositories-for-high-throughput-assays}}

Genome browser contain lots of auxiliary high-throughput data. However,
there are many more public high-throughput data sets available and they
are certainly not available through genome browsers. Normally, every
high-throughput dataset associated with a publication should be
deposited to public archieves. There are two major public archives we
use to deposit data. One of them is \emph{Gene expression Omnibus(GEO) }
hosted at \url{http://www.ncbi.nlm.nih.gov/geo/}, and the other one is
\emph{European nucleotide archive(ENA)} hosted at
\url{http://www.ebi.ac.uk/ena}. These repositories accept
high-throughput datasets and users can freely download and use these
public data sets for their own research. Many data sets in these
repositories are in their raw format, for example the format the
sequencer provides mostly. Some data sets will also have processed data
but that is not a norm.

Apart from these repositories, there are multiple multi-national
consortia that is dedicated to certain genome biology or disease related
problems and they maintain their own databases and provide access to
processed and raw data. Some of these consortia is mentioned below.

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.32\columnwidth}\raggedright
Consortium\strut
\end{minipage} & \begin{minipage}[b]{0.62\columnwidth}\raggedright
what is it for?\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{https://www.encodeproject.org/}{ENCODE}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Transcription factor binding sites, gene expression and epigenomics data
for cell lines\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{http://www.roadmapepigenomics.org/}{Epigenomics Roadmap}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Epigenomics data for multiple cell types\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{http://cancergenome.nih.gov/}{The cancer genome atlas
(TCGA)}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Expression, mutation and epigenomics data for multiple cancer
types\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
\href{http://www.1000genomes.org/}{1000 genomes project}\strut
\end{minipage} & \begin{minipage}[t]{0.62\columnwidth}\raggedright
Human genetic variation data obtained by sequencing 1000s of
individuals\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{Rintro}{%
\chapter{Introduction to R for genomic data analysis}\label{Rintro}}

The aim of computational genomics is to provide biological
interpretation and insights from high dimensional genomics data.
Generally speaking, it is similar to any other kind of data analysis
endeavor but often times doing computational genomics will require
domain specific knowledge and tools.

As new high-throughout experimental techniques on the rise, data
analysis capabilities are sought-after features for researchers. The aim
of this chapter is to first familiarize the readers with data analysis
steps and then provide basics of R programming within the context of
genomic data analysis.R is a free statistical programming language that
is popular among researchers and data miners to build software and
analyze data. Although basic R programming tutorials are easily
accessible, we are aiming to introduce the subject with the genomic
context in the background. The examples and narrative will always be
from real-life situations when you try to analyze genomic data with R.
We believe tailoring material to the context of genomics makes a
difference when learning this programming language for sake of analyzing
genomic data.

\hypertarget{steps-of-genomic-data-analysis}{%
\section{Steps of (genomic) data
analysis}\label{steps-of-genomic-data-analysis}}

Regardless of the analysis type, the data analysis has a common pattern.
We will discuss this general pattern and how it applies to genomics
problems. The data analysis steps typically include data collection,
quality check and cleaning, processing, modeling, visualization and
reporting. Although, one expects to go through these steps in a linear
fashion, it is normal to go back and repeat the steps with different
parameters or tools. In practice, data analysis requires going through
the same steps over and over again in order to be able to do a
combination of the following: a) answering other related questions, b)
dealing with data quality issues later realized, c) including new data
sets to the analysis.

We will now go through brief explanation of the steps within the context
of genomic data analysis.

\hypertarget{data-collection}{%
\subsection{Data collection}\label{data-collection}}

Data collection refers to any source, experiment or survey that provides
data for the data analysis question you have. In genomics, data
collection is done by high-throughput assays introduced in chapter
\ref{intro}. One can also use publicly available data sets and
specialized data bases also mentioned in chapter \ref{intro}. How much
data and what type of data you should collect depends on the question
you are trying to answer ant the technical and biological variability of
the system you are studying.

\hypertarget{data-quality-check-and-cleaning}{%
\subsection{Data quality check and
cleaning}\label{data-quality-check-and-cleaning}}

In general, the data analysis almost always deals with imperfect data.
It is common to have missing values or measurements that are noisy. Data
quality check and cleaning aims to identify any data quality issue and
clean it from the dataset.

High-throughout genomics data is produced by technologies that could
embed technical biases into the data. If we were to give an example from
sequencing, the sequenced reads do not have the same quality of bases
called. Towards the ends of the reads, you might have bases that might
be called incorrectly. Identifying those low quality bases and removing
them will improve read mapping step.

\hypertarget{data-processing}{%
\subsection{Data processing}\label{data-processing}}

This step refers to processing the data to a format that is suitable for
exploratory analysis and modeling. Often times, the data will not come
in ready to analyze format. You may need to convert it to other formats
by transforming data points (such as log transforming, normalizing etc),
or subset the data set with some arbitrary or pre-defined condition. In
terms of genomics, processing includes multiple steps. Following the
sequencing analysis example above, processing will include aligning
reads to the genome and quantification over genes or regions of
interest. This simply counting how many reads are covering your regions
of interest. This quantity can give you ideas about how much a gene
expressed if your experimental protocol was RNA sequencing. This can be
followed by some normalization to aid the next step.

\hypertarget{exploratory-data-analysis-and-modeling}{%
\subsection{Exploratory data analysis and
modeling}\label{exploratory-data-analysis-and-modeling}}

This phase usually takes in the processed or semi-processed data and
applies machine-learning or statistical methods to explore the data.
Typically, one needs to see relationship between variables measured,
relationship between samples based on the variables measured. At this
point, we might be looking to see if the samples group as expected by
the experimental design, are there outliers or any other anomalies ?
After this step you might want to do additional clean up or
re-processing to deal with anomalies.

Another related step is modeling. This generally refers to modelling
your variable of interest based on other variables you measured. In the
context of genomics, it could be that you are trying to predict disease
status of the patients from expression of genes you measured from their
tissue samples. Then your variable of interest is the disease status and
. This is generally called predictive modeling and could be solved with
regression based or any other machine-learning methods. This kind of
approach is generally called ``predictive modeling''.

Statistical modeling would also be a part of this modeling step, this
can cover predictive modeling as well where we use statistical methods
such as linear regression. Other analyses such as hypothesis testing,
where we have an expectation and we are trying to confirm that
expectation is also related to statistical modeling. A good example of
this in genomics is the differential gene expression analysis. This can
be formulated as comparing two data sets, in this case expression values
from condition A and condition B, with the expectation that condition A
and condition B has similar expression values. You will see more on this
on chapter \ref{stats}.

\hypertarget{visualization-and-reporting}{%
\subsection{Visualization and
reporting}\label{visualization-and-reporting}}

Visualization is necessary for all the previous steps more or less. But
in the final phase, we need final figures, tables and text that
describes the outcome of your analysis. This will be your report. In
genomics, we use common data visualization methods as well as specific
visualization methods developed or popularized by genomic data analysis.
You will see many popular visualization methods in chapters \ref{stats}
and @\{genomicIntervals\}

\hypertarget{why-use-r-for-genomics}{%
\subsection{Why use R for genomics ?}\label{why-use-r-for-genomics}}

R, with its statistical analysis heritage, plotting features and rich
user-contributed packages is one of the best languages for the task of
analyzing genomic data. High-dimensional genomics datasets are usually
suitable to be analyzed with core R packages and functions. On top of
that, Bioconductor and CRAN have an array of specialized tools for doing
genomics specific analysis. Here is a list of computational genomics
tasks that can be completed using R.

\hypertarget{data-cleanup-and-processing}{%
\subsubsection{Data cleanup and
processing}\label{data-cleanup-and-processing}}

Most of general data clean up, such as removing incomplete columns and
values, reorganizing and transforming data, these tasks can be achieved
using R. In addition, with the help of packages R can connect to
databases in various formats such as mySQL, mongoDB, etc., and query and
get the data to R environment using database specific tools.

On top of these, genomic data specific processing and quality check can
be achieved via R/Bioconductor packages. For example, sequencing read
quality checks and even HT-read alignments can be achieved via R
packages.

\hypertarget{general-data-anaylsis-and-exploration}{%
\subsubsection{General data anaylsis and
exploration}\label{general-data-anaylsis-and-exploration}}

Most genomics data sets are suitable for application of general data
analysis tools. In some cases, you may need to preprocess the data to
get it to a state that is suitable for application such tools. Here is a
non-exhaustive list of what kind of things can be done via R.

\begin{itemize}
\tightlist
\item
  unsupervised data analysis: clustering (k-means, hierarchical), matrix
  factorization (PCA, ICA etc)
\item
  supervised data analysis: generalized linear models, support vector
  machines, randomForests
\end{itemize}

\hypertarget{genomics-specific-data-analysis-methods}{%
\subsubsection{Genomics specific data analysis
methods}\label{genomics-specific-data-analysis-methods}}

R/Bioconductor gives you access to multitude of other bioinformatics
specific algorithms. Here are some of the things you can do.

\begin{itemize}
\tightlist
\item
  Sequence analysis: TF binding motifs, GC content and CpG counts of a
  given DNA sequence
\item
  Differential expression (or arrays and sequencing based measurements)
\item
  Gene set/Pathway analysis: What kind of genes are enriched in my gene
  set
\item
  Genomic Interval opertaions such as Overlapping CpG islands with
  transcription start sites, and filtering based on overlaps
\item
  Overlapping aligned reads with exons and counting aligned reads per
  gene
\end{itemize}

\hypertarget{visualization}{%
\subsubsection{Visualization}\label{visualization}}

Visualization is an important part of all data analysis techniques
including computational genomics. Again, you can use core visualization
techniques in R and also genomics specific ones with the help of
specific packages. Here are some of the things you can do with R.

\begin{itemize}
\tightlist
\item
  Basic plots: Histograms, scatter plots, bar plots, box plots, heatmaps
\item
  ideograms and circus plots for genomics provides visualization of
  different features over the whole genome.
\item
  meta-profiles of genomic features, such as read enrichment over all
  promoters
\item
  Visualization of quantitative assays for given locus in the genome
\end{itemize}

\hypertarget{getting-started-with-r}{%
\section{Getting started with R}\label{getting-started-with-r}}

Download and install R \url{http://cran.r-project.org/} and RStudio
\url{http://www.rstudio.com/} if you do not have them already. Rstudio
is optional but it is a great tool if you are just starting to learn R.
You will need specific data sets to run the codes in this document.
Download the data.zip{[}URL to come{]} and extract it to your directory
of choice. The folder name should be ``data'' and your R working
directory should be level above the data folder. That means in your R
console, when you type ``dir(``data'')'' you should be able to see the
contents of the data folder. You can change your working directory by
\emph{setwd()} command and get your current working directory with
\emph{getwd()} command in R. In RStudio, you can click on the top menu
and change the location of your working directory via user interface.

\hypertarget{installing-packages}{%
\subsection{Installing packages}\label{installing-packages}}

R packages are add-ons to base R that help you achieve additional tasks
that are not directly supported by base R. It is by the action of these
extra functionality that R excels as a tool for computational genomics.
Bioconductor project (\url{http://bioconductor.org/}) is a dedicated
package repository for computational biology related packages. However
main package repository of R, called CRAN, has also computational
biology related packages. In addition,
R-Forge(\url{http://r-forge.r-project.org/}),
GitHub(\url{https://github}. com/), and
googlecode(\url{http://code.google.com}) are other locations where R
packages might be hosted. You can install CRAN packages using
install.packages(). (\# is the comment character in R)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install package named "randomForests" from CRAN}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"randomForests"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can install bioconductor packages with a specific installer script

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the installer package}
\KeywordTok{source}\NormalTok{(}\StringTok{"http://bioconductor.org/biocLite.R"}\NormalTok{)}
\CommentTok{# install bioconductor package "rtracklayer"}
\KeywordTok{biocLite}\NormalTok{(}\StringTok{"rtracklayer"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can install packages from github using \emph{install\_github()}
function from \textbf{devtools}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{install_github}\NormalTok{(}\StringTok{"hadley/stringr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Another way to install packages are from the source.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# download the source file}
\KeywordTok{download.file}\NormalTok{(}\StringTok{"http://goo.gl/3pvHYI"}\NormalTok{,}
               \DataTypeTok{destfile=}\StringTok{"methylKit_0.5.7.tar.gz"}\NormalTok{)}
\CommentTok{# install the package from the source file}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"methylKit_0.5.7.tar.gz"}\NormalTok{,}
                 \DataTypeTok{repos=}\OtherTok{NULL}\NormalTok{,}\DataTypeTok{type=}\StringTok{"source"}\NormalTok{)}
\CommentTok{# delete the source file}
\KeywordTok{unlink}\NormalTok{(}\StringTok{"methylKit_0.5.7.tar.gz"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can also update CRAN and Bioconductor packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# updating CRAN packages}
\KeywordTok{update.packages}\NormalTok{()}

\CommentTok{# updating bioconductor packages}
\KeywordTok{source}\NormalTok{(}\StringTok{"http://bioconductor.org/biocLite.R"}\NormalTok{)}
\KeywordTok{biocLite}\NormalTok{(}\StringTok{"BiocUpgrade"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{installing-packages-in-custom-locations}{%
\subsection{Installing packages in custom
locations}\label{installing-packages-in-custom-locations}}

If you will be using R on servers or computing clusters rather than your
personal computer it is unlikely that you will have administrator access
to install packages. In that case, you can install packages in custom
locations by telling R where to look for additional packages. This is
done by setting up an .Renviron file in your home directory and add the
following line:

\begin{verbatim}
R_LIBS=~/Rlibs
\end{verbatim}

This tells R that ``Rlibs'' directory at your home directory will be the
first choice of locations to look for packages and install packages (The
directory name and location is up to you above is just an example). You
should go and create that directory now. After that, start a fresh R
session and start installing packages. From now on, packages will be
installed to your local directory where you have read-write access.

\hypertarget{getting-help-on-functions-and-packages}{%
\subsection{Getting help on functions and
packages}\label{getting-help-on-functions-and-packages}}

You can get help on functions by help() and help.search() functions. You
can list the functions in a package with ls() function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{ls}\NormalTok{(}\StringTok{"package:MASS"}\NormalTok{) }\CommentTok{# functions in the package}
\KeywordTok{ls}\NormalTok{() }\CommentTok{# objects in your R enviroment}
\CommentTok{# get help on hist() function}
\NormalTok{?hist}
\KeywordTok{help}\NormalTok{(}\StringTok{"hist"}\NormalTok{)}
\CommentTok{# search the word "hist" in help pages}
\KeywordTok{help.search}\NormalTok{(}\StringTok{"hist"}\NormalTok{)}
\NormalTok{??hist}
\end{Highlighting}
\end{Shaded}

\hypertarget{more-help-needed}{%
\subsubsection{More help needed?}\label{more-help-needed}}

In addition, check package vignettes for help and practical
understanding of the functions. All Bionconductor packages have
vignettes that walk you through example analysis. Google search will
always be helpful as well, there are many blogs and web pages that have
posts about R. R-help, Stackoverflow and R-bloggers are usually source
of good and reliable information.

\hypertarget{computations-in-r}{%
\section{Computations in R}\label{computations-in-r}}

R can be used as an ordinary calculator, some say it is an over-grown
calculator. Here are some examples. Remember \textbf{\#} is the comment
character. The comments give details about the operations in case they
are not clear.

\begin{verbatim}
2 + 3 * 5       # Note the order of operations.
log(10)        # Natural logarithm with base e
5^2            # 5 raised to the second power
3/2            # Division
sqrt(16)      # Square root
abs(3-7)      # Absolute value of 3-7
pi             # The number
exp(2)        # exponential function
# This is a comment line

\end{verbatim}

\hypertarget{data-structures}{%
\section{Data structures}\label{data-structures}}

R has multiple data structures. If you are familiar with excel you can
think of data structures as building blocks of a table and the table
itself, and a table is similar to a sheet in excel. Most of the time you
will deal with tabular data sets, you will manipulate them, take
sub-sections of them. It is essential to know what are the common data
structures in R and how they can be used. R deals with named data
structures, this means you can give names to data structures and
manipulate or operate on them using those names.

\hypertarget{vectors}{%
\subsection{Vectors}\label{vectors}}

Vectors are one the core R data structures. It is basically a list of
elements of the same type (numeric,character or logical). Later you will
see that every column of a table will be represented as a vector. R
handles vectors easily and intuitively. You can create vectors with c()
function, however that is not the only way. The operations on vectors
will propagate to all the elements of the vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{)    }\CommentTok{#create a vector named x with 5 components}
\NormalTok{x =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{)  }
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  3  2 10  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y<-}\DecValTok{1}\OperatorTok{:}\DecValTok{5}              \CommentTok{#create a vector of consecutive integers y}
\NormalTok{y}\OperatorTok{+}\DecValTok{2}                 \CommentTok{#scalar addition}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3 4 5 6 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{*}\NormalTok{y                 }\CommentTok{#scalar multiplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  4  6  8 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}\OperatorTok{^}\DecValTok{2}                 \CommentTok{#raise each component to the second power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  4  9 16 25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{^}\NormalTok{y                 }\CommentTok{#raise 2 to the first through fifth power}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  4  8 16 32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y                   }\CommentTok{#y itself has not been unchanged}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y<-y}\OperatorTok{*}\DecValTok{2}
\NormalTok{y                   }\CommentTok{#it is now changed}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  4  6  8 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r1<-}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)        }\CommentTok{# create a vector of 1s, length 3}
\KeywordTok{length}\NormalTok{(r1)           }\CommentTok{#length of the vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(r1)            }\CommentTok{# class of the vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a<-}\DecValTok{1}                \CommentTok{# this is actually a vector length one}
\end{Highlighting}
\end{Shaded}

\hypertarget{matrices}{%
\subsection{Matrices}\label{matrices}}

A matrix refers to a numeric array of rows and columns. You can think of
it as a stacked version of vectors where each row or column is a vector.
One of the easiest ways to create a matrix is to combine vectors of
equal length using \emph{cbind()}, meaning `column bind'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{y<-}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{)}
\NormalTok{m1<-}\KeywordTok{cbind}\NormalTok{(x,y);m1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      x y
## [1,] 1 4
## [2,] 2 5
## [3,] 3 6
## [4,] 4 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t}\NormalTok{(m1)                }\CommentTok{# transpose of m1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [,1] [,2] [,3] [,4]
## x    1    2    3    4
## y    4    5    6    7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(m1)              }\CommentTok{# 2 by 5 matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 2
\end{verbatim}

You can also directly list the elements and specify the matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{,}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{9}\NormalTok{),}\DataTypeTok{nrow=}\DecValTok{3}\NormalTok{)}
\NormalTok{m2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    5    2
## [2,]    3   -1    3
## [3,]    2    2    9
\end{verbatim}

Matrices and the next data structure \textbf{data frames} are tabular
data structures. You can subset them using
\textbf{\protect\hyperlink{section-8}{}} and providing desired rows and
columns to subset. Figure \ref{fig:slicingDataFrames} shows how that
works conceptually.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/slicingDataFrames} 

}

\caption{slicing/subsetting of a matrix and a data frame}\label{fig:slicingDataFrames}
\end{figure}

\hypertarget{data-frames}{%
\subsection{Data Frames}\label{data-frames}}

A data frame is more general than a matrix, in that different columns
can have different modes (numeric, character, factor, etc.). A data
frame can be constructed by data.frame() function. For example, we
illustrate how to construct a data frame from genomic intervals or
coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chr <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"chr1"}\NormalTok{, }\StringTok{"chr1"}\NormalTok{, }\StringTok{"chr2"}\NormalTok{, }\StringTok{"chr2"}\NormalTok{)}
\NormalTok{strand <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"-"}\NormalTok{,}\StringTok{"-"}\NormalTok{,}\StringTok{"+"}\NormalTok{,}\StringTok{"+"}\NormalTok{)}
\NormalTok{start<-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{,}\DecValTok{4000}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{400}\NormalTok{)}
\NormalTok{end<-}\KeywordTok{c}\NormalTok{(}\DecValTok{250}\NormalTok{,}\DecValTok{410}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{450}\NormalTok{)}
\NormalTok{mydata <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(chr,start,end,strand)}
\CommentTok{#change column names}
\KeywordTok{names}\NormalTok{(mydata) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"chr"}\NormalTok{,}\StringTok{"start"}\NormalTok{,}\StringTok{"end"}\NormalTok{,}\StringTok{"strand"}\NormalTok{)}
\NormalTok{mydata }\CommentTok{# OR this will work too}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 1 chr1   200 250      -
## 2 chr1  4000 410      -
## 3 chr2   100 200      +
## 4 chr2   400 450      +
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{chr=}\NormalTok{chr,}\DataTypeTok{start=}\NormalTok{start,}\DataTypeTok{end=}\NormalTok{end,}\DataTypeTok{strand=}\NormalTok{strand)}
\NormalTok{mydata}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 1 chr1   200 250      -
## 2 chr1  4000 410      -
## 3 chr2   100 200      +
## 4 chr2   400 450      +
\end{verbatim}

There are a variety of ways to extract the elements of a data frame. You
can extract certain columns using column numbers or names, or you can
extract certain rows by using row numbers. You can also extract data
using logical arguments, such as extracting all rows that has a value in
a column larger than your threshold.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[,}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{] }\CommentTok{# columns 2,3,4 of data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   start end strand
## 1   200 250      -
## 2  4000 410      -
## 3   100 200      +
## 4   400 450      +
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[,}\KeywordTok{c}\NormalTok{(}\StringTok{"chr"}\NormalTok{,}\StringTok{"start"}\NormalTok{)] }\CommentTok{# columns chr and start from data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start
## 1 chr1   200
## 2 chr1  4000
## 3 chr2   100
## 4 chr2   400
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata}\OperatorTok{$}\NormalTok{start }\CommentTok{# variable start in the data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  200 4000  100  400
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),] }\CommentTok{# get 1st and 3rd rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 1 chr1   200 250      -
## 3 chr2   100 200      +
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mydata[mydata}\OperatorTok{$}\NormalTok{start}\OperatorTok{>}\DecValTok{400}\NormalTok{,] }\CommentTok{# get all rows where start>400}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    chr start end strand
## 2 chr1  4000 410      -
\end{verbatim}

\hypertarget{lists}{%
\subsection{Lists}\label{lists}}

An ordered collection of objects (components). A list allows you to
gather a variety of (possibly unrelated) objects under one name.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# example of a list with 4 components}
\CommentTok{# a string, a numeric vector, a matrix, and a scalar}
\NormalTok{w <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{name=}\StringTok{"Fred"}\NormalTok{,}
       \DataTypeTok{mynumbers=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}
       \DataTypeTok{mymatrix=}\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,}\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{),}
       \DataTypeTok{age=}\FloatTok{5.3}\NormalTok{)}
\NormalTok{w}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $name
## [1] "Fred"
## 
## $mynumbers
## [1] 1 2 3
## 
## $mymatrix
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
## 
## $age
## [1] 5.3
\end{verbatim}

You can extract elements of a list using the
\textbf{{[}\protect\hyperlink{section-8}{}{]}} convention using either
its position in the list or its name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w[[}\DecValTok{3}\NormalTok{]] }\CommentTok{# 3rd component of the list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w[[}\StringTok{"mynumbers"}\NormalTok{]] }\CommentTok{# component named mynumbers in list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w}\OperatorTok{$}\NormalTok{age}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.3
\end{verbatim}

\hypertarget{factors}{%
\subsection{Factors}\label{factors}}

Factors are used to store categorical data. They are important for
statistical modeling since categorical variables are treated differently
in statistical models than continuous variables. This ensures
categorical data treated accordingly in statistical models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{features=}\KeywordTok{c}\NormalTok{(}\StringTok{"promoter"}\NormalTok{,}\StringTok{"exon"}\NormalTok{,}\StringTok{"intron"}\NormalTok{)}
\NormalTok{f.feat=}\KeywordTok{factor}\NormalTok{(features)}
\end{Highlighting}
\end{Shaded}

Important thing to note is that when you are reading a data.frame with
read.table() or creating a data frame with \textbf{data.frame()}
character columns are stored as factors by default, to change this
behavior you need to set \textbf{stringsAsFactors=FALSE} in
\textbf{read.table()} and/or \textbf{data.frame()} function arguments.

\hypertarget{data-types}{%
\section{Data types}\label{data-types}}

There are four common data types in R, they are \textbf{numeric},
\textbf{logical}, \textbf{character} and \textbf{integer}. All these
data types can be used to create vectors natively.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#create a numeric vector x with 5 components}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  3  2 10  5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#create a logical vector x}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a character vector}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(}\StringTok{"sds"}\NormalTok{,}\StringTok{"sd"}\NormalTok{,}\StringTok{"as"}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sds" "sd"  "as"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create an integer vector}
\NormalTok{x<-}\KeywordTok{c}\NormalTok{(1L,2L,3L)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "integer"
\end{verbatim}

\hypertarget{reading-and-writing-data}{%
\section{Reading and writing data}\label{reading-and-writing-data}}

Most of the genomics data sets are in the form of genomic intervals
associated with a score. That means mostly the data will be in table
format with columns denoting chromosome, start positions, end positions,
strand and score. One of the popular formats is BED format used
primarily by UCSC genome browser but most other genome browsers and
tools will support BED format. We have all the annotation data in BED
format. In R, you can easily read tabular format data with read.table()
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{enhancerFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"subset.enhancers.hg18.bed"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgiFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"subset.cpgi.hg18.bed"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\CommentTok{# read enhancer marker BED file}
\NormalTok{enh.df <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(enhancerFilePath, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{) }

\CommentTok{# read CpG island BED file}
\NormalTok{cpgi.df <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(cpgiFilePath, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{) }

\CommentTok{# check first lines to see how the data looks like}
\KeywordTok{head}\NormalTok{(enh.df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      V1     V2     V3 V4   V5 V6    V7    V8 V9
## 1 chr20 266275 267925  . 1000  .  9.11 13.17 -1
## 2 chr20 287400 294500  . 1000  . 10.53 13.02 -1
## 3 chr20 300500 302500  . 1000  .  9.10 13.39 -1
## 4 chr20 330400 331800  . 1000  .  6.39 13.51 -1
## 5 chr20 341425 343400  . 1000  .  6.20 12.99 -1
## 6 chr20 437975 439900  . 1000  .  6.31 13.52 -1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(cpgi.df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      V1     V2     V3       V4
## 1 chr20 195575 195851  CpG:_28
## 2 chr20 207789 208148  CpG:_32
## 3 chr20 219055 219437  CpG:_33
## 4 chr20 225831 227155 CpG:_135
## 5 chr20 252826 256323 CpG:_286
## 6 chr20 275376 276977 CpG:_116
\end{verbatim}

You can save your data by writing it to disk as a text file. A data
frame or matrix can be written out by using write.table() function. Now
let us write out cpgi.df, we will write it out as a tab-separated file,
pay attention to the arguments.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.table}\NormalTok{(cpgi.df,}\DataTypeTok{file=}\StringTok{"cpgi.txt"}\NormalTok{,}\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{,}
            \DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{col.names=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can save your R objects directly into a file using save() and
saveRDS() and load them back in with load() and readRDS(). By using
these functions you can save any R object whether or not they are in
data frame or matrix classes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{save}\NormalTok{(cpgi.df,enh.df,}\DataTypeTok{file=}\StringTok{"mydata.RData"}\NormalTok{)}
\KeywordTok{load}\NormalTok{(}\StringTok{"mydata.RData"}\NormalTok{)}
\CommentTok{# saveRDS() can save one object at a type}
\KeywordTok{saveRDS}\NormalTok{(cpgi.df,}\DataTypeTok{file=}\StringTok{"cpgi.rds"}\NormalTok{)}
\NormalTok{x=}\KeywordTok{readRDS}\NormalTok{(}\StringTok{"cpgi.rds"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

One important thing is that with \texttt{save()} you can save many
objects at a time and when they are loaded into memory with
\texttt{load(} they retain their variable names. For example, in the
above code when you use \texttt{load("mydata.RData")} in a fresh R
session, an object names ``cpg.df'' will be created. That means you have
to figure out what name you gave it to the objects before saving them.
On the contrary to that, when you save an object by \texttt{saveRDS()}
and read by \texttt{readRDS()} the name of the object is not retained,
you need to assign the output of \texttt{readRDS()} to a new variable
(``x'' in the above code chunk).

\hypertarget{plotting-in-r}{%
\section{Plotting in R}\label{plotting-in-r}}

R has great support for plotting and customizing plots. We will show
only a few below. Let us sample 50 values from normal distribution and
plot them as a histogram.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sample 50 values from normal distribution}
\CommentTok{# and store them in vector x}
\NormalTok{x<-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(x) }\CommentTok{# plot the histogram of those values}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.65\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-20-1} \end{center}

We can modify all the plots by providing certain arguments to the
plotting function. Now let's give a title to the plot using
\textbf{`main'} argument. We can also change the color of the bars using
\textbf{`col'} argument. You can simply provide the name of the color.
Below, we are using \textbf{`red'} for the color. See Figure below for
the result this chunk.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(x,}\DataTypeTok{main=}\StringTok{"Hello histogram!!!"}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.65\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-21-1} \end{center}

Next, we will make a scatter plot. Scatter plots are one the most common
plots you will encounter in data analysis. We will sample another set of
50 values and plotted those against the ones we sampled earlier.
Scatterplot shows values of two variables for a set of data points. It
is useful to visualize relationships between two variables. It is
frequently used in connection with correlation and linear regression.
There are other variants of scatter plots which show density of the
points with different colors. We will show examples of those that in
following chapters. The scatter plot from our sampling experiment is
shown in the figure. Notice that, in addition to main we used
\textbf{``xlab''} and \textbf{``ylab''} arguments to give labels to the
plot. You can customize the plots even more than this. See
\textbf{?plot} and \textbf{?par} for more arguments that can help you
customize the plots.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# randomly sample 50 points from normal distribution}
\NormalTok{y<-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{)}
\CommentTok{#plot a scatter plot}
\CommentTok{# control x-axis and y-axis labels}
\KeywordTok{plot}\NormalTok{(x,y,}\DataTypeTok{main=}\StringTok{"scatterplot of random samples"}\NormalTok{,}
        \DataTypeTok{ylab=}\StringTok{"y values"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"x values"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.65\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-22-1} \end{center}

we can also plot boxplots for vectors x and y. Boxplots depict groups of
numerical data through their quartiles. The edges of the box denote 1st
and 3rd quartile, and the line that crosses the box is the median.
Whiskers usually are defined using interquantile range:

\emph{lowerWhisker=Q1-1.5{[}IQR{]} and upperWhisker=Q1+1.5{[}IQR{]}}

In addition, outliers can be depicted as dots. In this case, outliers
are the values that remain outside the whiskers.

\begin{Shaded}
\begin{Highlighting}[]
 \KeywordTok{boxplot}\NormalTok{(x,y,}\DataTypeTok{main=}\StringTok{"boxplots of random samples"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.65\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-23-1} \end{center}

Next up is bar plot which you can plot by \textbf{barplot()} function.
We are going to plot four imaginary percentage values and color them
with two colors, and this time we will also show how to draw a legend on
the plot using \textbf{legend()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perc=}\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{70}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{25}\NormalTok{)}
\KeywordTok{barplot}\NormalTok{(}\DataTypeTok{height=}\NormalTok{perc,}
        \DataTypeTok{names.arg=}\KeywordTok{c}\NormalTok{(}\StringTok{"CpGi"}\NormalTok{,}\StringTok{"exon"}\NormalTok{,}\StringTok{"CpGi"}\NormalTok{,}\StringTok{"exon"}\NormalTok{),}
        \DataTypeTok{ylab=}\StringTok{"percentages"}\NormalTok{,}\DataTypeTok{main=}\StringTok{"imagine %s"}\NormalTok{,}
        \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"blue"}\NormalTok{))}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topright"}\NormalTok{,}\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{"test"}\NormalTok{,}\StringTok{"control"}\NormalTok{),}
       \DataTypeTok{fill=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{,}\StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.65\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-24-1} \end{center}

\hypertarget{saving-plots}{%
\section{Saving plots}\label{saving-plots}}

If you want to save your plots to an image file there are couple of ways
of doing that. Normally, you will have to do the following: 1. Open a
graphics device 2. Create the plot 3. Close the graphics device

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pdf}\NormalTok{(}\StringTok{"mygraphs/myplot.pdf"}\NormalTok{,}\DataTypeTok{width=}\DecValTok{5}\NormalTok{,}\DataTypeTok{height=}\DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x,y)}
\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Alternatively, you can first create the plot then copy the plot to a
graphic device.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(x,y)}
\KeywordTok{dev.copy}\NormalTok{(pdf,}\StringTok{"mygraphs/myplot.pdf"}\NormalTok{,}\DataTypeTok{width=}\DecValTok{7}\NormalTok{,}\DataTypeTok{height=}\DecValTok{5}\NormalTok{)}
\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{functions-and-control-structures-for-ifelse-etc.}{%
\section{Functions and control structures (for, if/else
etc.)}\label{functions-and-control-structures-for-ifelse-etc.}}

\hypertarget{user-defined-functions}{%
\subsection{User defined functions}\label{user-defined-functions}}

Functions are useful for transforming larger chunks of code to re-usable
pieces of code. Generally, if you need to execute certain tasks with
variable parameters then it is time you write a function. A function in
R takes different arguments and returns a definite output, much like
mathematical functions. Here is a simple function takes two arguments, x
and y, and returns the sum of their squares.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sqSum<-}\ControlFlowTok{function}\NormalTok{(x,y)\{}
\NormalTok{result=x}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{y}\OperatorTok{^}\DecValTok{2}
\KeywordTok{return}\NormalTok{(result)}
\NormalTok{\}}
\CommentTok{# now try the function out}
\KeywordTok{sqSum}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13
\end{verbatim}

Functions can also output plots and/or messages to the terminal. Here is
a function that prints a message to the terminal:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sqSumPrint<-}\ControlFlowTok{function}\NormalTok{(x,y)\{}
\NormalTok{result=x}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{y}\OperatorTok{^}\DecValTok{2}
\KeywordTok{cat}\NormalTok{(}\StringTok{"here is the result:"}\NormalTok{,result,}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{# now try the function out}
\KeywordTok{sqSumPrint}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## here is the result: 13
\end{verbatim}

Sometimes we would want to execute a certain part of the code only if
certain condition is satisfied. This condition can be anything from the
type of an object (Ex: if object is a matrix execute certain code), or
it can be more complicated such as if object value is between certain
thresholds. Let us see how they can be used3. They can be used anywhere
in your code, now we will use it in a function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgi.df <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"intro2R_data/data/subset.cpgi.hg18.bed"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{# function takes input one row}
\CommentTok{# of CpGi data frame}
\NormalTok{largeCpGi<-}\ControlFlowTok{function}\NormalTok{(bedRow)\{}
\NormalTok{ cpglen=bedRow[}\DecValTok{3}\NormalTok{]}\OperatorTok{-}\NormalTok{bedRow[}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\DecValTok{1}
 \ControlFlowTok{if}\NormalTok{(cpglen}\OperatorTok{>}\DecValTok{1500}\NormalTok{)\{}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"this is large}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{ \}}
 \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(cpglen}\OperatorTok{<=}\DecValTok{1500} \OperatorTok{&}\StringTok{ }\NormalTok{cpglen}\OperatorTok{>}\DecValTok{700}\NormalTok{)\{}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"this is normal}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{ \}}
 \ControlFlowTok{else}\NormalTok{\{}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"this is short}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{ \}}
\NormalTok{\}}
\KeywordTok{largeCpGi}\NormalTok{(cpgi.df[}\DecValTok{10}\NormalTok{,])}
\KeywordTok{largeCpGi}\NormalTok{(cpgi.df[}\DecValTok{100}\NormalTok{,])}
\KeywordTok{largeCpGi}\NormalTok{(cpgi.df[}\DecValTok{1000}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

\hypertarget{loops-and-looping-structures-in-r}{%
\subsection{Loops and looping structures in
R}\label{loops-and-looping-structures-in-r}}

When you need to repeat a certain task or a execute a function multiple
times, you can do that with the help of loops. A loop will execute the
task until a certain condition is reached. The loop below is called a
``for-loop'' and it executes the task sequentially 10 times.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)\{ }\CommentTok{# number of repetitions}
\KeywordTok{cat}\NormalTok{(}\StringTok{"This is iteration"}\NormalTok{) }\CommentTok{# the task to be repeated}
\KeywordTok{print}\NormalTok{(i)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## This is iteration[1] 1
## This is iteration[1] 2
## This is iteration[1] 3
## This is iteration[1] 4
## This is iteration[1] 5
## This is iteration[1] 6
## This is iteration[1] 7
## This is iteration[1] 8
## This is iteration[1] 9
## This is iteration[1] 10
\end{verbatim}

The task above is a bit pointless, normally in a loop, you would want to
do something meaningful. Let us calculate the length of the CpG islands
we read in earlier. Although this is not the most efficient way of doing
that particular task, it serves as a good example for looping. The code
below will be execute hundred times, and it will calculate the length of
the CpG islands for the first 100 islands in the data frame (by
subtracting the end coordinate from the start coordinate).

\textbf{Note:}If you are going to run a loop that has a lot of
repetitions, it is smart to try the loop with few repetitions first and
check the results. This will help you make sure the code in the loop
works before executing it for thousands of times.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is where we will keep the lenghts}
\CommentTok{# for now it is an empty vector}
\NormalTok{result=}\KeywordTok{c}\NormalTok{()}
\CommentTok{# start the loop}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{)\{}
    \CommentTok{#calculate the length}
\NormalTok{    len=cpgi.df[i,}\DecValTok{3}\NormalTok{]}\OperatorTok{-}\NormalTok{cpgi.df[i,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\DecValTok{1}
    \CommentTok{#append the length to the result}
\NormalTok{    result=}\KeywordTok{c}\NormalTok{(result,len)}
\NormalTok{\}}
\CommentTok{# check the results}
\KeywordTok{head}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  277  360  383 1325 3498 1602
\end{verbatim}

\hypertarget{apply-family-functions-instead-of-loops}{%
\subsubsection{apply family functions instead of
loops}\label{apply-family-functions-instead-of-loops}}

R has other ways of repeating tasks that tend to be more efficient than
using loops. They are known as the \textbf{``apply''} family of
functions, which include \emph{apply, lapply, mapply and tapply} (and
some other variants). All of these functions apply a given function to a
set of instances and returns the result of those functions for each
instance. The differences between them is that they take different type
of inputs. For example apply works on data frames or matrices and
applies the function on each row or column of the data structure.
\emph{lapply} works on lists or vectors and applies a function which
takes the list element as an argument. Next we will demonstrate how to
use \textbf{apply()} on a matrix. The example applies the sum function
on the rows of a matrix, it basically sums up the values on each row of
the matrix, which is conceptualized in Figure \ref{fig:applyConcept}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/apply} 

}

\caption{apply concept in R}\label{fig:applyConcept}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat=}\KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\NormalTok{result<-}\KeywordTok{apply}\NormalTok{(mat,}\DecValTok{1}\NormalTok{,sum)}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12  3  5  6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# OR you can define the function as an argument to apply()}
\NormalTok{result<-}\KeywordTok{apply}\NormalTok{(mat,}\DecValTok{1}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x))}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12  3  5  6
\end{verbatim}

Notice that we used a second argument which equals to 1, that indicates
that rows of the matrix/ data frame will be the input for the function.
If we change the second argument to 2, this will indicate that columns
should be the input for the function that will be applied. See Figure
\ref{fig:applyConcept2} for the visualization of apply() on columns.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/apply2} 

}

\caption{apply function on columns}\label{fig:applyConcept2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result<-}\KeywordTok{apply}\NormalTok{(mat,}\DecValTok{2}\NormalTok{,sum)}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9 3 6 2 3 3
\end{verbatim}

Next, we will use \textbf{lapply()}, which applies a function on a list
or a vector. The function that will be applied is a simple function that
takes the square of a given number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\KeywordTok{lapply}\NormalTok{(input,}\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 9
\end{verbatim}

\textbf{mapply()} is another member of apply family, it can apply a
function on an unlimited set of vectors/lists, it is like a version of
lapply that can handle multiple vectors as arguments. In this case, the
argument to the mapply() is the function to be applied and the sets of
parameters to be supplied as arguments of the function. This
conceptualized Figure \ref{fig:mapplyConcept}, the function to be
applied is a function that takes to arguments and sums them up. The
arguments to be summed up are in the format of vectors, Xs and Ys.
\texttt{mapply()} applies the summation function to each pair in Xs and
Ys vector. Notice that the order of the input function and extra
arguments are different for mapply.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/mapply} 

}

\caption{mapply concept}\label{fig:mapplyConcept}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Xs=}\DecValTok{0}\OperatorTok{:}\DecValTok{5}
\NormalTok{Ys=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{result<-}\KeywordTok{mapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x,y) }\KeywordTok{sum}\NormalTok{(x,y),Xs,Ys)}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4 6 7 8
\end{verbatim}

\hypertarget{apply-family-functions-on-multiple-cores}{%
\subsubsection{apply family functions on multiple
cores}\label{apply-family-functions-on-multiple-cores}}

If you have large data sets apply family functions can be slow (although
probably still better than for loops). If that is the case, you can
easily use the parallel versions of those functions from parallel
package. These functions essentially divide your tasks to smaller chunks
run them on separate CPUs and merge the results from those parallel
operations. This concept is visualized at Figure below ,
\texttt{mcapply} runs the summation function on three different
processors. Each processor executes the summation function on a part of
the data set, and the results are merged and returned as a single vector
that has the same order as the input parameters Xs and Ys.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/mcmapply} 

}

\caption{mcapplyconcept}\label{fig:mcapplyConcept}
\end{figure}

\hypertarget{vectorized-functions-in-r}{%
\subsubsection{Vectorized Functions in
R}\label{vectorized-functions-in-r}}

The above examples have been put forward to illustrate functions and
loops in R because functions using sum() are not complicated and easy to
understand. You will probably need to use loops and looping structures
with more complicated functions. In reality, most of the operations we
used do not need the use of loops or looping structures because there
are already vectorized functions that can achieve the same outcomes,
meaning if the input arguments are R vectors the output will be a vector
as well, so no need for loops or vectorization.

For example, instead of using mapply() and sum() functions we can just
use + operator and sum up Xs and Ys.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result=Xs}\OperatorTok{+}\NormalTok{Ys}
\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4 6 7 8
\end{verbatim}

In order to get the column or row sums, we can use the vectorized
functions colSums() and rowSums().

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colSums}\NormalTok{(mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9 3 6 2 3 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rowSums}\NormalTok{(mat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12  3  5  6
\end{verbatim}

However, remember that not every function is vectorized in R, use the
ones that are. But sooner or later, apply family functions will come in
handy.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\hypertarget{computations-in-r-1}{%
\subsection{Computations in R}\label{computations-in-r-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sum 2 and 3, use +
\item
  Take the square root of 36, use sqrt()
\item
  Take the log10 of 1000, use function \texttt{log10()}
\item
  Take the log2 of 32, use function \texttt{log2()}
\item
  Assign the sum of 2,3 and 4 to variable x
\item
  Find the absolute value of \texttt{5\ -\ 145} using \texttt{abs()}
  function
\item
  Calculate the square root of 625, divide it by 5 and assign it to
  variable \texttt{x}. Ex: \texttt{y=\ log10(1000)/5}, the previous
  statement takes log10 of 1000, divides it by 5 and assigns the value
  to variable y
\item
  Multiply the value you get from previous exercise with 10000, assign
  it variable x Ex: \texttt{y=y*5}, multiplies y with 5 and assigns the
  value to y.
\end{enumerate}

\textbf{KEY CONCEPT:} results of computations or arbitrary values can be
stored in variables we can re-use those variables later on and
over-write them with new values

\hypertarget{data-structures-in-r}{%
\subsection{Data structures in R}\label{data-structures-in-r}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  Make a vector of 1,2,3,5 and 10 using \texttt{c()}, assign it to
  \texttt{vec} variable. Ex: \texttt{vec1=c(1,3,4)} makes a vector out
  of 1,3,4.
\item
  Check the length of your vector with length(). Ex:
  \texttt{length(vec1)} should return 3
\item
  Make a vector of all numbers between 2 and 15. Ex: \texttt{vec=1:6}
  makes a vector of numbers between 1 and 6, assigns to vec variable
\item
  Make a vector of 4s repeated 10 times using \texttt{rep()} function.
  Ex: \texttt{rep(x=2,times=5)} makes a vector of 2s repeated 5 times
\item
  Make a logical vector with TRUE, FALSE values of length 4, use
  \texttt{c()}. Ex: \texttt{c(TRUE,FALSE)}
\item
  Make a character vector of gene names PAX6,ZIC2,OCT4 and SOX2. Ex:
  \texttt{avec=c("a","b","c")} a makes a character vector of a,b and c
\item
  Subset the vector using \texttt{{[}{]}} notation, get 5th and 6th
  elements. Ex: \texttt{vec1{[}1{]}} gets the first element.
  \texttt{vec1{[}c(1,3){]}} gets 1st and 3rd elements
\item
  You can also subset any vector using a logical vector in
  \texttt{{[}{]}}. Run the following:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myvec=}\DecValTok{1}\OperatorTok{:}\DecValTok{5}
\NormalTok{myvec[}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{)] }\CommentTok{# the length of the logical vector should be equal to length(myvec) }
\NormalTok{myvec[}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{FALSE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{17}
\tightlist
\item
  \texttt{==,\textgreater{},\textless{},\ \textgreater{}=,\ \textless{}=}
  operators create logical vectors. See the results of the following
  operations:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myvec }\OperatorTok{>}\StringTok{ }\DecValTok{3}
\NormalTok{myvec }\OperatorTok{==}\StringTok{ }\DecValTok{4}
\NormalTok{myvec }\OperatorTok{<=}\StringTok{ }\DecValTok{2}
\NormalTok{myvec }\OperatorTok{!=}\StringTok{ }\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{18}
\item
  Use \texttt{\textgreater{}} operator in \texttt{myvec{[}\ {]}} to get
  elements larger than 2 in \texttt{myvec} whic is described above
\item
  make a 5x3 matrix (5 rows, 3 columns) using \texttt{matrix()}. Ex:
  matrix(1:6,nrow=3,ncol=2) makes a 3x2 matrix using numbers between 1
  and 6
\item
  What happens when you use \texttt{byrow\ =\ TRUE} in your matrix() as
  an additional argument? Ex:
  \texttt{mat=matrix(1:6,nrow=3,ncol=2,byrow\ =\ TRUE)}
\item
  Extract first 3 columns and first 3 rows of your matrix using
  \texttt{{[}{]}} notation.
\item
  Extract last two rows of the matrix you created earlier.
\end{enumerate}

Ex: \texttt{mat{[}2:3,{]}} or \texttt{mat{[}c(2,3),{]}} extracts 2nd and
3rd rows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{23}
\item
  Extract the first two columns and run \texttt{class()} on the result.
\item
  Extract first column and run \texttt{class()} on the result, compare
  with the above exercise.
\item
  Make a data frame with 3 columns and 5 rows, make sure first column is
  sequence of numbers 1:5, and second column is a character vector.
\end{enumerate}

Ex:
\texttt{df=data.frame(col1=1:3,col2=c("a","b","c"),col3=3:1)\ \#\ 3x3\ data\ frame}.

Remember you need to make 3x5 data frame

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{26}
\tightlist
\item
  Extract first two columns and first two rows.
\end{enumerate}

\textbf{HINT:} Same notation as matrices

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{27}
\tightlist
\item
  Extract last two rows of the data frame you made.
\end{enumerate}

\textbf{HINT:} Same notation as matrices

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{28}
\item
  Extract last two columns using column names of the data frame you
  made.
\item
  Extract second column using column names. You can use \texttt{{[}{]}}
  or \texttt{\$} as in lists, use both in two different answers.
\item
  Extract rows where 1st column is larger than 3. \textbf{HINT:} you can
  get a logical vector using \texttt{\textgreater{}} operator ,logical
  vectors can be used in \texttt{{[}{]}} when subsetting.
\item
  Extract rows where 1st column is larger than or equal to 3.
\item
  Convert data frame to the matrix. \textbf{HINT:} use
  \texttt{as.matrix()}.
\end{enumerate}

Observe what happens to numeric values in the data frame.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{33}
\tightlist
\item
  Make a list using \texttt{list()} function, your list should have 4
  elements the one below has 2.
\end{enumerate}

Ex: \texttt{mylist=\ list(a=c(1,2,3),b=c("apple,"orange"))}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{34}
\tightlist
\item
  Select the 1st element of the list you made using \texttt{\$}
  notation.
\end{enumerate}

Ex: \texttt{mylist\$a} selects first element named ``a''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{35}
\item
  Select the 4th element of the list you made earlier using \texttt{\$}
  notation.
\item
  Select the 1st element of your list using \texttt{{[}\ {]}} notation.
\end{enumerate}

Ex: \texttt{mylist{[}1{]}} selects first element named ``a'', you get a
list with one element.

Ex: \texttt{mylist{[}"a"{]}} selects first element named ``a'', you get
a list with one element.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{37}
\item
  select the 4th element of your list using
  `\protect\hyperlink{section-8}{}' notation.
\item
  Make a factor using factor(), with 5 elements. Ex:
  \texttt{fa=factor(c("a","a","b"))}
\item
  Convert a character vector to factor using \texttt{as.factor()}.
  First, make a character vector using \texttt{c()} then use
  \texttt{as.factor()}.
\item
  Convert the factor you made above to character using
  \texttt{as.character()}.
\end{enumerate}

\hypertarget{reading-in-and-writing-data-out-in-r}{%
\subsection{Reading in and writing data out in
R}\label{reading-in-and-writing-data-out-in-r}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{41}
\item
  Read CpG island (CpGi) data from the compGenomRData package
  \texttt{CpGi.table.hg18.txt}, this is a tab-separated file, store it
  in a variable called ``cpgi''. Use
  \texttt{cpgtFilePath=system.file("extdata",\ \ \ \ \ \ \ \ \ \ \ \ \ "CpGi.table.hg18.txt",\ \ \ \ \ \ \ \ \ \ \ \ \ package="compGenomRData")}
  to get the file path within the installed \texttt{compGenomRData}
  package.
\item
  Use \texttt{head()} on CpGi to see first few rows.
\item
  Why doesn't the following work? see `sep' argument at
  \texttt{help(read.table)}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgtFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"CpGi.table.hg18.txt"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgtFilePath}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "/Users/aakalin/Rlibs/compGenomRData/extdata/CpGi.table.hg18.txt"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgiSepComma=}\KeywordTok{read.table}\NormalTok{(cpgtFilePath,}\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{sep=}\StringTok{","}\NormalTok{)}
\KeywordTok{head}\NormalTok{(cpgiSepComma)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   chrom.chromStart.chromEnd.name.length.cpgNum.gcNum.perCpg.perGc.obsExp
## 1         chr1\t18598\t19673\tCpG: 116\t1075\t116\t787\t21.6\t73.2\t0.83
## 2          chr1\t124987\t125426\tCpG: 30\t439\t30\t295\t13.7\t67.2\t0.64
## 3          chr1\t317653\t318092\tCpG: 29\t439\t29\t295\t13.2\t67.2\t0.62
## 4         chr1\t427014\t428027\tCpG: 84\t1013\t84\t734\t16.6\t72.5\t0.64
## 5         chr1\t439136\t440407\tCpG: 99\t1271\t99\t777\t15.6\t61.1\t0.84
## 6            chr1\t523082\t523977\tCpG: 94\t895\t94\t570\t21\t63.7\t1.04
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{42}
\item
  What happens when \texttt{stringsAsFactors=FALSE} ?
  \texttt{cpgiHF=read.table("intro2R\_data/data/CpGi.table.hg18.txt",header=FALSE,sep="\textbackslash{}t",\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ stringsAsFactors=FALSE)}
\item
  Read only first 10 rows of the CpGi table.
\item
  Use
  \texttt{cpgtFilePath=system.file("extdata","CpGi.table.hg18.txt",package="compGenomRData")}
  to get the file path, then use \texttt{read.table()} with argument
  \texttt{header=FALSE}. Use \texttt{head()} to see the results.
\item
  Write CpG islands to a text file called ``my.cpgi.file.txt''. Write
  the file to your home folder, you can use
  \texttt{file="\textasciitilde{}/my.cpgi.file.txt"} in linux.
  \texttt{\textasciitilde{}/} denotes home folder.
\item
  Same as above but this time make sure use
  \texttt{quote=FALSE},\texttt{sep="\textbackslash{}t"} and
  \texttt{row.names=FALSE} arguments. Save the file to
  ``my.cpgi.file2.txt'' and compare it with ``my.cpgi.file.txt''
\item
  Write out the first 10 rows of `cpgi' data frame. \textbf{HINT:} use
  subsetting for data frames we learned before.
\item
  Write the first 3 columns of `cpgi' data frame.
\item
  Write CpG islands only on chr1. \textbf{HINT:} use subsetting with
  \texttt{{[}{]}}, feed a logical vector using \texttt{==} operator.
\item
  Read two other data sets ``rn4.refseq.bed'' and
  ``rn4.refseq2name.txt'' with \texttt{header=FALSE}, assign them to df1
  and df2 respectively. They are again included in the compGenomRData
  package, and you can use \texttt{system.file()} function to get the
  file paths.
\item
  Use \texttt{head()} to see what is inside of the the data frames
  above.
\item
  Merge data sets using \texttt{merge()} and assign the results to
  variable named `new.df', and use \texttt{head()} to see the results.
\end{enumerate}

\hypertarget{plotting-in-r-1}{%
\subsection{Plotting in R}\label{plotting-in-r-1}}

Please \textbf{run the following} for the rest of the exercises.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1001}\NormalTok{)}
\NormalTok{x1=}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\OperatorTok{+}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{0}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{15}\NormalTok{)}
\NormalTok{y1=}\DecValTok{1}\OperatorTok{:}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\item
  Make a scatter plot using \texttt{x1} and \texttt{y1} vectors
  generated above.
\item
  Use \texttt{main} argument to give a title to \texttt{plot()} as in
  \texttt{plot(x,y,main="title")}
\item
  Use \texttt{xlab} argument to set a label to x-axis.Use \texttt{ylab}
  argument to set a label to y-axis.
\item
  See what \texttt{mtext(side=3,text="hi\ there")} does. \textbf{HINT:}
  mtext stands for margin text.
\item
  See what \texttt{mtext(side=2,text="hi\ there")} does.check your plot
  after execution.
\item
  You can use \texttt{paste()} as `text' argument in mtext() try that,
  you need to re-plot. your plot first. \textbf{HINT:}
  \texttt{mtext(side=3,text=paste(...))} See what \texttt{paste()} is
  used for.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{"Text"}\NormalTok{,}\StringTok{"here"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Text here"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myText=}\KeywordTok{paste}\NormalTok{(}\StringTok{"Text"}\NormalTok{,}\StringTok{"here"}\NormalTok{)}
\NormalTok{myText}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Text here"
\end{verbatim}

Use \emph{mtext()} and \emph{paste()} to put a margin text on the plot.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\tightlist
\item
  \texttt{cor()} calculates correlation between two vectors. Pearson
  correlation is a measure of the linear correlation (dependence)
  between two variables X and Y.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corxy=}\KeywordTok{cor}\NormalTok{(x1,y1) }\CommentTok{# calculates pearson correlation}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\item
  Try use \texttt{mtext()},\texttt{cor()} and \texttt{paste()} to
  display correlation coefficient on your scatterplot ?
\item
  Change the colors of your plot using \texttt{col} argument. Ex:
  \texttt{plot(x,y,col="red")}
\item
  Use \texttt{pch=19} as an argument in your \texttt{plot()} command.
\item
  Use \texttt{pch=18} as an argument to your \texttt{plot()} command.
\item
  Make histogram of x1 with \texttt{hist()} function.Histogram is a
  graphical representation of the data distribution.
\item
  You can change colors with `col', add labels with `xlab', `ylab', and
  add a `title' with `main' arguments. Try all these in a histogram.
\item
  Make boxplot of y1 with \texttt{boxplot()}.
\item
  Make boxplots of \texttt{x1} and \texttt{y1} vectors in the same plot.
\item
  In boxplot use horizontal = TRUE argument
\item
  make multiple plots with par(mfrow=c(2,1))

  \begin{itemize}
  \tightlist
  \item
    run par(mfrow=c(2,1))
  \item
    make a boxplot
  \item
    make a histogram
  \end{itemize}
\item
  Do the same as above but this time with par(mfrow=c(1,2))
\item
  Save your plot using ``Export'' button in Rstudio
\item
  Save your plot by running :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dev.copy}\NormalTok{(pdf,}\DataTypeTok{file=}\StringTok{"plot.file.pdf"}\NormalTok{);}\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\tightlist
\item
  Save your plot running :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dev.copy}\NormalTok{(png,}\DataTypeTok{filename=}\StringTok{"plot.file.png"}\NormalTok{);}\KeywordTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\item
  Another way to save the plot is the following
\item
  Open a graphics device
\item
  Create the plot
\item
  Close the graphics device
\item
  \textbf{EXTRA: Making color density scatterplot}. You can make a
  scatter plot showing density of points rather than points themselves.
  If you use points it looks like this:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x2=}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\OperatorTok{+}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{0}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{200}\NormalTok{)}
\NormalTok{y2=}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}
\KeywordTok{plot}\NormalTok{(x2,y2,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-115-1} \end{center}

If you use smoothScatter() function, you get the densities.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{smoothScatter}\NormalTok{(x2,y2,}\DataTypeTok{colramp=}\KeywordTok{colorRampPalette}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"white"}\NormalTok{,}\StringTok{"blue"}\NormalTok{, }\StringTok{"green"}\NormalTok{,}\StringTok{"yellow"}\NormalTok{,}\StringTok{"red"}\NormalTok{))) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-116-1} \end{center}

Now, plot with \texttt{colramp=heat.colors} argument and then use a
custom color scale using the following argument.

\begin{verbatim}
colramp = colorRampPalette(c("white","blue", "green","yellow","red")))
\end{verbatim}

\hypertarget{functions-and-control-structures-for-ifelse-etc.-1}{%
\subsection{Functions and control structures (for, if/else
etc.)}\label{functions-and-control-structures-for-ifelse-etc.-1}}

Read CpG island data as shown below for the rest of the exercises.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpgtFilePath=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}
                \StringTok{"CpGi.table.hg18.txt"}\NormalTok{,}
                \DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{cpgi=}\KeywordTok{read.table}\NormalTok{(cpgtFilePath,}\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(cpgi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   chrom chromStart chromEnd     name length cpgNum
## 1  chr1      18598    19673 CpG: 116   1075    116
## 2  chr1     124987   125426  CpG: 30    439     30
## 3  chr1     317653   318092  CpG: 29    439     29
## 4  chr1     427014   428027  CpG: 84   1013     84
## 5  chr1     439136   440407  CpG: 99   1271     99
## 6  chr1     523082   523977  CpG: 94    895     94
##   gcNum perCpg perGc obsExp
## 1   787   21.6  73.2   0.83
## 2   295   13.7  67.2   0.64
## 3   295   13.2  67.2   0.62
## 4   734   16.6  72.5   0.64
## 5   777   15.6  61.1   0.84
## 6   570   21.0  63.7   1.04
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\item
  Check values at perGc column using a histogram. `perGc' stands for GC
  percent =\textgreater{} percentage of C+G nucleotides
\item
  Make a boxplot for `perGc' column
\item
  Use if/else structure to decide if given GC percent high, low or
  medium. If it is low, high, or medium. low \textless{} 60,
  high\textgreater{}75, medium is between 60 and 75 use greater or less
  than operators \textless{} or \textgreater{} . Fill in the values in
  the in code below, where it is written `YOU\_FILL\_IN'
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GCper=}\DecValTok{65}

  \CommentTok{# check if GC value is lower than 60, }
  \CommentTok{# assign "low" to result}
  \ControlFlowTok{if}\NormalTok{(}\StringTok{'YOU_FILL_IN'}\NormalTok{)\{}
\NormalTok{    result=}\StringTok{"low"}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"low"}\NormalTok{)}
\NormalTok{  \}}
  \ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(}\StringTok{'YOU_FILL_IN'}\NormalTok{)\{  }\CommentTok{# check if GC value is higher than 75, assign "high" to result}
\NormalTok{    result=}\StringTok{"high"}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"high"}\NormalTok{)}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{ }\CommentTok{# if those two conditions fail then it must be "medium"}
\NormalTok{    result=}\StringTok{"medium"}
\NormalTok{  \}}

\NormalTok{result}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\tightlist
\item
  Write a function that takes a value of GC percent and decides if it is
  low, high, or medium. low \textless{} 60, high\textgreater{}75, medium
  is between 60 and 75. Fill in the values in the in code below, where
  it is written `YOU\_FILL\_IN'
\end{enumerate}

\begin{verbatim}
GCclass<-function(my.gc){
  
  YOU_FILL_IN
  
  return(result)
}
GCclass(10) # should return "low"
GCclass(90) # should return "high"
GCclass(65) # should return "medium"
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\tightlist
\item
  Use a for loop to get GC percentage classes for gcValues below. Use
  the function you wrote above.
\end{enumerate}

\begin{verbatim}
gcValues=c(10,50,70,65,90)
for( i in YOU_FILL_IN){
  YOU_FILL_IN
}
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\tightlist
\item
  Use \texttt{lapply} to get to get GC percentage classes for gcValues.
  Example:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vec=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\NormalTok{power2=}\ControlFlowTok{function}\NormalTok{(x)\{ }\KeywordTok{return}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{)  \}}
    \KeywordTok{lapply}\NormalTok{(vec,power2)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{43}
\item
  Use sapply to get values to get GC percentage classes for gcValues
\item
  Is there a way to decide on the GC percentage class of given vector of
  GCpercentages without using if/else structure and loops ? if so, how
  can you do it? \textbf{HINT:} subsetting using \textless{} and
  \textgreater{} operators
\end{enumerate}

\hypertarget{stats}{%
\chapter{Statistics and Exploratory Data Analysis for
Genomics}\label{stats}}

This chapter will summarize statistics and exploratory data analysis
methods frequently used in computational genomics. As these fields are
continuously evolving, the techniques introduced here do not form an
exhaustive list but mostly corner stone methods that are often and still
being used. In addition, we focused on giving intuitive and practical
understanding of the methods with relevant examples from the field.\\
If you want to dig deeper into statistics and math, beyond what is
described here, we included appropriate references with annotation after
each major section.

\hypertarget{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions}{%
\section{How to summarize collection of data points: The idea behind
statistical
distributions}\label{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions}}

In biology and many other fields data is collected via experimentation.
The nature of the experiments and natural variation in biology makes it
impossible to get the same exact measurements every time you measure
something. For example, if you are measuring gene expression values for
a certain gene, say PAX6, and let's assume you are measuring expression
per sample and cell with any method( microarrays, rt-qPCR, etc.). You
will not get the same expression value even if your samples are
homogeneous. Due to technical bias in experiments or natural variation
in the samples. Instead, we would like to describe this collection of
data some other way that represents the general properties of the data.
The figure shows a sample of 20 expression values from PAX6 gene.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-128-1} 

}

\caption{Expression of PAX6 gene in 20 replicate experiments}\label{fig:unnamed-chunk-128}
\end{figure}

\hypertarget{describing-the-central-tendency-mean-and-median}{%
\subsection{Describing the central tendency: mean and
median}\label{describing-the-central-tendency-mean-and-median}}

As seen in the figure above, the points from this sample are distributed
around a central value and the histogram below the dot plot shows number
of points in each bin. Another observation is that there are some bins
that have more points than others. If we want to summarize what we
observe, we can try to represent the collection of data points with an
expression value that is typical to get, something that represents the
general tendency we observe on the dot plot and the histogram. This
value is sometimes called central value or central tendency, and there
are different ways to calculate such a value. In the figure above, we
see that all the values are spread around 6.13 (red line), and that is
indeed what we call mean value of this sample of expression values. It
can be calculated with the following formula
\(\overline{X}=\sum_{i=1}^n x_i/n\), where \(x_i\) is the expression
value of an experiment and \(n\) is the number of expression value
obtained from the experiments. In R, \texttt{mean()} function will
calculate the mean of a provided vector of numbers. This is called a
``sample mean''. In reality the possible values of PAX6 expression for
all cells (provided each cell is of the identical cell type and is in
identical conditions) are much much more than 20. If we had the time and
the funding to sample all cells and measure PAX6 expression we would get
a collection values that would be called, in statistics, a
``population''. In our case the population will look like the left hand
side of the figure below. What we have done with our 20 data points is
that we took a sample of PAX6 expression values from this population,
and calculated the sample mean.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-129-1} 

}

\caption{Expression of all possible PAX6 gene expressions measures on all available biological samples (left). Expression of PAX6 gene from statistical sample, a random subset, from the population of biological samples (Right). }\label{fig:unnamed-chunk-129}
\end{figure}

The mean of the population is calculated the same way but traditionally
Greek letter \(\mu\) is used to denote the population mean. Normally, we
would not have access to the population and we will use sample mean and
other quantities derived from the sample to estimate the population
properties. This is the basic idea behind statistical inference which we
will see this in action in later sections as well. We estimate the
population parameters from the sample parameters and there is some
uncertainty associated with those estimates. We will be trying to assess
those uncertainties and make decisions in the presence of those
uncertainties.

We are not yet done with measuring central tendency. There are other
ways to describe it, such as the median value. Mean can be affected by
outliers easily. If certain values are very high or low from the bulk of
the sample this will shift mean towards those outliers. However, median
is not affected by outliers. It is simply the value in a distribution
where half of the values are above and the other half is below. In R,
\texttt{median()} function will calculate the mean of a provided vector
of numbers.

Let's create a set of random numbers and calculate their mean and median
using R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#create 10 random numbers from uniform distribution }
\NormalTok{x=}\KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{# calculate mean}
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3739
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate median}
\KeywordTok{median}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3278
\end{verbatim}

\hypertarget{describing-the-spread-measurements-of-variation}{%
\subsection{Describing the spread: measurements of
variation}\label{describing-the-spread-measurements-of-variation}}

Another useful way to summarize a collection of data points is to
measure how variable the values are. You can simply describe the range
of the values , such as minimum and maximum values. You can easily do
that in R with \texttt{range()} function. A more common way to calculate
variation is by calculating something called ``standard deviation'' or
the related quantity called ``variance''. This is a quantity that shows
how variable the values are, a value around zero indicates there is not
much variation in the values of the data points, and a high value
indicates high variation in the values. The variance is the squared
distance of data points from the mean. Population variance is again a
quantity we usually do not have access to and is simply calculate as
follows \(\sigma^2=\sum_{i=1}^n \frac{(x_i-\mu)^2}{n}\), where \(\mu\)
is the population mean, \(x_i\) is the ith data point in the population
and \(n\) is the population size. However, when the we have only access
to a sample this formulation is biased. It means that it underestimates
the population variance, so we make a small adjustment when we calculate
the sample variance, denoted as \(s^2\): \[
\begin{aligned}
s^2=\sum_{i=1}^n \frac{(x_i-\overline{X})^2}{n-1} && \text{ where $x_i$ is the ith data point and
$\overline{X}$ is the sample mean.}
\end{aligned}
\]

The sample standard deviation is simply the square-root of the sample
variance. The good thing about standard deviation is that it has the
same unit as the mean so it is more intuitive.\\
\[s=\sqrt{\sum_{i=1}^n \frac{(x_i-\overline{X})^2}{n-1}}\]

We can calculate sample standard deviation and variation with
\texttt{sd()} and \texttt{var()} functions in R. These functions take
vector of numeric values as input and calculate the desired quantities.
Below we use those functions on a randomly generated vector of numbers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{6}\NormalTok{,}\DataTypeTok{sd=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{var}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2531
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5031
\end{verbatim}

One potential problem with the variance is that it could be affected by
outliers. The points that are too far away from the mean will have a
large affect on the variance even though there might be few of them. A
way to measure variance that could be less affected by outliers is
looking at where bulk of the distribution is. How do we define where the
bulk is? One common way is to look at the the difference between 75th
percentile and 25th percentile, this effectively removes a lot of
potential outliers which will be towards the edges of the range of
values. This is called interquartile range , and can be easily
calculated using R via \texttt{IQR()} function and the quantiles of a
vector is calculated with \texttt{quantile()} function.

Let us plot the boxplot for a random vector and also calculate IQR using
R. In the boxplot below, 25th and 75th percentiles are the edges of the
box, and the median is marked with a thick line going through roughly
middle the box.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{6}\NormalTok{,}\DataTypeTok{sd=}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{IQR}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5011
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{quantile}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    0%   25%   50%   75%  100% 
## 5.437 5.743 5.860 6.244 6.558
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(x,}\DataTypeTok{horizontal =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-134-1} 

}

\caption{Boxplot showing 25th percentile and 75th percentile and median for a set of points sample from a normal distribution with mean=6 and standard deviation=0.7}\label{fig:unnamed-chunk-134}
\end{figure}

\hypertarget{frequently-used-statistical-distributions}{%
\subsubsection{Frequently used statistical
distributions}\label{frequently-used-statistical-distributions}}

The distributions have parameters (such as mean and variance) that
summarizes them but also they are functions that assigns each outcome of
a statistical experiment to its probability of occurrence. One
distribution that you will frequently encounter is the normal
distribution or Gaussian distribution. The normal distribution has a
typical ``bell-curve'' shape and, characterized by mean and standard
deviation. A set of data points that follow normal distribution mostly
will be close to the mean but spread around it controlled by the
standard deviation parameter. That means if we sample data points from a
normal distribution we are more likely to sample nearby the mean and
sometimes away from the mean. Probability of an event occurring is
higher if it is nearby the mean. The effect of the parameters for normal
distribution can be observed in the following plot.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-135-1} 

}

\caption{Different parameters for normal distribution and effect of those on the shape of the distribution}\label{fig:unnamed-chunk-135}
\end{figure}

The normal distribution is often denoted by
\(\mathcal{N}(\mu,\,\sigma^2)\) When a random variable \(X\) is
distributed normally with mean \(\mu\) and variance \(\sigma^2\), we
write:

\[X\ \sim\ \mathcal{N}(\mu,\,\sigma^2).\]

The probability density function of Normal distribution with mean
\(\mu\) and standard deviation \(\sigma\) is as follows

\[P(x)=\frac{1}{\sigma\sqrt{2\pi} } \; e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \]

The probability density function gives the probability of observing a
value on a normal distribution defined by \(\mu\) and \(\sigma\)
parameters.

Often times, we do not need the exact probability of a value but we need
the probability of observing a value larger or smaller than a critical
value or reference point. For example, we might want to know the
probability of \(X\) being smaller than or equal to -2 for a normal
distribution with mean 0 and standard deviation 2.
,\(P(X <= -2 \; | \; \mu=0,\sigma=2)\). In this case, what we want is
the are under the curve shaded in blue. To be able to that we need to
integrate the probability density function but we will usually let
software do that. Traditionally, one calculates a Z-score which is
simply \((X-\mu)/\sigma=(-2-0)/2= -1\), and corresponds to how many
standard deviations you are away from the mean. This is also called
``standardization'', the corresponding value is distributed in
``standard normal distribution'' where \(\mathcal{N}(0,\,1)\).

After calculating the Z-score, we can go look up in a table, that
contains the area under the curve for the left and right side of the
Z-score, but again we use software for that tables are outdated.

Below we are showing the Z-score and the associated probabilities
derived from the calculation above for
\(P(X <= -2 \; | \; \mu=0,\sigma=2)\).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/zscore-1} 

}

\caption{Z-score and associated probabilities for Z= -1}\label{fig:zscore}
\end{figure}

In R, family of \texttt{*norm} functions
(\texttt{rnorm},\texttt{dnorm},\texttt{qnorm} and \texttt{pnorm}) can be
used to operate with normal distribution, such as calculating
probabilities and generating random numbers drawn from normal
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the value of probability density function when X= -2,}
\CommentTok{# where mean=0 and sd=2}
\KeywordTok{dnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.121
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the probability of P(X =< -2) where mean=0 and sd=2}
\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1587
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the probability of P(X > -2) where mean=0 and sd=2}
\KeywordTok{pnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{,}\DataTypeTok{lower.tail =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8413
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get 5 random numbers from normal dist with  mean=0 and sd=2}
\KeywordTok{rnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{ , }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1.8109 -1.9221 -0.5147  0.8217 -0.7901
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get y value corresponding to P(X > y) = 0.15 with  mean=0 and sd=2}
\KeywordTok{qnorm}\NormalTok{( }\FloatTok{0.15}\NormalTok{, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{ , }\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.073
\end{verbatim}

There are many other distribution functions in R that can be used the
same way. You have to enter the distribution specific parameters along
with your critical value, quantiles or number of random numbers
depending on which function you are using in the family.We will list
some of those functions below.

\begin{itemize}
\item
  \texttt{dbinom} is for binomial distribution. This distribution is
  usually used to model fractional data and binary data. Examples from
  genomics includes methylation data.
\item
  \texttt{dpois} is used for Poisson distribution and \texttt{dnbinom}
  is used for negative binomial distribution. These distributions are
  used to model count data such as sequencing read counts.
\item
  \texttt{df} (F distribution) and \texttt{dchisq} (Chi-Squared
  distribution) are used in relation to distribution of variation. F
  distribution is used to model ratios of variation and Chi-Squared
  distribution is used to model distribution of variations. You will
  frequently encounter these in linear models and generalized linear
  models.
\end{itemize}

\hypertarget{precision-of-estimates-confidence-intervals}{%
\subsection{Precision of estimates: Confidence
intervals}\label{precision-of-estimates-confidence-intervals}}

When we take a random sample from a population and compute a statistic,
such as the mean, we are trying to approximate the mean of the
population. How well this sample statistic estimates the population
value will always be a concern. A confidence interval addresses this
concern because it provides a range of values which is plausible to
contain the population parameter of interest. Normally, we would not
have access to a population. If we did, we would not have to estimate
the population parameters and its precision.

When we do not have access to the population, one way to estimate
intervals is to repeatedly take samples from the original sample with
replacement, that is we take a data point from the sample we replace,
and we take another data point until we have sample size of the original
sample. Then, we calculate the parameter of interest, in this case mean,
and repeat this step a large number of times, such as 1000. At this
point, we would have a distribution of re-sampled means, we can then
calculate the 2.5th and 97.5th percentiles and these will be our
so-called 95\% confidence interval. This procedure, resampling with
replacement to estimate the precision of population parameter estimates,
is known as the \textbf{bootstrap}.

Let's see how we can do this in practice. We simulate a sample coming
from a normal distribution (but we pretend we don't know the population
parameters). We will try to estimate the precision of the mean of the
sample using bootstrap to build confidence intervals.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{21}\NormalTok{)}
\NormalTok{sample1=}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{5}\NormalTok{) }\CommentTok{# simulate a sample}

\CommentTok{# do bootstrap resampling, sampling with replacement}
\NormalTok{boot.means=}\KeywordTok{do}\NormalTok{(}\DecValTok{1000}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{resample}\NormalTok{(sample1))}

\CommentTok{# get percentiles from the bootstrap means}
\NormalTok{q=}\KeywordTok{quantile}\NormalTok{(boot.means[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{p=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{,}\FloatTok{0.975}\NormalTok{))}

\CommentTok{# plot the histogram}
\KeywordTok{hist}\NormalTok{(boot.means[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{,}
                    \DataTypeTok{xlab=}\StringTok{"sample means"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{c}\NormalTok{(q[}\DecValTok{1}\NormalTok{], q[}\DecValTok{2}\NormalTok{] ),}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\NormalTok{q[}\DecValTok{1}\NormalTok{],}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\KeywordTok{round}\NormalTok{(q[}\DecValTok{1}\NormalTok{],}\DecValTok{3}\NormalTok{),}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\NormalTok{q[}\DecValTok{2}\NormalTok{],}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\KeywordTok{round}\NormalTok{(q[}\DecValTok{2}\NormalTok{],}\DecValTok{3}\NormalTok{),}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-137-1} 

}

\caption{Precision estimate of the sample mean using 1000 bootstrap samples. Confidence intervals derived from the bootstrap samples are shown with red lines.}\label{fig:unnamed-chunk-137}
\end{figure}

If we had a convenient mathematical method to calculate confidence
interval we could also do without resampling methods. It turns out that
if we take repeated samples from a population of with sample size \(n\),
the distribution of means ( \(\overline{X}\)) of those samples will be
approximately normal with mean \(\mu\) and standard deviation
\(\sigma/\sqrt{n}\). This is also known as \textbf{Central Limit
Theorem(CLT)} and is one of the most important theorems in statistics.
This also means that \(\frac{\overline{X}-\mu}{\sigma\sqrt{n}}\) has a
standard normal distribution and we can calculate the Z-score and then
we can get the percentiles associated with the Z-score. Below, we are
showing the Z-score calculation for the distribution of
\(\overline{X}\), and then we are deriving the confidence intervals
starting with the fact that probability of Z being between -1.96 and
1.96 is 0.95. We then use algebra to show that the probability that
unknown \(\mu\) is captured between \(\overline{X}-1.96\sigma/\sqrt{n}\)
and \(\overline{X}+1.96\sigma/\sqrt{n}\) is 0.95, which is commonly
known as 95\% confidence interval.

\[\begin{array}{ccc}
Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\\
P(-1.96 < Z < 1.96)=0.95 \\
P(-1.96 < \frac{\overline{X}-\mu}{\sigma/\sqrt{n}} < 1.96)=0.95\\
P(\mu-1.96\sigma/\sqrt{n} < \overline{X} < \mu+1.96\sigma/\sqrt{n})=0.95\\
P(\overline{X}-1.96\sigma/\sqrt{n} < \mu < \overline{X}+1.96\sigma/\sqrt{n})=0.95\\
confint=[\overline{X}-1.96\sigma/\sqrt{n},\overline{X}+1.96\sigma/\sqrt{n}]
\end{array}\]

A 95\% confidence interval for population mean is the most common common
interval to use, and would mean that we would expect 95\% of the
interval estimates to include the population parameter, in this case the
mean. However, we can pick any value such as 99\% or 90\%. We can
generalize the confidence interval for \(100(1-\alpha)\) as follows:

\[\overline{X} \pm Z_{\alpha/2}\sigma/\sqrt{n}\]

In R, we can do this using \texttt{qnorm()} function to get Z-scores
associated with \({\alpha/2}\) and \({1-\alpha/2}\). As you can see, the
confidence intervals we calculated using CLT are very similar to the
ones we got from bootstrap for the same sample. For bootstrap we got
\([19.21, 21.989]\) and for the CLT based estimate we got
\([19.23638, 22.00819]\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha=}\FloatTok{0.05}
\NormalTok{sd=}\DecValTok{5}
\NormalTok{n=}\DecValTok{50}
\KeywordTok{mean}\NormalTok{(sample1)}\OperatorTok{+}\KeywordTok{qnorm}\NormalTok{(}\KeywordTok{c}\NormalTok{(alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{,}\DecValTok{1}\OperatorTok{-}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}\OperatorTok{*}\NormalTok{sd}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 19.24 22.01
\end{verbatim}

The good thing about CLT as long as the sample size is large regardless
of the population distribution, the distribution of sample means drawn
from that population will always be normal. Here we are repeatedly
drawing samples 1000 times with sample size \(n\)=10,30, and 100 from a
bimodal, exponential and a uniform distribution and we are getting
sample mean distributions following normal distribution.

\begin{figure}

{\centering \includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-139-1} 

}

\caption{Sample means are normally distributed regardless of the population distribution they are drawn from.}\label{fig:unnamed-chunk-139}
\end{figure}

However, we should note that how we constructed the confidence interval
using standard normal distribution, \(N(0,1)\), only works when the when
we know the population standard deviation. In reality, we usually have
only access to a sample and have no idea about the population standard
deviation. If this is the case we should use estimate the standard
deviation using sample standard deviation and use something called
\emph{t distribution} instead of standard normal distribution in our
interval calculation. Our confidence interval becomes
\(\overline{X} \pm t_{\alpha/2}s/\sqrt{n}\), with t distribution
parameter \(d.f=n-1\), since now the following quantity is t distributed
\(\frac{\overline{X}-\mu}{s/\sqrt{n}}\) instead of standard normal
distribution.

The t distribution is similar to standard normal distribution has mean 0
but its spread is larger than the normal distribution especially when
sample size is small, and has one parameter \(v\) for the degrees of
freedom, which is \(n-1\) in this case. Degrees of freedom is simply
number of data points minus number of parameters estimated. Here we are
estimating the mean from the data and the distribution is for the means,
therefore degrees of freedom is \(n-1\).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-140-1} 

}

\caption{Normal distribution and t distribution with different degrees of freedom. With increasing degrees of freedom, t distribution approximates the normal distribution better.}\label{fig:unnamed-chunk-140}
\end{figure}

\hypertarget{how-to-test-for-differences-between-samples}{%
\section{How to test for differences between
samples}\label{how-to-test-for-differences-between-samples}}

Often times we would want to compare sets of samples. Such comparisons
include if wild-type samples have different expression compared to
mutants or if healthy samples are different from disease samples in some
measurable feature (blood count, gene expression, methylation of certain
loci). Since there is variability in our measurements, we need to take
that into account when comparing the sets of samples. We can simply
subtract the means of two samples, but given the variability of
sampling, at the very least we need to decide a cutoff value for
differences of means, small differences of means can be explained by
random chance due to sampling. That means we need to compare the
difference we get to a value that is typical to get if the difference
between two group means were only due to sampling. If you followed the
logic above, here we actually introduced two core ideas of something
called ``hypothesis testing'', this is simply using statistics to
determine the probability that a given hypothesis (if two sample sets
are from the same population or not) is true. Formally, those two core
ideas are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decide on a hypothesis to test, often called ``null hypothesis''
  (\(H_0\)). In our case, the hypothesis is there is no difference
  between sets of samples. An the ``Alternative hypothesis'' (\(H_1\))
  is there is a difference between the samples.
\item
  Decide on a statistic to test the truth of the null hypothesis.
\item
  Calculate the statistic
\item
  Compare it to a reference value to establish significance, the
  P-value. Based on that either reject or not reject the null
  hypothesis, \(H_0\)
\end{enumerate}

\hypertarget{randomization-based-testing-for-difference-of-the-means}{%
\subsection{randomization based testing for difference of the
means}\label{randomization-based-testing-for-difference-of-the-means}}

There is one intuitive way to go about this. If we believe there are no
differences between samples that means the sample labels (test-control
or healthy-disease) has no meaning. So, if we randomly assign labels to
the samples that and calculate the difference of the mean, this creates
a null distribution for the \(H_0\) where we can compare the real
difference and measure how unlikely it is to get such a value under the
expectation of the null hypothesis. We can calculate all possible
permutations to calculate the null distribution. However, sometimes that
is not very feasible and equivalent approach would be generating the
null distribution by taking a smaller number of random samples with
shuffled group membership.

Below, we are doing this process in R. We are first simulating two
samples from two different distributions. These would be equivalent to
gene expression measurements obtained under different conditions. Then,
we calculate the differences in the means and do the randomization
procedure to get a null distribution when we assume there is no
difference between samples, \(H_0\). We then calculate how often we
would get the original difference we calculated under the assumption
that \(H_0\) is true.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{2}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{org.diff=}\KeywordTok{mean}\NormalTok{(gene1)}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(gene2)}
\NormalTok{gene.df=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{exp=}\KeywordTok{c}\NormalTok{(gene1,gene2),}
                  \DataTypeTok{group=}\KeywordTok{c}\NormalTok{( }\KeywordTok{rep}\NormalTok{(}\StringTok{"test"}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"control"}\NormalTok{,}\DecValTok{30}\NormalTok{) ) )}


\NormalTok{exp.null <-}\StringTok{ }\KeywordTok{do}\NormalTok{(}\DecValTok{1000}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{diff}\NormalTok{(mosaic}\OperatorTok{::}\KeywordTok{mean}\NormalTok{(exp }\OperatorTok{~}\StringTok{ }\KeywordTok{shuffle}\NormalTok{(group), }\DataTypeTok{data=}\NormalTok{gene.df))}
\KeywordTok{hist}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{xlab=}\StringTok{"null distribution | no difference in samples"}\NormalTok{,}
     \DataTypeTok{main=}\KeywordTok{expression}\NormalTok{(}\KeywordTok{paste}\NormalTok{(H[}\DecValTok{0}\NormalTok{],}\StringTok{" :no difference in means"}\NormalTok{) ),}
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\KeywordTok{quantile}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{],}\FloatTok{0.95}\NormalTok{),}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{ )}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{org.diff,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{ )}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{quantile}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{],}\FloatTok{0.95}\NormalTok{),}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\StringTok{"0.05"}\NormalTok{,}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\DataTypeTok{x=}\NormalTok{org.diff,}\DataTypeTok{y=}\DecValTok{200}\NormalTok{,}\StringTok{"org. diff."}\NormalTok{,}\DataTypeTok{adj=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{),}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-141-1} 

}

\caption{The null distribution for differences of means obtained via randomization. The original difference is marked via blue line. The red line marks the value that corresponds to P-value of 0.05}\label{fig:unnamed-chunk-141}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p.val=}\KeywordTok{sum}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{]}\OperatorTok{>}\NormalTok{org.diff)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(exp.null[,}\DecValTok{1}\NormalTok{])}
\NormalTok{p.val}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

After doing random permutations and getting a null distribution, it is
possible to get a confidence interval for the distribution of difference
in means. This is simply the 2.5th and 97.5th percentiles of the null
distribution, and directly related to the P-value calculation above.

\hypertarget{using-t-test-for-difference-of-the-means-between-two-samples}{%
\subsection{Using t-test for difference of the means between two
samples}\label{using-t-test-for-difference-of-the-means-between-two-samples}}

We can also calculate the difference between means using a t-test.
Sometimes we will have too few data points in a sample to do meaningful
randomization test, also randomization takes more time than doing a
t-test. This is a test that depends on the t distribution. The line of
thought follows from the CLT and we can show differences in means are t
distributed. There are couple of variants of the t-test for this
purpose. If we assume the variances are equal we can use the following
version

\[t = \frac{\bar {X}_1 - \bar{X}_2}{s_{X_1X_2} \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\]
where
\[s_{X_1X_2} = \sqrt{\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}}\]
In the first equation above the quantity is t distributed with
\(n_1+n_2-2\) degrees of freedom. We can calculate the quantity then use
software to look for the percentile of that value in that t
distribution, which is our P-value. When we can not assume equal
variances we use ``Welch's t-test'' which is the default t-test in R and
also works well when variances and the sample sizes are the same. For
this test we calculate the following quantity:

\[t = {\overline{X}_1 - \overline{X}_2 \over s_{\overline{X}_1 - \overline{X}_2}}\]
where
\[s_{\overline{X}_1 - \overline{X}_2} = \sqrt{{s_1^2 \over n_1} + {s_2^2  \over n_2}}
\] and the degrees of freedom equals to:
\[\mathrm{d.f.} = \frac{(s_1^2/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1) + (s_2^2/n_2)^2/(n_2-1)}
\]

Luckily, R does all those calculations for us. Below we will show the
use of \texttt{t.test()} function in R. We will use it on the samples we
simulated above.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Welch's t-test}
\NormalTok{stats}\OperatorTok{::}\KeywordTok{t.test}\NormalTok{(gene1,gene2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  gene1 and gene2
## t = 3.8, df = 48, p-value = 5e-04
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.8724 2.8728
## sample estimates:
## mean of x mean of y 
##     4.058     2.185
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# t-test with equal varience assumption}
\NormalTok{stats}\OperatorTok{::}\KeywordTok{t.test}\NormalTok{(gene1,gene2,}\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  gene1 and gene2
## t = 3.8, df = 58, p-value = 4e-04
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.8771 2.8681
## sample estimates:
## mean of x mean of y 
##     4.058     2.185
\end{verbatim}

A final word on t-tests: they generally assume population where samples
coming from have normal distribution, however it is been shown t-test
can tolerate deviations from normality. Especially, when two
distributions are moderately skewed in the same direction. This is due
to central limit theorem which says means of samples will be distributed
normally no matter the population distribution if sample sizes are
large.

\hypertarget{multiple-testing-correction}{%
\subsection{multiple testing
correction}\label{multiple-testing-correction}}

We should think of hypothesis testing as a non-error-free method of
making decisions. There will be times when we declare something
significant and accept \(H_1\) but we will be wrong. These decisions are
also called ``false positives'' or ``false discoveries'', this is also
known as ``type I error''. Similarly, we can fail to reject a hypothesis
when we actually should. These cases are known as ``false negatives'',
also known as ``type II error''.

The ratio of true negatives to the sum of true negatives and false
positives (\(\frac{TN}{FP+TN}\)) is known as specificity. And we usually
want to decrease the FP and get higher specificity. The ratio of true
positives to the sum of true positives and false negatives
(\(\frac{TP}{TP+FN}\)) is known as sensitivity. And, again we usually
want to decrease the FN and get higher sensitivity. Sensitivity is also
known as ``power of a test'' in the context of hypothesis testing. More
powerful tests will be highly sensitive and will do less type II errors.
For the t-test the power is positively associated with sample size and
the effect size. Higher the sample size, smaller the standard error and
looking for the larger effect sizes will similarly increase the power.

The general summary of these the different combination of the decisions
are included in the table below.

\begin{longtable}[]{@{}lccl@{}}
\toprule
\begin{minipage}[b]{0.17\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
\(H_0\) is TRUE, {[}Gene is NOT differentially expressed{]}\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\centering
\(H_1\) is TRUE, {[}Gene is differentially expressed{]}\strut
\end{minipage} & \begin{minipage}[b]{0.27\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.17\columnwidth}\raggedright
Accept \(H_0\) (claim that the gene is not differentially
expressed)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
True Negatives (TN)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
False Negatives (FN) ,type II error\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
\(m_0\): number of truly null hypotheses\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.17\columnwidth}\raggedright
reject \(H_0\) (claim that the gene is differentially expressed)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
False Positives (FP) ,type I error\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\centering
True Positives (TP)\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
\(m-m_0\): number of truly alternative hypotheses\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

We expect to make more type I errors as the number of tests increase,
that means we will reject the null hypothesis by mistake. For example,
if we perform a test the 5\% significance level, there is a 5\% chance
of incorrectly rejecting the null hypothesis if the null hypothesis is
true. However, if we make 1000 tests where all null hypotheses are true
for each of them, the average number of incorrect rejections is 50. And
if we apply the rules of probability, there are is almost a 100\% chance
that we will have at least one incorrect rejection. There are multiple
statistical techniques to prevent this from happening. These techniques
generally shrink the P-values obtained from multiple tests to higher
values, if the individual P-value is low enough it survives this
process. The most simple method is just to multiply the individual,
P-value (\(p_i\)) with the number of tests (\(m\)): \(m \cdot p_i\),
this is called ``Bonferroni correction''. However, this is too harsh if
you have thousands of tests. Other methods are developed to remedy this.
Those methods rely on ranking the P-values and dividing \(m \cdot p_i\)
by the rank,\(i\), :\(\frac{m \cdot p_i }{i}\), this is derived from
Benjamini--Hochberg procedure. This procedure is developed to control
for ``False Discovery Rate (FDR)'' , which is proportion of false
positives among all significant tests. And in practical terms, we get
the ``FDR adjusted P-value'' from the procedure described above. This
gives us an estimate of proportion of false discoveries for a given
test. To elaborate, p-value of 0.05 implies that 5\% of all tests will
be false positives. An FDR adjusted p-value of 0.05 implies that 5\% of
significant tests will be false positives. The FDR adjusted P-values
will result in a lower number of false positives.

One final method that is also popular is called the ``q-value'' method
and related to the method above. This procedure relies on estimating the
proportion of true null hypotheses from the distribution of raw p-values
and using that quantity to come up with what is called a ``q-value'',
which is also an FDR adjusted P-value . That can be practically defined
as ``the proportion of significant features that turn out to be false
leads.'' A q-value 0.01 would mean 1\% of the tests called significant
at this level will be truly null on average. Within the genomics
community q-value and FDR adjusted P-value are synonymous although they
can be calculated differently.

In R, the base function \texttt{p.adjust()} implements most of the
p-value correction methods described above. For the q-value, we can use
the \texttt{qvalue} package from Bioconductor. Below we are
demonstrating how to use them on a set of simulated p-values.The plot
shows that Bonferroni correction does a terrible job. FDR(BH) and
q-value approach are better but q-value approach is more permissive than
FDR(BH).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(qvalue)}
\KeywordTok{data}\NormalTok{(hedenfalk)}

\NormalTok{qvalues <-}\StringTok{ }\KeywordTok{qvalue}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p)}\OperatorTok{$}\NormalTok{q}
\NormalTok{bonf.pval=}\KeywordTok{p.adjust}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,}\DataTypeTok{method =}\StringTok{"bonferroni"}\NormalTok{)}
\NormalTok{fdr.adj.pval=}\KeywordTok{p.adjust}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,}\DataTypeTok{method =}\StringTok{"fdr"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,qvalues,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
     \DataTypeTok{xlab=}\StringTok{"raw P-values"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"adjusted P-values"}\NormalTok{)}
\KeywordTok{points}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,bonf.pval,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\KeywordTok{points}\NormalTok{(hedenfalk}\OperatorTok{$}\NormalTok{p,fdr.adj.pval,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{"q-value"}\NormalTok{,}\StringTok{"FDR (BH)"}\NormalTok{,}\StringTok{"Bonferroni"}\NormalTok{),}
       \DataTypeTok{fill=}\KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{,}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/multtest-1} 

}

\caption{Adjusted P-values via different methods and their relationship to raw P-values}\label{fig:multtest}
\end{figure}

\hypertarget{moderated-t-tests-using-information-from-multiple-comparisons}{%
\subsection{moderated t-tests: using information from multiple
comparisons}\label{moderated-t-tests-using-information-from-multiple-comparisons}}

In genomics, we usually do not do one test but many, as described above.
That means we may be able to use the information from the parameters
obtained from all comparisons to influence the individual parameters.
For example, if you have many variances calculated for thousands of
genes across samples, you can force individual variance estimates to
shrunk towards the mean or the median of the distribution of variances.
This usually creates better performance in individual variance estimates
and therefore better performance in significance testing which depends
on variance estimates. How much the values be shrunk towards a common
value comes in many flavors. These tests in general are called moderated
t-tests or shrinkage t-tests. One approach popularized by Limma software
is to use so-called ``Empirical Bayesian methods''. The main formulation
in these methods is \(\hat{V_g} = aV_0 + bV_g\), where \(V_0\) is the
background variability\\
and \(V_g\) is the individual variability. Then, these methods estimate
\(a\) and \(b\) in various ways to come up with shrunk version of
variability, \(\hat{V_g}\). In a Bayesian viewpoint, the prior knowledge
is used to calculate the variability of an individual gene. In this
case, \(V_0\) would be the prior knowledge we have on variability of the
genes and we use that knowledge to influence our estimate for the
individual genes.

Below we are simulating a gene expression matrix with 1000 genes, and 3
test and 3 control groups. Each row is a gene and in normal
circumstances we would like to find out differentially expressed genes.
In this case, we are simulating them from the same distribution so in
reality we do not expect any differences. We then use the adjusted
standard error estimates in empirical Bayesian spirit but in a very
crude way. We just shrink the gene-wise standard error estimates towards
the median with equal \(a\) and \(b\) weights. That is to say, we add
individual estimate to the median of standard error distribution from
all genes and divide that quantity by 2. So if we plug that in the to
the above formula what we do is:

\[ \hat{V_g} = (V_0 + V_g)/2 \]

In the code below, we are avoiding for loops or apply family functions
by using vectorized operations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{#sample data matrix from normal distribution}

\NormalTok{gset=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{3000}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{200}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{70}\NormalTok{)}
\NormalTok{data=}\KeywordTok{matrix}\NormalTok{(gset,}\DataTypeTok{ncol=}\DecValTok{6}\NormalTok{)}

\CommentTok{# set groups}
\NormalTok{group1=}\DecValTok{1}\OperatorTok{:}\DecValTok{3}
\NormalTok{group2=}\DecValTok{4}\OperatorTok{:}\DecValTok{6}
\NormalTok{n1=}\DecValTok{3}
\NormalTok{n2=}\DecValTok{3}
\NormalTok{dx=}\KeywordTok{rowMeans}\NormalTok{(data[,group1])}\OperatorTok{-}\KeywordTok{rowMeans}\NormalTok{(data[,group2])}
  
\KeywordTok{require}\NormalTok{(matrixStats)}

\CommentTok{# get the esimate of pooled variance }
\NormalTok{stderr <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{( (}\KeywordTok{rowVars}\NormalTok{(data[,group1])}\OperatorTok{*}\NormalTok{(n1}\DecValTok{-1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{rowVars}\NormalTok{(data[,group2])}\OperatorTok{*}\NormalTok{(n2}\DecValTok{-1}\NormalTok{)) }\OperatorTok{/}\StringTok{ }\NormalTok{(n1}\OperatorTok{+}\NormalTok{n2}\DecValTok{-2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{( }\DecValTok{1}\OperatorTok{/}\NormalTok{n1 }\OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{n2 ))}

\CommentTok{# do the shrinking towards median}
\NormalTok{mod.stderr <-}\StringTok{ }\NormalTok{(stderr }\OperatorTok{+}\StringTok{ }\KeywordTok{median}\NormalTok{(stderr)) }\OperatorTok{/}\StringTok{ }\DecValTok{2} \CommentTok{# moderation in variation}

\CommentTok{# esimate t statistic with moderated variance}
\NormalTok{t.mod =}\StringTok{ }\NormalTok{dx }\OperatorTok{/}\StringTok{ }\NormalTok{mod.stderr}

\CommentTok{# calculate P-value of rejecting null }
\NormalTok{p.mod =}\StringTok{ }\DecValTok{2}\OperatorTok{*}\KeywordTok{pt}\NormalTok{( }\OperatorTok{-}\KeywordTok{abs}\NormalTok{(t.mod), n1}\OperatorTok{+}\NormalTok{n2}\DecValTok{-2}\NormalTok{ )}

\CommentTok{# esimate t statistic without moderated variance}
\NormalTok{t =}\StringTok{ }\NormalTok{dx }\OperatorTok{/}\StringTok{ }\NormalTok{stderr}

\CommentTok{# calculate P-value of rejecting null }
\NormalTok{p =}\StringTok{ }\DecValTok{2}\OperatorTok{*}\KeywordTok{pt}\NormalTok{( }\OperatorTok{-}\KeywordTok{abs}\NormalTok{(t), n1}\OperatorTok{+}\NormalTok{n2}\DecValTok{-2}\NormalTok{ )}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(p,}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{,}\DataTypeTok{main=}\StringTok{""}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"P-values t-test"}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"signifcant tests:"}\NormalTok{,}\KeywordTok{sum}\NormalTok{(p}\OperatorTok{<}\FloatTok{0.05}\NormalTok{))  )}
\KeywordTok{hist}\NormalTok{(p.mod,}\DataTypeTok{col=}\StringTok{"cornflowerblue"}\NormalTok{,}\DataTypeTok{border=}\StringTok{"white"}\NormalTok{,}\DataTypeTok{main=}\StringTok{""}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"P-values mod. t-test"}\NormalTok{)}
\KeywordTok{mtext}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"signifcant tests:"}\NormalTok{,}\KeywordTok{sum}\NormalTok{(p.mod}\OperatorTok{<}\FloatTok{0.05}\NormalTok{))  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-142-1} 

}

\caption{The distributions of P-values obtained by t-tests and moderated t-tests}\label{fig:unnamed-chunk-142}
\end{figure}

\BeginKnitrBlock{rmdtip}
\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  basic statistical concepts

  \begin{itemize}
  \tightlist
  \item
    ``Cartoon guide to statistics'' by Gonick \& Smith
  \item
    ``Introduction to statistics'' by Mine Rundel, et al.~(Free e-book)
  \end{itemize}
\item
  Hands-on statistics recipes with R

  \begin{itemize}
  \tightlist
  \item
    ``The R book'' by Crawley
  \end{itemize}
\item
  moderated tests

  \begin{itemize}
  \tightlist
  \item
    comparison of moderated tests for differential expression
    \url{http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-17}
  \item
    limma method: Smyth, G. K. (2004). Linear models and empirical Bayes
    methods for assessing differential expression in microarray
    experiments. Statistical Applications in Genetics and Molecular
    Biology, 3, No.~1, Article 3.
    \url{http://www.statsci.org/smyth/pubs/ebayes.pdf}
  \end{itemize}
\end{itemize}
\EndKnitrBlock{rmdtip}

\hypertarget{relationship-between-variables-linear-models-and-correlation}{%
\section{Relationship between variables: linear models and
correlation}\label{relationship-between-variables-linear-models-and-correlation}}

In genomics, we would often need to measure or model the relationship
between variables. We might want to know about expression of a
particular gene in liver in relation to the dosage of a drug that
patient receives. Or, we may want to know DNA methylation of certain
locus in the genome in relation to age of the sample donor's. Or, we
might be interested in the relationship between histone modifications
and gene expression. Is there a linear relationship, the more histone
modification the more the gene is expressed ?

In these situations and many more, linear regression or linear models
can be used to model the relationship with a ``dependent'' or
``response'' variable (expression or methylation in the above examples)
and one or more ``independent''" or ``explanatory'' variables (age, drug
dosage or histone modification in the above examples). Our simple linear
model has the following components.

\[  Y= \beta_0+\beta_1X + \epsilon \]

In the equation above, \(Y\) is the response variable and \(X\) is the
explanatory variable. \(\epsilon\) is the mean-zero error term. Since,
the line fit will not be able to precisely predict the \(Y\) values,
there will be some error associated with each prediction when we compare
it to the original \(Y\) values. This error is captured in \(\epsilon\)
term. We can alternatively write the model as follows to emphasize that
the model approximates \(Y\), in this case notice that we removed the
\(\epsilon\) term: \(Y \sim \beta_0+\beta_1X\)

The graph below shows the relationship between histone modification
(trimethylated forms of histone H3 at lysine 4, aka H3K4me3) and gene
expression for 100 genes. The blue line is our model with estimated
coefficients (\(\hat{y}=\hat{\beta}_0 + \hat{\beta}_1X\), where
\(\hat{\beta}_0\) and \(\hat{\beta}_1\) the estimated values of
\(\beta_0\) and \(\beta_1\), and \(\hat{y}\) indicates the prediction).
The red lines indicate the individual errors per data point, indicated
as \(\epsilon\) in the formula above.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-143-1} 

}

\caption{Relationship between histone modification score and gene expression. Increasing histone modification, H3K4me3, seems to be associated with increasing gene expression. Each dot is a gene}\label{fig:unnamed-chunk-143}
\end{figure}

There could be more than one explanatory variable, we then simply add
more \(X\) and \(\beta\) to our model. If there are two explanatory
variables our model will look like this:

\[  Y= \beta_0+\beta_1X_1 +\beta_2X_2 + \epsilon \]

In this case, we will be fitting a plane rather than a line. However,
the fitting process which we will describe in the later sections will
not change. For our gene expression problem. We can introduce one more
histone modification, H3K27me3. We will then have a linear model with 2
explanatory variables and the fitted plane will look like the one below.
The gene expression values are shown as dots below and above the fitted
plane.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-144-1} 

}

\caption{Association of Gene expression with H3K4me3 and H27Kme3 histone modifications.}\label{fig:unnamed-chunk-144}
\end{figure}

\hypertarget{matrix-notation-for-linear-models}{%
\subsubsection{Matrix notation for linear
models}\label{matrix-notation-for-linear-models}}

We can naturally have more explanatory variables than just two.The
formula below has \(n\) explanatory variables.

\[Y= \beta_0+\beta_1X_1+\beta_2X_2 +  \beta_3X_3 + .. + \beta_nX_n +\epsilon\]

If there are many variables, it would be easier to write the model in
matrix notation. The matrix form of linear model with two explanatory
variables will look like the one below. First matrix would be our data
matrix. This contains our explanatory variables and a column of 1s. The
second term is a column vector of \(\beta\) values. We add a vector of
error terms,\(\epsilon\)s to the matrix multiplication.

\[
 \mathbf{Y} = \left[\begin{array}{rrr}
1 & X_{1,1} & X_{1,2} \\
1 & X_{2,1} & X_{2,2} \\
1 & X_{3,1} & X_{3,2} \\
1 & X_{4,1} & X_{4,2}
\end{array}\right]
%
\left[\begin{array}{rrr}
\beta_0 \\
\beta_1 \\
\beta_2 
\end{array}\right]
% 
+
\left[\begin{array}{rrr}
\epsilon_1 \\
\epsilon_2 \\ 
\epsilon_3 \\ 
\epsilon_0
\end{array}\right]
\]

The multiplication of data matrix and \(\beta\) vector and addition of
the error terms simply results in the the following set of equations per
data point:

\[
\begin{aligned}
Y_1= \beta_0+\beta_1X_{1,1}+\beta_2X_{1,2} +\epsilon_1 \\
Y_2= \beta_0+\beta_1X_{2,1}+\beta_2X_{2,2} +\epsilon_2 \\
Y_3= \beta_0+\beta_1X_{3,1}+\beta_2X_{3,2} +\epsilon_3 \\
Y_4= \beta_0+\beta_1X_{4,1}+\beta_2X_{4,2} +\epsilon_4 
\end{aligned}
\]

This expression involving the multiplication of the data matrix, the
\(\beta\) vector and vector of error terms (\(\epsilon\)) could be
simply written as follows.

\[Y=X\beta + \epsilon\]

In the equation above \(Y\) is the vector of response variables and
\(X\) is the data matrix and \(\beta\) is the vector of coefficients.
This notation is more concise and often used in scientific papers.
However, this also means you need some understanding of linear algebra
to follow the math laid out in such resources.

\hypertarget{how-to-fit-a-line}{%
\subsection{How to fit a line}\label{how-to-fit-a-line}}

At this point a major questions is left unanswered: How did we fit this
line? We basically need to define \(\beta\) values in a structured way.
There are multiple ways or understanding how to do this, all of which
converges to the same end point. We will describe them one by one.

\hypertarget{the-cost-or-loss-function-approach}{%
\subsubsection{The cost or loss function
approach}\label{the-cost-or-loss-function-approach}}

This is the first approach and in my opinion is easiest to understand.
We try to optimize a function, often called ``cost function'' or ``loss
function''. The cost function is the sum of squared differences between
the predicted \(\hat{Y}\) values from our model and the original \(Y\)
values. The optimization procedure tries to find \(\beta\) values that
minimizes this difference between reality and the predicted values.

\[min \sum{(y_i-(\beta_0+\beta_1x_i))^2}\]

Note that this is related to the the error term, \(\epsilon\), we
already mentioned above, we are trying to minimize the squared sum of
\(\epsilon_i\) for each data point. We can do this minimization by a bit
of calculus. The rough algorithm is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pick a random starting point, random \(\beta\) values
\item
  Take the partial derivatives of the cost function to see which
  direction is the way to go in the cost function.
\item
  Take a step toward the direction that minimizes the cost function.

  \begin{itemize}
  \tightlist
  \item
    step size is parameter to choose, there are many variants.
  \end{itemize}
\item
  repeat step 2,3 until convergence.
\end{enumerate}

This is the basis of ``gradient descent'' algorithm. With the help of
partial derivatives we define a ``gradient'' on the cost function and
follow that through multiple iterations and until convergence, meaning
until the results do not improve defined by a margin. The algorithm
usually converges to optimum \(\beta\) values. Below, we show the cost
function over various \(\beta_0\) and \(\beta_1\) values for the histone
modification and gene expression data set. The algorithm will pick a
point on this graph and traverse it incrementally based on the
derivatives and converge on the bottom of the cost function ``well''.

\begin{figure}

{\centering \includegraphics{compgenomrReloaded_files/figure-latex/3dcostfunc-1} 

}

\caption{Cost function landscape for linear regression with changing beta values. The optimization process tries to find the lowest point in this landscape by implementing a strategy for updating beta values towards the lowest point in the landscape.}\label{fig:3dcostfunc}
\end{figure}

\hypertarget{not-cost-function-but-maximum-likelihood-function}{%
\subsubsection{Not cost function but maximum likelihood
function}\label{not-cost-function-but-maximum-likelihood-function}}

We can also think of this problem from more a statistical point of view.
In essence, we are looking for best statistical parameters, in this case
\(\beta\) values, for our model that are most likely to produce such a
scatter of data points given the explanatory variables.This is called
``Maximum likelihood'' approach. Probability of observing a \(Y\) value,
given that the distribution of it on a given \(X\) value follows a
normal distribution with mean \(\beta_0+\beta_1x_i\) and variance
\(s^2\) , and is shown below. Note that this assumes variance is
constant and \(s^2=\frac{\sum{\epsilon_i}}{n-2}\) is an unbiased
estimation for population variance, \(\sigma^2\).

\[P(y_{i})=\frac{1}{s\sqrt{2\pi} }e^{-\frac{1}{2}\left(\frac{y_i-(\beta_0 + \beta_1x_i)}{s}\right)^2}\]

Following from this, then the likelihood function ,shown as \(L\) below,
for linear regression is multiplication of \(P(y_{i})\) for all data
points.

\[L=P(y_1)P(y_2)P(y_3)..P(y_n)=\prod\limits_{i=1}^n{P_i}\]

This can be simplified to this by some algebra and taking logs (since it
is easier to add than multiply)

\[ln(L) = -nln(s\sqrt{2\pi}) - \frac{1}{2s^2} \sum\limits_{i=1}^n{(y_i-(\beta_0 - \beta_1x_i))^2} \]

As you can see, the right part of the function is the negative of the
cost function defined above. If we wanted to optimize this function we
would need to take derivative of the function with respect to \(\beta\)
parameters. That means we can ignore the first part since there is no
\(\beta\) terms there. This simply reduces to the negative of the cost
function. Hence, this approach produces exactly the same result as the
cost function approach. The difference is that we defined our problem
within the domain of statistics. This particular function has still to
be optimized. This can be done with some calculus without the need for
an iterative approach.

\hypertarget{linear-algebra-and-closed-form-solution-to-linear-regression}{%
\subsubsection{Linear algebra and closed-form solution to linear
regression}\label{linear-algebra-and-closed-form-solution-to-linear-regression}}

The last approach we will describe is the minimization process using
linear algebra. If you find this concept challenging, feel free to skip
it but scientific publications and other books frequently use matrix
notation and linear algebra to define and solve regression problems. In
this case, we do not use an iterative approach. Instead, we will
minimize cost function by explicitly taking its derivatives with respect
to \(\beta\)'s and setting them to zero. This is doable by employing
linear algebra and matrix calculus. This approach is also called
``ordinary least squares''. We will not show the whole derivation here
but the following expression is what we are trying to minimize in matrix
notation, this is basically a different notation of the same
minimization problem defined above. Remember
\(\epsilon_i=Y_i-(\beta_0+\beta_1x_i)\)

\[
\begin{aligned}
\sum\epsilon_{i}^2=\epsilon^T\epsilon=(Y-{\beta}{X})^T(Y-{\beta}{X}) \\
=Y^T{Y}-2{\beta}^T{Y}+{\beta}^TX^TX{\beta}
\end{aligned}
\] After rearranging the terms, we take the derivative of
\(\epsilon^T\epsilon\) with respect to \(\beta\), and equalize that to
zero. We then arrive at the following for estimated \(\beta\) values,
\(\hat{\beta}\):

\[\hat{\beta}=(X^TX)^{-1}X^TY\]

This requires for you to calculate the inverse of the \(X^TX\) term,
which could be slow for large matrices. Iterative approach over the cost
function derivatives will be faster for larger problems. The linear
algebra notation is something you will see in the papers or other
resources often. If you input the data matrix X and solve the
\((X^TX)^{-1}\) , you get the following values for \(\beta_0\) and
\(\beta_1\) for simple regression . However, we should note that this
simple linear regression case can easily be solved algebraically without
the need for matrix operations. This can be done by taking the
derivative of \(\sum{(y_i-(\beta_0+\beta_1x_i))^2}\) with respect to
\(\beta_1\), rearranging the terms and equalizing the derivative to
zero.

\[\hat{\beta_1}=\frac{\sum{(x_i-\overline{X})(y_i-\overline{Y})}}{ \sum{(x_i-\overline{X})^2} }\]
\[\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}\]

\hypertarget{fitting-lines-in-r}{%
\subsubsection{Fitting lines in R}\label{fitting-lines-in-r}}

After all this theory, you will be surprised how easy it is to fit lines
in R. This is achieved just by \texttt{lm()} command, stands for linear
models. Let's do this for a simulated data set and plot the fit. First
step is to simulate the data, we will decide on \(\beta_0\) and
\(\beta_1\) values. The we will decide on the variance
parameter,\(\sigma\) to be used in simulation of error terms,
\(\epsilon\). We will first find \(Y\) values, just using the linear
equation \(Y=\beta0+\beta_1X\), for a set of \(X\) values. Then, we will
add the error terms get our simulated values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random number seed, so that the random numbers from the text}
\CommentTok{# is the same when you run the code.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{32}\NormalTok{)}

\CommentTok{# get 50 X values between 1 and 100}
\NormalTok{x =}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{)}

\CommentTok{# set b0,b1 and varience (sigma)}
\NormalTok{b0 =}\StringTok{ }\DecValTok{10}
\NormalTok{b1 =}\StringTok{ }\DecValTok{2}
\NormalTok{sigma =}\StringTok{ }\DecValTok{20}
\CommentTok{# simulate error terms from normal distribution}
\NormalTok{eps =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{0}\NormalTok{,sigma)}
\CommentTok{# get y values from the linear equation and addition of error terms}
\NormalTok{y =}\StringTok{ }\NormalTok{b0 }\OperatorTok{+}\StringTok{ }\NormalTok{b1}\OperatorTok{*}\NormalTok{x}\OperatorTok{+}\StringTok{ }\NormalTok{eps}
\end{Highlighting}
\end{Shaded}

Now let us fit a line using lm() function. The function requires a
formula, and optionally a data frame. We need the pass the following
expression within the lm function, \texttt{y\textasciitilde{}x}, where
\texttt{y} is the simulated \(Y\) values and \texttt{x} is the
explanatory variables \(X\). We will then use \texttt{abline()} function
to draw the fit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1=}\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x)}

\CommentTok{# plot the data points}
\KeywordTok{plot}\NormalTok{(x,y,}\DataTypeTok{pch=}\DecValTok{20}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"Gene Expression"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"Histone modification score"}\NormalTok{)}
\CommentTok{# plot the linear fit}
\KeywordTok{abline}\NormalTok{(mod1,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/unnamed-chunk-146-1} 

}

\caption{Gene expression and histone modification score modelled by linear regression}\label{fig:unnamed-chunk-146}
\end{figure}

\hypertarget{how-to-estimate-the-error-of-the-coefficients}{%
\subsection{How to estimate the error of the
coefficients}\label{how-to-estimate-the-error-of-the-coefficients}}

Since we are using a sample to estimate the coefficients they are not
exact, with every random sample they will vary. Below, we are taking
multiple samples from the population and fitting lines to each sample,
with each sample the lines slightly change.We are overlaying the points
and the lines for each sample on top of the other samples .When we take
200 samples and fit lines for each of them,the lines fit are variable.
And, we get a normal-like distribution of \(\beta\) values with a
defined mean and standard deviation a, which is called standard error of
the coefficients.

\begin{figure}

{\centering \includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-147-1} 

}

\caption{Regression coefficients vary with every random sample. The figure illustrates the variability of regression coefficients when regression is done using a sample of data points. Histograms depict this variability for $b_0$ and $b_1$ coefficients.}\label{fig:unnamed-chunk-147}
\end{figure}

As usually we will not have access to the population to do repeated
sampling, model fitting and estimation of the standard error for the
coefficients. But there is statistical theory that helps us infer the
population properties from the sample. When we assume that error terms
have constant variance and mean zero , we can model the uncertainty in
the regression coefficients, \(\beta\)s. The estimates for standard
errors of \(\beta\)s for simple regression are as follows and shown
without derivation.

\[
\begin{aligned}
s=RSE=\sqrt{\frac{\sum{(y_i-(\beta_0+\beta_1x_i))^2}}{n-2}  } =\sqrt{\frac{\sum{\epsilon^2}}{n-2}  } \\
SE(\hat{\beta_1})=\frac{s}{\sqrt{\sum{(x_i-\overline{X})^2}}} \\
SE(\hat{\beta_0})=s\sqrt{ \frac{1}{n} + \frac{\overline{X}^2}{\sum{(x_i-\overline{X})^2} }  }
\end{aligned}
\]

Notice that that \(SE(\beta_1)\) depends on the estimate of variance of
residuals shown as \(s\) or \textbf{Residual Standard Error (RSE)}.
Notice alsos standard error depends on the spread of \(X\). If \(X\)
values have more variation, the standard error will be lower. This
intuitively makes sense since if the spread of the \(X\) is low, the
regression line will be able to wiggle more compared to a regression
line that is fit to the same number of points but covers a greater range
on the X-axis.

The standard error estimates can also be used to calculate confidence
intervals and test hypotheses, since the following quantity called
t-score approximately follows a t-distribution with \(n-p\) degrees of
freedom, where \(n\) is the number of data points and \(p\) is the
number of coefficients estimated.

\[ \frac{\hat{\beta_i}-\beta_test}{SE(\hat{\beta_i})}\]

Often, we would like to test the null hypothesis if a coefficient is
equal to zero or not. For simple regression this could mean if there is
a relationship between explanatory variable and response variable. We
would calculate the t-score as follows
\(\frac{\hat{\beta_i}-0}{SE(\hat{\beta_i})}\), and compare it
t-distribution with \(d.f.=n-p\) to get the p-value.

We can also calculate the uncertainty of the regression coefficients
using confidence intervals, the range of values that are likely to
contain \(\beta_i\). The 95\% confidence interval for \(\hat{\beta_i}\)
is \(\hat{\beta_i}\)  \(t_{0.975}SE(\hat{\beta_i})\). \(t_{0.975}\) is
the 97.5\% percentile of the t-distribution with \(d.f. = n  p\).

In R, \texttt{summary()} function will test all the coefficients for the
null hypothesis \(\beta_i=0\). The function takes the model output
obtained from the \texttt{lm()} function. To demonstrate this, let us
first get some data. The procedure below simulates data to be used in a
regression setting and it is useful to examine what the linear model
expect to model the data.

Since we have the data, we can build our model and call the
\texttt{summary} function. We will then use \texttt{confint()} function
to get the confidence intervals on the coefficients and \texttt{coef()}
function to pull out the estimated coefficients from the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1=}\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x)}
\KeywordTok{summary}\NormalTok{(mod1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -77.11 -18.44   0.33  16.06  57.23 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  13.2454     6.2887    2.11    0.038 *  
## x             0.4995     0.0513    9.74  4.5e-16 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 28.8 on 98 degrees of freedom
## Multiple R-squared:  0.492,  Adjusted R-squared:  0.486 
## F-statistic: 94.8 on 1 and 98 DF,  p-value: 4.54e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get confidence intervals }
\KeywordTok{confint}\NormalTok{(mod1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              2.5 %  97.5 %
## (Intercept) 0.7657 25.7251
## x           0.3977  0.6014
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pull out coefficients from the model}
\KeywordTok{coef}\NormalTok{(mod1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           x 
##     13.2454      0.4995
\end{verbatim}

The \texttt{summary()} function prints out an extensive list of values.
The ``Coefficients'' section has the estimates, their standard error, t
score and the p-value from the hypothesis test \(H_0:\beta_i=0\). As you
can see, the estimate we get for the coefficients and their standard
errors are close to the ones we get from the repeatedly sampling and
getting a distribution of coefficients. This is statistical inference at
work, we can estimate the population properties within a certain error
using just a sample.

\hypertarget{accuracy-of-the-model}{%
\subsection{Accuracy of the model}\label{accuracy-of-the-model}}

If you have observed the table output by \texttt{summary()} function,
you must have noticed there are some other outputs, such as ``Residual
standard error'', ``Multiple R-squared'' and ``F-statistic''. These are
metrics that are useful for assessing the accuracy of the model. We will
explain them one by one.

\_ (RSE)\_ simply is the square-root of the the sum of squared error
terms, divided by degrees of freedom, \(n-p\), for simple linear
regression case, \(n-2\). Sum of of the squares of the error terms is
also called \textbf{``Residual sum of squares''}, RSS. So RSE is
calculated as follows:

\[ s=RSE=\sqrt{\frac{\sum{(y_i-\hat{Y_i})^2 }}{n-p}}=\sqrt{\frac{RSS}{n-p}}\]

RSE is a way of assessing the model fit. The larger the RSE the worse
the model is. However, this is an absolute measure in the units of \(Y\)
and we have nothing to compare against. One idea is that we divide it by
RSS of a simpler model for comparative purposes. That simpler model is
in this case is the model with the intercept,\(\beta_0\). A very bad
model will have close zero coefficients for explanatory variables, and
the RSS of that model will be close to the RSS of the model with only
the intercept. In such a model intercept will be equal to
\(\overline{Y}\). As it turns out, RSS of the the model with just the
intercept is called \emph{``Total Sum of Squares'' or TSS}. A good model
will have a low \(RSS/TSS\). The metric \(R^2\) uses these quantities to
calculate a score between 0 and 1, and closer to 1 the better the model.
Here is how it is calculated:

\[R^2=1-\frac{RSS}{TSS}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}\]

\(TSS-RSS\) part of the formula often referred to as ``explained
variability'' in the model. The bottom part is for ``total
variability''. With this interpretation, higher the ``explained
variability'' better the model. For simple linear regression with one
explanatory variable, the square root of \(R^2\) is a quantity known as
absolute value of the correlation coefficient, which can be calculated
for any pair of variables, not only the response and the explanatory
variables. \emph{Correlation} is a general measure of linear
relationship between two variables. One of the most popular flavors of
correlation is the Pearson correlation coefficient. Formally, It is the
\emph{covariance} of X and Y divided by multiplication of standard
deviations of X and Y. In R, it can be calculated with \texttt{cor()}
function.

\[ 
r_{xy}=\frac{cov(X,Y)}{\sigma_x\sigma_y}
      =\frac{\sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
            {\sqrt{\sum\limits_{i=1}^n (x_i-\bar{x})^2 \sum\limits_{i=1}^n (y_i-\bar{y})^2}}
\] In the equation above, cov is the covariance, this is again a measure
of how much two variables change together, like correlation. If two
variables show similar behavior they will usually have positive
covariance value, if they have opposite behavior, the covariance will
have negative value. However, these values are boundless. A normalized
way of looking at covariance is to divide covariance by the
multiplication of standard errors of X and Y. This bounds the values to
-1 and 1, and as mentioned above called Pearson correlation coefficient.
The values that change in a similar manner will have a positive
coefficient, the values that change in opposite manner will have
negative coefficient, and pairs do not have a linear relationship will
have 0 or near 0 correlation. In the figure below, we are showing
\(R^2\), correlation coefficient and covariance for different scatter
plots.

\begin{figure}

{\centering \includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-150-1} 

}

\caption{Correlation and covariance for different scatter plots}\label{fig:unnamed-chunk-150}
\end{figure}

For simple linear regression, correlation can be used to asses the
model. However, this becomes useless as a measure of general accuracy if
the there are more than one explanatory variable as in multiple linear
regression. In that case, \(R^2\) is a measure of accuracy for the
model. Interestingly, square of the correlation of predicted values and
original response variables (\((cor(Y,\hat{Y}))^2\) ) equals to \(R^2\)
for multiple linear regression.

The last accuracy measure or the model fit in general we are going to
explain is \emph{F-statistic}. This is a quantity that depends on RSS
and TSS again. It can also answer one important question that other
metrics can not easily answer. That question is whether or not any of
the explanatory variables have predictive value or in other words if all
the explanatory variables are zero. We can write the null hypothesis as
follows:

\[H_0: \beta_1=\beta_2=\beta_3=...=\beta_p=0 \]

where the alternative is:

\[H_1: \text{at least one } \beta_i \neq 0 \]

Remember \(TSS-RSS\) is analogous to ``explained variability'' and the
RSS is analogous to ``unexplained variability''. For the F-statistic, we
divide explained variance to unexplained variance. Explained variance is
just the \(TSS-RSS\) divided by degrees of freedom, and unexplained
variance is the RSE. The ratio will follow the F-distribution with two
parameters, the degrees of freedom for the explained variance and the
degrees of freedom for the the unexplained variance.F-statistic for a
linear model is calculated as follows.

\[F=\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}=\frac{(TSS-RSS)/(p-1)}{RSE} \sim F(p-1,n-p)\]

If the variances are the same, the ratio will be 1, and when \(H_0\) is
true, then it can be shown that expected value of \((TSS-RSS)/(p-1)\)
will be \(\sigma^2\) which is estimated by RSE. So, if the variances are
significantly different, the ratio will need to be significantly bigger
than 1. If the ratio is large enough we can reject the null hypothesis.
To asses that we need to use software or look up the tables for F
statistics with calculated parameters. In R, function \texttt{qf()} can
be used to calculate critical value of the ratio. Benefit of the F-test
over looking at significance of coefficients one by one is that we
circumvent multiple testing problem. If there are lots of explanatory
variables at least 5\% of the time (assuming we use 0.05 as P-value
significance cutoff), p-values from coefficient t-tests will be wrong.
In summary, F-test is a better choice for testing if there is any
association between the explanatory variables and the response variable.

\hypertarget{regression-with-categorical-variables}{%
\subsection{Regression with categorical
variables}\label{regression-with-categorical-variables}}

An important feature of linear regression is that categorical variables
can be used as explanatory variables, this feature is very useful in
genomics where explanatory variables often could be categorical. To put
it in context, in our histone modification example we can also include
if promoters have CpG islands or not as a variable. In addition, in
differential gene expression, we usually test the difference between
different condition which can be encoded as categorical variables in a
linear regression. We can sure use t-test for that as well if there are
only 2 conditions, but if there are more conditions and other variables
to control for such as Age or sex of the samples, we need to take those
into account for our statistics, and t-test alone can not handle such
complexity. In addition, when we have categorical variables we can also
have numeric variables in the model and we certainly do not have to
include only one type of variable in a model.

The simplest model with categorical variables include two levels that
can be encoded in 0 and 1.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{2}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene.df=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{exp=}\KeywordTok{c}\NormalTok{(gene1,gene2),}
                  \DataTypeTok{group=}\KeywordTok{c}\NormalTok{( }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{30}\NormalTok{) ) )}

\NormalTok{mod2=}\KeywordTok{lm}\NormalTok{(exp}\OperatorTok{~}\NormalTok{group,}\DataTypeTok{data=}\NormalTok{gene.df)}
\KeywordTok{summary}\NormalTok{(mod2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.729 -1.066  0.012  1.384  4.563 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    2.185      0.352    6.21    6e-08 ***
## group          1.873      0.497    3.77  0.00039 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.93 on 58 degrees of freedom
## Multiple R-squared:  0.196,  Adjusted R-squared:  0.183 
## F-statistic: 14.2 on 1 and 58 DF,  p-value: 0.000391
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{require}\NormalTok{(mosaic)}
\KeywordTok{plotModel}\NormalTok{(mod2)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-151-1} 

}

\caption{Linear model with a categorical variable coded as 0 and 1}\label{fig:unnamed-chunk-151}
\end{figure}

we can even compare more levels, we do not even have to encode them
ourselves. We can pass categorical variables to \texttt{lm()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gene.df=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{exp=}\KeywordTok{c}\NormalTok{(gene1,gene2,gene2),}
                  \DataTypeTok{group=}\KeywordTok{c}\NormalTok{( }\KeywordTok{rep}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"B"}\NormalTok{,}\DecValTok{30}\NormalTok{),}\KeywordTok{rep}\NormalTok{(}\StringTok{"C"}\NormalTok{,}\DecValTok{30}\NormalTok{) ) }
\NormalTok{                  )}

\NormalTok{mod3=}\KeywordTok{lm}\NormalTok{(exp}\OperatorTok{~}\NormalTok{group,}\DataTypeTok{data=}\NormalTok{gene.df)}
\KeywordTok{summary}\NormalTok{(mod3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = exp ~ group, data = gene.df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.729 -1.079 -0.098  1.484  4.563 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    4.058      0.378    10.7  < 2e-16 ***
## groupB        -1.873      0.535    -3.5  0.00073 ***
## groupC        -1.873      0.535    -3.5  0.00073 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.07 on 87 degrees of freedom
## Multiple R-squared:  0.158,  Adjusted R-squared:  0.139 
## F-statistic: 8.17 on 2 and 87 DF,  p-value: 0.000558
\end{verbatim}

\hypertarget{regression-pitfalls}{%
\subsection{Regression pitfalls}\label{regression-pitfalls}}

In most cases one should look at the error terms (residuals) vs fitted
values plot. Any structure in this plot indicates problems such as
non-linearity, correlation of error terms, non-constant variance or
unusual values driving the fit. Below we briefly explain the potential
issues with the linear regression.

\hypertarget{non-linearity}{%
\paragraph{non-linearity}\label{non-linearity}}

If the true relationship is far from linearity, prediction accuracy is
reduced and all the other conclusions are questionable. In some cases,
transforming the data with \(logX\), \(\sqrt{X}\) and \(X^2\) could
resolve the issue.

\hypertarget{correlation-of-explanatory-variables}{%
\paragraph{correlation of explanatory
variables}\label{correlation-of-explanatory-variables}}

If the explanatory variables are correlated that could lead to something
known as multicolinearity. When this happens SE estimates of the
coefficients will be too large. This is usually observed in time-course
data.

\hypertarget{correlation-of-error-terms}{%
\paragraph{correlation of error
terms}\label{correlation-of-error-terms}}

This assumes that the errors of the response variables are uncorrelated
with each other. If they are confidence intervals in the coefficients
might too narrow.

\hypertarget{non-constant-variance-of-error-terms}{%
\paragraph{Non-constant variance of error
terms}\label{non-constant-variance-of-error-terms}}

This means that different response variables have the same variance in
their errors, regardless of the values of the predictor variables. If
the errors are not constant, if for the errors grow as X grows this will
result in unreliable estimates in standard errors as the model assumes
constant variance. Transformation of data, such as \(logX\) and
\(\sqrt{X}\) could help in some cases.

\hypertarget{outliers-and-high-leverage-points}{%
\paragraph{outliers and high leverage
points}\label{outliers-and-high-leverage-points}}

Outliers are extreme values for Y and high leverage points are unusual X
values. Both of these extremes have power to affect the fitted line and
the standard errors. In some cases (measurement error), they can be
removed from the data for a better fit.

\BeginKnitrBlock{rmdtip}
\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  linear models and derivations of equations including matrix notation

  \begin{itemize}
  \tightlist
  \item
    Applied Linear Statistical Models by Kutner, Nachtsheim, et al.
  \item
    Elements of statistical learning by Hastie \& Tibshirani
  \item
    An Introduction to statistical learning by James, Witten, et al.
  \end{itemize}
\end{itemize}
\EndKnitrBlock{rmdtip}

\hypertarget{clustering-grouping-samples-based-on-their-similarity}{%
\section{Clustering: grouping samples based on their
similarity}\label{clustering-grouping-samples-based-on-their-similarity}}

In genomics, we would very frequently want to assess how our samples
relate to each other. Are our replicates similar to each other? Do the
samples from the same treatment group have the similar genome-wide
signals ? Do the patients with similar diseases have similar gene
expression profiles ? Take the last question for example. We need to
define a distance or similarity metric between patients' expression
profiles and use that metric to find groups of patients that are more
similar to each other than the rest of the patients. This, in essence,
is the general idea behind clustering. We need a distance metric and a
method to utilize that distance metric to find self-similar groups.
Clustering is a ubiquitous procedure in bioinformatics as well as any
field that deals with high-dimensional data. It is very likely every
genomics paper containing multiple samples have some sort of clustering.
Due to this ubiquity and general usefulness, it is an essential
technique to learn.

\hypertarget{distance-metrics}{%
\subsection{Distance metrics}\label{distance-metrics}}

The first required step for clustering is the distance metric. This is
simply a measurement of how similar gene expressions to each other are.
There are many options for distance metrics and the choice of the metric
is quite important for clustering. Consider a simple example where we
have four patients and expression of three genes measured. Which
patients look similar to each other based on their gene expression
profiles ?

\begin{table}

\caption{\label{tab:expTable}Gene expressions from patients}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & IRX4 & OCT4 & PAX6\\
\hline
patient1 & 11 & 10 & 1\\
\hline
patient2 & 13 & 13 & 3\\
\hline
patient3 & 2 & 4 & 10\\
\hline
patient4 & 1 & 3 & 9\\
\hline
\end{tabular}
\end{table}

It may not be obvious from the table at first sight but if we plot the
gene expression profile for each patient, we will see that expression
profiles of patient 1 and patient 2 is more similar to each other than
patient 3 or patient 4.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/expPlot-1} 

}

\caption{Gene expression values for different patients. Certain patients have similar gene expression values to each other.}\label{fig:expPlot}
\end{figure}

But how can we quantify what see by eye ? A simple metric for distance
between gene expression vectors between a given patient pair is the sum
of absolute difference between gene expression values This can be
formulated as follows: \(d_{AB}={\sum _{i=1}^{n}|e_{Ai}-e_{Bi}|}\),
where \(d_{AB}\) is the distance between patient A and B, and \(e_{Ai}\)
and \(e_{Bi}\) expression value of the \(i\)th gene for patient A and B.
This distance metric is called \textbf{``Manhattan distance''} or
\textbf{``L1 norm''}.

Another distance metric using sum of squared distances and taking a
square root of resulting value, that can be formulaized as:
\(d_{AB}={{\sqrt {\sum _{i=1}^{n}(e_{Ai}-e_{Bi})^{2}}}}\). This distance
is called \textbf{``Euclidean Distance''} or \textbf{``L2 norm''}. This
is usually the default distance metric for many clustering algorithms.
due to squaring operation values that are very different get higher
contribution to the distance. Due to this, compared to Manhattan
distance it can be more affected by outliers but generally if the
outliers are rare this distance metric works well.

The last metric we will introduce is the \textbf{``correlation
distance''}. This is simply \(d_{AB}=1-\rho\), where \(\rho\) is the
pearson correlation coefficient between two vectors, in our case those
vectors are gene expression profiles of patients. Using this distance
the gene expression vectors that have a similar pattern will have a
small distance whereas when the vectors have different patterns they
will have a large distance. In this case, the linear correlation between
vectors matters, the the scale of the vectors might be different.

Now let's see how we can calculate these distance in R. First, we have
our gene expression per patient table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          IRX4 OCT4 PAX6
## patient1   11   10    1
## patient2   13   13    3
## patient3    2    4   10
## patient4    1    3    9
\end{verbatim}

Next, we calculate the distance metrics using \texttt{dist} function and
\texttt{1-cor()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(df,}\DataTypeTok{method=}\StringTok{"manhattan"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          patient1 patient2 patient3
## patient2        7                  
## patient3       24       27         
## patient4       25       28        3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(df,}\DataTypeTok{method=}\StringTok{"euclidean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          patient1 patient2 patient3
## patient2    4.123                  
## patient3   14.071   15.843         
## patient4   14.595   16.733    1.732
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.dist}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{cor}\NormalTok{(}\KeywordTok{t}\NormalTok{(df)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          patient1 patient2 patient3
## patient2 0.004129                  
## patient3 1.988522 1.970725         
## patient4 1.988522 1.970725 0.000000
\end{verbatim}

\hypertarget{scaling-before-calculating-the-distance}{%
\subsubsection{Scaling before calculating the
distance}\label{scaling-before-calculating-the-distance}}

Before we proceed to the clustering, one more thing we need to take
care. Should we normalize our data ? Scale of the vectors in our
expression matrix can affect the distance calculation. Gene expression
tables are usually have some sort of normalization, so the values are in
comparable scales. But somehow if a gene's expression values were on
much higher scale than the other genes, that gene will effect the
distance more than other when using Euclidean or Manhattan distance. If
that is the case we can scale the variables.The traditional way of
scaling variables is to subtract their mean, and divide by their
standard deviation, this operation is also called ``standardization''.
If this is done on all genes, each gene will have the same affect on
distance measures. The decision to apply scaling ultimately depends on
our data and what you want to achieve. If the gene expression values are
previously normalized between patients, having genes that dominate the
distance metric could have a biological meaning and therefore it may not
be desireable to further scale variables. In R, the standardization is
done via \texttt{scale()} function. Here we scale the gene expression
values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          IRX4 OCT4 PAX6
## patient1   11   10    1
## patient2   13   13    3
## patient3    2    4   10
## patient4    1    3    9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{scale}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             IRX4    OCT4    PAX6
## patient1  0.6933  0.5213 -1.0734
## patient2  1.0195  1.1468 -0.6214
## patient3 -0.7748 -0.7298  0.9604
## patient4 -0.9379 -0.9383  0.7344
## attr(,"scaled:center")
## IRX4 OCT4 PAX6 
## 6.75 7.50 5.75 
## attr(,"scaled:scale")
##  IRX4  OCT4  PAX6 
## 6.131 4.796 4.425
\end{verbatim}

\hypertarget{hiearchical-clustering}{%
\subsection{Hiearchical clustering}\label{hiearchical-clustering}}

This is one of the most ubiqutous clustering algorithms. Using this
algorithm you can see the relationship of individual data points and
relationships of clusters. This is achieved successively joining small
clusters to each other based on the intercluster distance. Eventually,
you get a tree structure or a dendrogram that shows the relationship
between the individual data points and clusters. The height of the
dendrogram is the distance between clusters. Here we can show how to use
this on our toy data set from four patients. The base function in R to
do hierarchical clustering in \texttt{hclust()}. Below, we apply that
function on Euclidean distances between patients.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d=}\KeywordTok{dist}\NormalTok{(df)}
\NormalTok{hc=}\KeywordTok{hclust}\NormalTok{(d,}\DataTypeTok{method=}\StringTok{"complete"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(hc)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/toyClust-1} 

}

\caption{Dendrogram of distance matrix}\label{fig:toyClust}
\end{figure}

In the above code snippet, we have used \texttt{method="complete"}
argument without explaining it. The \texttt{method} argument defines the
criteria that directs how the sub-clusters are merged. During clustering
starting with single-member clusters, the clusters are merged based on
the distance between them. There are many different ways to define
distance between clusters and based on which definition you use the
hierarchical clustering results change. So the \texttt{method} argument
controls that. There are a couple of values this argument can take, we
list them and their description below:

\begin{itemize}
\tightlist
\item
  \textbf{``complete''} stands for ``Complete Linkage'' and the distance
  between two clusters is defined as largest distance between any
  members of the two clusters.
\item
  \textbf{``single''} stands for ``Single Linkage'' and the distance
  between two clusters is defined as smallest distance between any
  members of the two clusters.
\item
  \textbf{``average''} stands for ``Average Linkage'' or more precisely
  UPGMA (Unweighted Pair Group Method with Arithmetic Mean) method. In
  this case, the distance between two clusters is defined as average
  distance between any members of the two clusters.
\item
  \textbf{``ward.D2''} and \textbf{``ward.D''} stands for different
  implementations of Ward's minimum variance method. This method aims to
  find compact, spherical clusters by selecting clusters to merge based
  on the change in the cluster variances. The clusters are merged if the
  increase in the combined variance over the sum of the cluster specific
  variances is minimum compared to alternative merging operations.
\end{itemize}

In real life, we would get expression profiles from thousands of genes
and we will typically have many more patients than our toy example. One
such data set is gene expression values from 60 bone marrow samples of
patients with one of the four main types of leukemia (ALL, AML, CLL,
CML) or no-leukemia controls. We trimmed that data set down to top 1000
most variable genes to be able to work with it easier and in addition
genes that are not very variable do not contribute much to the distances
between patients. We will now use this data set to cluster the patients
and display the values as a heatmap and a dendrogram. The heatmap shows
the expression values of genes across patients in a color coded manner.
The heatmap function, \texttt{pheatmap()}, we will use performs the
clustering as well. The matrix that contains gene expressions has the
genes in the rows and the patients in the columns. Therefore, we will
also use a column-side color code to mark the patients based on their
leukemia type. For the hierarchical clustering, we will use Ward's
method designated by \texttt{clustering\_method} argument to
\texttt{pheatmap()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pheatmap)}
\NormalTok{expFile=}\KeywordTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{,}\StringTok{"leukemiaExpressionSubset.rds"}\NormalTok{,}\DataTypeTok{package=}\StringTok{"compGenomRData"}\NormalTok{)}
\NormalTok{mat=}\KeywordTok{readRDS}\NormalTok{(expFile)}

\CommentTok{# set the leukemia type annotation for each sample}
\NormalTok{annotation_col =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                    \DataTypeTok{LeukemiaType =}\KeywordTok{substr}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(mat),}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{rownames}\NormalTok{(annotation_col)=}\KeywordTok{colnames}\NormalTok{(mat)}
  

\KeywordTok{pheatmap}\NormalTok{(mat,}\DataTypeTok{show_rownames=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{show_colnames=}\OtherTok{FALSE}\NormalTok{,}\DataTypeTok{annotation_col=}\NormalTok{annotation_col,}\DataTypeTok{scale =} \StringTok{"none"}\NormalTok{,}\DataTypeTok{clustering_method=}\StringTok{"ward.D2"}\NormalTok{,}\DataTypeTok{clustering_distance_cols=}\StringTok{"euclidean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/heatmap1-1} \end{center}

As we can observe in the heatmap each cluster has a distinct set of
expression values. The main clusters almost perfectly distinguish the
leukemia types. Only one CML patient is clustered as a non-leukemia
sample. This could mean that gene expression profiles are enough to
classify leukemia type. More detailed analysis and experiments are
needed to verify that but by looking at this exploratory analysis we can
decide where to focus our efforts next.

\hypertarget{where-to-cut-the-tree}{%
\subsubsection{where to cut the tree ?}\label{where-to-cut-the-tree}}

The example above seems like a clear cut example where we can pick by
eye clusters from the dendrogram. This is mostly due to the Ward's
method where compact clusters are preffered. However, as it is usually
the case we do not have patient labels and it would be difficult to tell
which leaves (patients) in the dendrogram we should consider as part of
the same cluster. In other words, how deep we should cut the dendrogram
so that every patient sample still connected via the remaining
sub-dendrograms constitute clusters. The \texttt{cutree()} function
provides the functionality to output either desired number of clusters
or clusters obtained from cutting the dendrogram at a certain height.
Below, we will cluster the patients with hierarchical clustering using
the default method ``complete linkage'' and cut the dendrogram at a
certain height. In this case, you will also observe that, changing from
Ward's distance to complete linkage had an effect on clustering. Now two
clusters that are defined by Ward's distance are closer to each other
and harder to separate from each other.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hcl=}\KeywordTok{hclust}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat)))}
\KeywordTok{plot}\NormalTok{(hcl,}\DataTypeTok{labels =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{hang=} \DecValTok{-1}\NormalTok{)}
\KeywordTok{rect.hclust}\NormalTok{(hcl, }\DataTypeTok{h =} \DecValTok{80}\NormalTok{, }\DataTypeTok{border =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/hclustNcut-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clu.k5=}\KeywordTok{cutree}\NormalTok{(hcl,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{) }\CommentTok{# cut tree so that there are 4 clusters}

\NormalTok{clu.h80=}\KeywordTok{cutree}\NormalTok{(hcl,}\DataTypeTok{h=}\DecValTok{80}\NormalTok{) }\CommentTok{# cut tree/dendrogram from height 80}
\KeywordTok{table}\NormalTok{(clu.k5) }\CommentTok{# number of samples for each cluster}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## clu.k5
##  1  2  3  4  5 
## 12  3  9 12 24
\end{verbatim}

Apart from the arbitrary values for the height or the number of the
clusters, how can we define clusters more systematically? As this is a
general question, we will show later how to decide the optimal number of
clusters later in this chapter.

\hypertarget{k-means-clustering}{%
\subsection{K-means clustering}\label{k-means-clustering}}

Another, very common clustering algorithm is k-means.This method divides
or partitions the data points, our working example patients, into a
pre-determined, ``k'' number of clusters. Hence, this type of methods
are generally called ``partioning'' methods. The algorithm is
initialized with randomly choosen \(k\) centers or centroids. In a
sense, a centroid is a data point with multiple values. In our working
example, it is a hypothetical patient with gene expression values. But
in the initialization phase, those gene expression values are choosen
randomly within the boundaries of the gene expression distributions from
real patients. As the next step in the algorithm, each patient is
assigned to the closest centroid and in the next iteration centroids are
set to the mean of values of the genes in the cluster. This process of
setting centroids and assigning patients to the clusters repeats itself
until sum of squared distances to cluster centroids is minimized.

As you might see, the cluster algorithm starts with random initial
centroids. This feature might yield different results for each run of
the algorithm. We will know show how to use k-means method on the gene
expression data set. We will use \texttt{set.seed()} for reproducbility.
In the wild, you might want to run this algorithm multiple times to see
if your clustering results are stable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{# we have to transpore the matrix t()}
\CommentTok{# so that we calculate distances between patients}
\NormalTok{kclu=}\KeywordTok{kmeans}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{centers=}\DecValTok{5}\NormalTok{)  }

\CommentTok{# number of data points in each cluster}
\KeywordTok{table}\NormalTok{(kclu}\OperatorTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1  2  3  4  5 
## 12 12 14 11 11
\end{verbatim}

Now let us check the percentage of each leukemia type in each cluster.
We can visualize this as a table. Looking at the table below, we see
that each of the 5 clusters are predominantly representing one of the 4
leukemia types or the control patients without leukemia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{type2kclu =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                    \DataTypeTok{LeukemiaType =}\KeywordTok{substr}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(mat),}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \DataTypeTok{cluster=}\NormalTok{kclu}\OperatorTok{$}\NormalTok{cluster)}

\KeywordTok{table}\NormalTok{(type2kclu)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             cluster
## LeukemiaType  1  2  3  4  5
##          ALL 12  0  0  0  0
##          AML  0  0  1  0 11
##          CLL  0 12  0  0  0
##          CML  0  0  1 11  0
##          NoL  0  0 12  0  0
\end{verbatim}

Another related and maybe more robust algorithm is called
\textbf{``k-medoids''} clustering. The procedure is almost identical to
k-means clustering with a couple of differences. In this case, centroids
choosen are real data points in our case patients, and the metric we are
trying to optimize in each iteration is based on manhattan distance to
the centroid. In k-means this was based on sum of squared distances so
euclidean distance. Below we are showing how to use k-medoids clustering
function \texttt{pam()} from the \texttt{cluster} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kmclu=cluster}\OperatorTok{::}\KeywordTok{pam}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{k=}\DecValTok{5}\NormalTok{) }\CommentTok{#  cluster using k-medoids}

\CommentTok{# make a data frame with Leukemia type and cluster id}
\NormalTok{type2kmclu =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
                    \DataTypeTok{LeukemiaType =}\KeywordTok{substr}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(mat),}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \DataTypeTok{cluster=}\NormalTok{kmclu}\OperatorTok{$}\NormalTok{cluster)}

\KeywordTok{table}\NormalTok{(type2kmclu)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             cluster
## LeukemiaType  1  2  3  4  5
##          ALL 12  0  0  0  0
##          AML  0 10  1  1  0
##          CLL  0  0  0  0 12
##          CML  0  0  0 12  0
##          NoL  0  0 12  0  0
\end{verbatim}

We can not visualize the clustering from partioning methods with a tree
like we did for hierarchical clustering. Even if we can get the
distances between patients the algorithm does not return the distances
between clusters out of the box. However, if we had a way to visualize
the distances between patients in 2 dimensions we could see the how
patients and clusters relate each other. It turns out, that there is a
way to compress between patient distances to a 2-dimensional plot. There
are many ways to do this and we introduce these dimension reduction
methods including the one we will use now later in this chapter. For
now, we are going to use a method called ``multi-dimensional scaling''
and plot the patients in a 2D plot color coded by their cluster
assignments.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate distances}
\NormalTok{dists=}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat))}

\CommentTok{# calculate MDS}
\NormalTok{mds=}\KeywordTok{cmdscale}\NormalTok{(dists)}

\CommentTok{# plot the patients in the 2D space}
\KeywordTok{plot}\NormalTok{(mds,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{5}\NormalTok{)[kclu}\OperatorTok{$}\NormalTok{cluster])}

\CommentTok{# set the legend for cluster colors}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}
       \DataTypeTok{legend=}\KeywordTok{paste}\NormalTok{(}\StringTok{"clu"}\NormalTok{,}\KeywordTok{unique}\NormalTok{(kclu}\OperatorTok{$}\NormalTok{cluster)),}
       \DataTypeTok{fill=}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{5}\NormalTok{)[}\KeywordTok{unique}\NormalTok{(kclu}\OperatorTok{$}\NormalTok{cluster)],}
       \DataTypeTok{border=}\OtherTok{NA}\NormalTok{,}\DataTypeTok{box.col=}\OtherTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/kmeansmds-1} \end{center}

The plot we obtained shows the separetion between clusters. However, it
does not do a great job showing the separation between cluster 3 and 4,
which represent CML and ``no leukemia'' patients. We might need another
dimension to properly visualize that separation. In addition, those two
clusters were closely related in the hierarhical clustering as well.

\hypertarget{how-to-choose-k-the-number-of-clusters}{%
\subsection{how to choose ``k'', the number of
clusters}\label{how-to-choose-k-the-number-of-clusters}}

Up to this point, we have avoided the question of selecting optimal
number clusters. How do we know where to cut our dendrogram or which k
to choose ? First of all, this is a difficult question. Usually,
clusters have different granuality. Some clusters are tight and compact
and some are wide,and both these types of clusters can be in the same
data set. When visualized, some large clusters may look like they may
have sub-clusters. So should we consider the large cluster as one
cluster or should we consider the sub-clusters as individual clusters ?
There are some metrics to help but there is no definite answer. We will
show a couple of them below.

\hypertarget{silhouhette}{%
\subsubsection{Silhouhette}\label{silhouhette}}

One way to determine how well the clustering is to measure the expected
self-similar nature of the points in a set of clusters. The silhouette
value does just that and it is a measure of how similar a data point is
to its own cluster compared to other clusters. The silhouette value
ranges from -1 to +1, where values that are positive indicates that the
data point is well matched to its own cluster, if the value is zero it
is a borderline case and if the value is minus it means that the data
point might be mis-clustered because it is more simialr to a neighboring
cluster. If most data points have a high value, then the clustering is
appropriate. Ideally, one can create many different clusterings with
different parameters such as \(k\),number of clusters and assess their
appropriateness using the average silhouette values. In R, silhouette
values are refered to as silhouette widths in the documentation.

A silhouette value is calculated for each data point. In our working
example, each patient will get silhouette values showing how well they
are matched to their assigned clusters. Formally this calculated as
follows. For each data point \(i\), we calculate
\({\displaystyle a(i)}\), which denotes the average distance between
\(i\) and all other data points within the same cluster. This shows how
well the point fits into that cluster. For the same data point, we also
calculate \({\displaystyle b(i)}\) b(i) denotes the lowest average
distance of \({\displaystyle i}\) to all points in any other cluster, of
which \({\displaystyle i}\) is not a member. The cluster with this
lowest average \(b(i)\) is the ``neighbouring cluster'' of data point
\({\displaystyle i}\) since it is the next best fit cluster for that
data point. Then, the silhouette value for a given data point is:

\(s(i) = \frac{b(i) - a(i)}{\max\{a(i),b(i)\}}\)

As described, this quantity is positive when \(b(i)\) is high and
\(a(i)\) is low, meaning that the data point \(i\) is self-similar to
its cluster. And the silhouette value, \(s(i)\), is negative if it is
more similar to its neighbours than its assigned cluster.

In R, we can calculate silhouette values using
\texttt{cluster::silhouette()} function. Below, we calculate the
silhouette values for k-medoids clustering with \texttt{pam()} function
with \texttt{k=5}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(cluster)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{pamclu=cluster}\OperatorTok{::}\KeywordTok{pam}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{silhouette}\NormalTok{(pamclu),}\DataTypeTok{main=}\OtherTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/sill-1} \end{center}

Now, let us calculate average silhouette value different \(k\) values
and compare. We will use \texttt{sapply()} function to get average
silhouette values accross \(k\) values between 2 and 7. Within
\texttt{sapply()} there is an anonymous function that that does the
clustering and calculates average silhouette values for each \(k\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ks=}\KeywordTok{sapply}\NormalTok{(}\DecValTok{2}\OperatorTok{:}\DecValTok{7}\NormalTok{,}
    \ControlFlowTok{function}\NormalTok{(i) }
      \KeywordTok{summary}\NormalTok{(}\KeywordTok{silhouette}\NormalTok{(}\KeywordTok{pam}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{k=}\NormalTok{i)))}\OperatorTok{$}\NormalTok{avg.width)}
\KeywordTok{plot}\NormalTok{(}\DecValTok{2}\OperatorTok{:}\DecValTok{7}\NormalTok{,Ks,}\DataTypeTok{xlab=}\StringTok{"k"}\NormalTok{,}\DataTypeTok{ylab=}\StringTok{"av. silhouette"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"b"}\NormalTok{,}
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/sillav-1} \end{center}

In this case, it seems the best value for \(k\) is 4. The k-medoids
function \texttt{pam()} will usually cluster CML and noLeukemia cases
together when \texttt{k=4}, which are also related clusters according to
hierarchical clustering we did earlier.

\hypertarget{gap-statistic}{%
\subsubsection{Gap statistic}\label{gap-statistic}}

As clustering aims to find self-similar data points, it would be
reasonable to expect with the correct number of clusters the total
within-cluster variation is minimized. Within-cluster variation for a
single cluster can simply be defined as sum of squares from the cluster
mean, which in this case is the centroid we defined in k-means
algorithm. The total within-cluster variation is then sum of
within-cluster variations for each cluster. This can be formally defined
as follows:

\(\displaystyle W_k = \sum_{k=1}^K \sum_{\mathrm{x}_i \in C_k} (\mathrm{x}_i - \mu_k )^2\)

Where \(\mathrm{x}_i\) is data point in cluster \(k\), and \(\mu_k\) is
the cluster mean, and \(W_k\) is the total within-cluster variation
quantity we described. However, the problem is that the variation
quantity decreases with number of clusters. The more centroids we have,
the smaller the distances to the centroids get. A more reliable approach
would be somehow calculating the expected variation from a reference
null distribution and compare that to the observed variation for each
\(k\). In gap statistic approach, the expected distribution is
calculated via sampling points from the boundaries of the original data
and calculating within-cluster variation quantity for multiple rounds of
sampling. This way we have an expectation how about the variability when
there is no expected clustering, and then compare that expected
variation to the observed within-cluster variation. The expected
variation should also go down with increasing number of clusters, but
for the optimal number of clusters the expected variation will be
furthest away from observed variation. This distance is called the
\textbf{``gap statistic''} and defined as follows:
\(\displaystyle \mathrm{Gap}_n(k) = E_n^*\{\log W_k\} - \log W_k\),
where \(E_n^*\{\log W_k\}\) is the expected variation in log-scale under
a sample size \(n\) from the reference distribution and \(\log W_k\) is
the observed variation. Our aim is choose the \(k\), number of clusters,
that maximizes \(\mathrm{Gap}_n(k)\).

We can easily calculate the gap statistic with
\texttt{cluster::clusGap()} function. We will now use that function to
calculate the gap statistic for our patient gene expression data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(cluster)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\CommentTok{# define the clustering function}
\NormalTok{pam1 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x,k) }
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{cluster =} \KeywordTok{pam}\NormalTok{(x,k, }\DataTypeTok{cluster.only=}\OtherTok{TRUE}\NormalTok{))}

\CommentTok{# calculate the gap statistic}
\NormalTok{pam.gap=}\StringTok{ }\KeywordTok{clusGap}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat), }\DataTypeTok{FUN =}\NormalTok{ pam1, }\DataTypeTok{K.max =} \DecValTok{8}\NormalTok{,}\DataTypeTok{B=}\DecValTok{50}\NormalTok{)}

\CommentTok{# plot the gap statistic accross k values}
\KeywordTok{plot}\NormalTok{(pam.gap, }\DataTypeTok{main =} \StringTok{"Gap statistic for the 'Leukemia' data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/clusGap-1} \end{center}

In this case, gap statistic shows \(k=7\) is the best. However, after
\(K=6\) the statistic has more or less a stable curve. In this case, we
know that there are 5 main patient categories but this does not mean
there is no sub-categories or sub-types for the cancers we are looking
at.

\url{https://statweb.stanford.edu/~gwalther/gap}

\hypertarget{other-methods}{%
\subsubsection{Other methods}\label{other-methods}}

There are several other methods that provide insight into how many
clusters. In fact, the package \texttt{NbClust} provides 30 different
ways to determine the number of optimal clusters and can offer a voting
mechanism to pick the best number. Below, we are showing how to use this
function for some of the optimal number of cluster detection methods.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(NbClust)}
\NormalTok{nb =}\StringTok{ }\KeywordTok{NbClust}\NormalTok{(}\DataTypeTok{data=}\KeywordTok{t}\NormalTok{(mat), }
             \DataTypeTok{distance =} \StringTok{"euclidean"}\NormalTok{, }\DataTypeTok{min.nc =} \DecValTok{2}\NormalTok{,}
        \DataTypeTok{max.nc =} \DecValTok{7}\NormalTok{, }\DataTypeTok{method =} \StringTok{"kmeans"}\NormalTok{,}
        \DataTypeTok{index=}\KeywordTok{c}\NormalTok{(}\StringTok{"kl"}\NormalTok{,}\StringTok{"ch"}\NormalTok{,}\StringTok{"cindex"}\NormalTok{,}\StringTok{"db"}\NormalTok{,}\StringTok{"silhouette"}\NormalTok{,}
                \StringTok{"duda"}\NormalTok{,}\StringTok{"pseudot2"}\NormalTok{,}\StringTok{"beale"}\NormalTok{,}\StringTok{"ratkowsky"}\NormalTok{,}
                \StringTok{"gap"}\NormalTok{,}\StringTok{"gamma"}\NormalTok{,}\StringTok{"mcclain"}\NormalTok{,}\StringTok{"gplus"}\NormalTok{,}
                \StringTok{"tau"}\NormalTok{,}\StringTok{"sdindex"}\NormalTok{,}\StringTok{"sdbw"}\NormalTok{))}

\KeywordTok{table}\NormalTok{(nb}\OperatorTok{$}\NormalTok{Best.nc[}\DecValTok{1}\NormalTok{,]) }\CommentTok{# consensus seems to be 3 clusters }
\end{Highlighting}
\end{Shaded}

However, the readers should keep in mind that clustering is an
exploratory technique. If you have solid labels for your data points
maybe clustering is just a sanity check, and you should just do
predictive modeling instead. However, in biology there are rarely solid
labels and things have different granularity. Take the leukemia patients
case we have been using for example, it is know that leukemia types have
subtypes and those sub-types that have different mutation profiles and
consequently have different molecular signatures. Because of this, it is
not surprising that some optimal cluster number techniques will find
more clusters to be appropriate. On the other hand, CML (Chronic myeloid
leukemia ) is a slow progressing disease and maybe as molecular
signatures goes could be the closest to no leukemia patients, clustering
algorithms may confuse the two depending on what granuality they are
operating with. It is always good to look at the heatmaps after
clustering, if you have meaningful self-similar data points even if the
labels you have do not agree that there can be different clusters you
can perform downstream analysis to understand the sub-clusters better.
As we have seen, we can estimate optimal number of clusters but we can
not take that estimation as the absolute truth, given more data points
or different set of expression signatures you may have different optimal
clusterings, or the supposed optimal clustering might overlook
previously known sub-groups of your data.

\hypertarget{dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d}{%
\section{Dimensionality reduction techniques: visualizing complex data
sets in
2D}\label{dimensionality-reduction-techniques-visualizing-complex-data-sets-in-2d}}

In statistics, dimension reduction techniques are a set of processes for
reducing the number of random variables by obtaining a set of principal
variables. For example, in the context of a gene expression matrix
accross different patient samples, this might mean getting a set of new
variables that cover the variation in sets of genes. This way samples
can be represented by a couple of principal variables instead of
thousands of genes. This is useful for visualization, clustering and
predictive modeling.

\hypertarget{principal-component-analysis}{%
\subsection{Principal component
analysis}\label{principal-component-analysis}}

Principal component analysis (PCA) is maybe the most popular technique
to examine high-dimensional data. There are multiple interpretations of
how PCA reduces dimensionality. We will first focus on geometrical
interpretation, where this operation can be interpreted as rotating the
orignal dimensions of the data. For this, we go back to our example gene
expression data set. In this example, we will represent our patients
with expression profiles of just two genes, CD33 (ENSG00000105383) and
PYGL (ENSG00000100504) genes. This way we can visualize them in a
scatterplot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000100504"}\NormalTok{,],}
\NormalTok{     mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000105383"}\NormalTok{,],}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"CD33 (ENSG00000105383)"}\NormalTok{,}
     \DataTypeTok{xlab=}\StringTok{"PYGL (ENSG00000100504)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-155-1} \end{center}

PCA rotates the original data space such that the axes of the new
coordinate system point into the directions of highest variance of the
data. The axes or new variables are termed principal components (PCs)
and are ordered by variance: The first component, PC 1, represents the
direction of the highest variance of the data. The direction of the
second component, PC 2, represents the highest of the remaining variance
orthogonal to the first component. This can be naturally extended to
obtain the required number of components which together span a component
space covering the desired amount of variance. In our toy example with
only two genes, the principal componets are drawn over the original
scatter plot and in the next plot we show the new coordinate system
based on the pricinpal components. We will calculate the PCA with the
\texttt{princomp()} function, this function returns the new coordinates
as well. These new coordinates are simply a projection of data over the
new coordinates. We will decorate the scatter plots with eigenvectors
showing the direction of greatest variation. Then, we will plot the new
coordinates. These are automatically calculated by \texttt{princomp()}
function. Notice that we are using the \texttt{scale()} function when
plotting coordinates and also before calculating PCA. This function
centers the data, meaning substracts the mean of the each column vector
from the elements in the vector. This essentially gives the columns a
zero mean. It also divides the data by the standard deviation of the
centered columns. These two operations helps bring the data to a common
scale which is important for PCA not to be affected by different scales
in the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\CommentTok{# create the subset of the data with two genes only}
\CommentTok{# notice that we transpose the matrix so samples are }
\CommentTok{# on the columns}
\NormalTok{sub.mat=}\KeywordTok{t}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"ENSG00000100504"}\NormalTok{,}\StringTok{"ENSG00000105383"}\NormalTok{),])}

\CommentTok{# ploting our genes of interest as scatter plots}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{scale}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000100504"}\NormalTok{,]),}
     \KeywordTok{scale}\NormalTok{(mat[}\KeywordTok{rownames}\NormalTok{(mat)}\OperatorTok{==}\StringTok{"ENSG00000105383"}\NormalTok{,]),}
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{ylab=}\StringTok{"CD33 (ENSG00000105383)"}\NormalTok{,}
     \DataTypeTok{xlab=}\StringTok{"PYGL (ENSG00000100504)"}\NormalTok{,}
     \DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType,}
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}

\CommentTok{# create the legend for the Leukemia types}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{,}
       \DataTypeTok{legend=}\KeywordTok{unique}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
       \DataTypeTok{fill =}\KeywordTok{palette}\NormalTok{(}\StringTok{"default"}\NormalTok{),}
       \DataTypeTok{border=}\OtherTok{NA}\NormalTok{,}\DataTypeTok{box.col=}\OtherTok{NA}\NormalTok{)}

\CommentTok{# calculate the PCA only for our genes and all the samples}
\NormalTok{pr=}\KeywordTok{princomp}\NormalTok{(}\KeywordTok{scale}\NormalTok{(sub.mat))}


\CommentTok{# plot the direction of eigenvectors}
\CommentTok{# pr$loadings returned by princomp has the eigenvectors}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], }
         \DataTypeTok{y1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{],}\DataTypeTok{col=}\StringTok{"pink"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }
         \DataTypeTok{y1 =}\NormalTok{ pr}\OperatorTok{$}\NormalTok{loadings[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{],}\DataTypeTok{col=}\StringTok{"gray"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}


\CommentTok{# plot the samples in the new coordinate system}
\KeywordTok{plot}\NormalTok{(}\OperatorTok{-}\NormalTok{pr}\OperatorTok{$}\NormalTok{scores,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType,}
     \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{))}

\CommentTok{# plot the new coordinate basis vectors}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =}\OperatorTok{-}\DecValTok{2}\NormalTok{, }
         \DataTypeTok{y1 =} \DecValTok{0}\NormalTok{,}\DataTypeTok{col=}\StringTok{"pink"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{arrows}\NormalTok{(}\DataTypeTok{x0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{y0=}\DecValTok{0}\NormalTok{, }\DataTypeTok{x1 =} \DecValTok{0}\NormalTok{, }
         \DataTypeTok{y1 =} \DecValTok{-1}\NormalTok{,}\DataTypeTok{col=}\StringTok{"gray"}\NormalTok{,}\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{compgenomrReloaded_files/figure-latex/pcaRot-1} 

}

\caption{Geometric interpretation of PCA finding eigenvectors that point to direction of highest variance. Eigenvectors can be used as a new coordinate system.}\label{fig:pcaRot}
\end{figure}

As you can see, the new coordinate system is useful by itself.The X-axis
which represents the first component separates the data along the
lympoblastic and myeloid leukemias.

PCA in this case is obtained by calculating eigenvectors of the
covariance matrix via an operation called eigen decomposition.
Covariance matrix is obtained by covariance of pairwise variables of our
expression matrix, which is simply
\({ \operatorname{cov} (X,Y)={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-\mu_X)(y_{i}-\mu_Y)}\),
where \(X\) and \(Y\) expression values of genes in a sample in our
example. This is a measure of how things vary together, if high
expressed genes in sample A are also highly expressed in sample B and
lowly expressed in sample A are also lowly expressed in sample B, then
sample A and B will have positive covariance. If the opposite is true
then they will have negative covariance. This quantity is related to
correlation and in fact correlation is standardized covariance.
Covariance of variables can be obtained with \texttt{cov()} function,
and eigen decomposition of such a matrix will produce a set of
ortahogonal vectors that span the directions of highest variation. In
2D, you can think of this operation as rotating two perpendicular lines
together until they point to the directions where most of the variation
in the data lies on, similar to the figure \ref{fig:pcaRot}. An
important intuition is that, after the rotation prescribed by
eigenvectors is complete the covariance between variables in this
rotated dataset will be zero. There is a proper mathematical
relationship between covariances of the rotated dataset and the original
dataset. That's why operating on covariance matrix is related to the
rotation of the original dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cov.mat=}\KeywordTok{cov}\NormalTok{(sub.mat) }\CommentTok{# calculate covariance matrix}
\NormalTok{cov.mat}
\KeywordTok{eigen}\NormalTok{(cov.mat) }\CommentTok{# obtain eigen decomposition for eigen values and vectors}
\end{Highlighting}
\end{Shaded}

Eigenvectors and eigenvalues of the covariance matrix indicates the
direction and the magnitute of variation of the data. In our visual
example the eigenvectors are so-called principal components. The
eigenvector indicates the direction and the eigen values indicate the
variation in that direction. Eigenvectors and values exist in pairs:
every eigenvector has a corresponding eigenvalue and the eigenvectors
are linearly independent from each other, this means they are orthogonal
or uncorrelated in the our working example above. The eigenvectors are
ranked by their corresponding eigen value, the higher the eigen value
the more important the eigenvector is, because it explains more of the
variation compared to the other eigenvectors. This feature of PCA makes
the dimension reduction possible. We can sometimes display data sets
that have many variables only in 2D or 3D because the these top
eigenvectors are sometimes enough to capture most of variation in the
data.

\hypertarget{singular-value-decomposition-and-principal-component-analysis}{%
\subsubsection{Singular value decomposition and principal component
analysis}\label{singular-value-decomposition-and-principal-component-analysis}}

A more common way to calculate PCA is through something called singular
value decomposition (SVD). This results in another interpretation of
PCA, which is called ``latent factor'' or ``latent component''
interpretation. In a moment, it will be more clear what we mean by
``latent factors''. SVD is a matrix factorization or decomposition
algorithm that decomposes an input matrix,\(X\), to three matrices as
follows: \(\displaystyle \mathrm{X} = USV^T\). In essence many matrices
can be decomposed as a product of multiple matrices and we will come to
other techniques later in this chapter. Singular Value Decomposition is
shown in figure \ref{fig:SVDcartoon}. \(U\) is the matrix with
eigenarrays on the columns and this has the same dimensions as the input
matrix, you might see elsewhere the columns are named as eigenassays.
\(S\) is the matrix that contain the singular values on the diagonal.
The singular values are also known as eigenvalues and their square is
proportional to explained variation by each eigenvector. Finally, the
matrix \(V^T\) contains the eigenvectors on its rows. It is
interpretation is still the same. Geometrically, eigenvectors point to
the direction of highest variance in the data. They are uncorrolated or
geometrically orthogonal to each other. These interpretations are
identical to the ones we made before. The slight difference is that the
decomposition seem to output \(V^T\) which is just the transpose of the
matrix \(V\). However, the SVD algorithms in R usually return the matrix
\(V\). If you want the eigenvectors, you either simply use the columns
of matrix \(V\) or rows of \(V^T\).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/SVDcartoon} 

}

\caption{Singular Value Decomposition (SVD) explained in a diagram. }\label{fig:SVDcartoon}
\end{figure}

One thing that is new in the figure \ref{fig:SVDcartoon} is the concept
of eigenarrays. The eigenarrays or sometimes called eigenassays
reprensent the sample space and can be used to plot the relationship
between samples rather than genes. In this way, SVD offers additional
information than the PCA using the covariance matrix. It offers us a way
to summarize both genes and samples. As we can project the gene
expression profiles over the top two eigengenes and get a 2D
representation of genes, but with SVD we can also project the samples
over the the top two eigenarrays and get a representation of samples in
2D scatterplot. Eigenvector could represent independent expression
programs across samples, such as cell-cycle if we had time-based
expression profiles. However, there is no guarantee that each
eigenvector will be biologically meaningful. Similarly each eigenarray
represent samples with specific expression characteristics. For example,
the samples that have a particular pathway activated might be corrolated
to an eigenarray returned by SVD.

Previously, in order to map samples to the reduced 2D space we had to
transpose the genes-by-samples matrix when using \texttt{princomp()}
function. We will now first use SVD on genes-by-samples matrix to get
eigenarrays and use that to plot samples on the reduced dimensions. We
will project the columns in our original expression data on eigenarrays
and use the first two dimensions in the scatter plot. If you look at the
code you will see that for the projection we use \(U^T X\) operation,
which is just \(V^T\) if you follow the linear algebra. We will also
perform the PCA this time with \texttt{prcomp()} function on the
transposed genes-by-samples matrix to get a similar information, and
plot the samples on the reduced coordinates.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{d=}\KeywordTok{svd}\NormalTok{(}\KeywordTok{scale}\NormalTok{(mat)) }\CommentTok{# apply SVD}
\NormalTok{assays=}\KeywordTok{t}\NormalTok{(d}\OperatorTok{$}\NormalTok{u) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{scale}\NormalTok{(mat) }\CommentTok{# projection on eigenassays}
\KeywordTok{plot}\NormalTok{(assays[}\DecValTok{1}\NormalTok{,],assays[}\DecValTok{2}\NormalTok{,],}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}
     \DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType)}
\CommentTok{#plot(d$v[,1],d$v[,2],pch=19,}
\CommentTok{#     col=annotation_col$LeukemiaType)}
\NormalTok{pr=}\KeywordTok{prcomp}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{center=}\OtherTok{TRUE}\NormalTok{,}\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{) }\CommentTok{# apply PCA on transposed matrix}

\CommentTok{# plot new coordinates from PCA, projections on eigenvectors}
\CommentTok{# since the matrix is transposed eigenvectors represent }
\KeywordTok{plot}\NormalTok{(pr}\OperatorTok{$}\NormalTok{x[,}\DecValTok{1}\NormalTok{],pr}\OperatorTok{$}\NormalTok{x[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{compgenomrReloaded_files/figure-latex/svd-1} 

}

\caption{SVD on matrix and its transpose}\label{fig:svd}
\end{figure}

As you can see in the figure \ref{fig:svd}, the two approaches yield
separation of samples, although they are slightly different. The
difference comes from the centering and scaling. In the first case, we
scale and center columns and the second case we scale and center rows
since the matrix is transposed. If we do not do any scaling or centering
we would get identical plots.

\hypertarget{eigenvectors-as-latent-factorsvariables}{%
\paragraph{Eigenvectors as latent
factors/variables}\label{eigenvectors-as-latent-factorsvariables}}

Finally, we can introduce the latent factor interpretation of PCA via
SVD. As we have already mentioned eigenvectors can also be interpreted
as expression programs that are shared by several genes such as cell
cycle expression program when measuring gene expression accross samples
taken in different time points. In this intrepretation, linear
combination of expression programs makes up the expression profile of
the genes. Linear combination simply means multiplying the expression
program with a weight and adding them up. Our \(USV^T\) matrix
multiplication can be rearranged to yield such an understanding, we can
multiply eigenarrays \(U\) with the diagonal eigenvalues \(S\), to
produce a m-by-n weights matrix called \(W\), so \(W=US\) and we can
re-write the equation as just weights by eigenvectors matrix, \(X=WV^T\)
as shown in figure \ref{fig:SVDasWeigths}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/SVDasWeights} 

}

\caption{Singular Value Decomposition (SVD) reorgonized as multiplication of m-by-n weights matrix and eigenvectors }\label{fig:SVDasWeigths}
\end{figure}

This simple transformation now makes it clear that indeed if
eigenvectors are representing expression programs, their linear
combination is making up individual gene expression profiles. As an
example, we can show the liner combination of the first two eigenvectors
can approximate the expression profile of an hypothetical gene in the
gene expression matrix. The figure \ref{fig:SVDlatentExample} shows
eigenvector 1 and eigenvector 2 combined with certain weights in \(W\)
matrix can approximate gene expression pattern our example gene.

\begin{figure}

{\centering \includegraphics[width=19.68in]{images/SVDlatentExample} 

}

\caption{Gene expression of a gene can be thought as linear combination of eigenvectors. }\label{fig:SVDlatentExample}
\end{figure}

However, SVD does not care about biology. The eigenvectors are just
obtained from the data with constraints of ortagonality and the
direction of variation. There are examples of eigenvectors representing
real expression programs but that does not mean eigenvectors will always
be biologically meaningful. Sometimes combination of them might make
more sense in biology than single eigenvectors. This is also the same
for the other matrix factorization techniques we describe below.

\hypertarget{other-dimension-reduction-techniques-using-other-matrix-factorization-methods}{%
\subsection{Other dimension reduction techniques using other matrix
factorization
methods}\label{other-dimension-reduction-techniques-using-other-matrix-factorization-methods}}

We must mention a few other techniques that are similar to SVD in
spirit. Remember we mentioned that every matrix can be decomposed to
other matrices where matrix multiplication operations reconstruct the
original matrix. In the case of SVD/PCA, the constraint is that
eigenvectors/arrays are ortogonal, however there are other decomposition
algorithms with other constraints.

\hypertarget{independent-component-analysis-ica}{%
\subsubsection{Independent component analysis
(ICA)}\label{independent-component-analysis-ica}}

We will first start with independent component analysis (ICA) which is
an extension of PCA. ICA algorithm decomposes a given matrix \(X\) as
follows: \(X=SA\). The rows of \(A\) could be interpreted similar to the
eigengenes and columns of \(S\) could be interpreted as eigenarrays,
these components are sometimes called metagenes and metasamples in the
literature. Traditionally, \(S\) is called source matrix and \(A\) is
called mixing matrix. ICA is developed for a problem called
``blind-source separation''. In this problem, multiple microphones
record sound from multiple instruments, and the task is disentagle
sounds from original instruments since each microphone is recording a
combination of sounds. In this respect, the matrix \(S\) contains the
original signals (sounds from different instruments) and their linear
combinations identified by the weights in \(A\), and the product of
\(A\) and \(S\) makes up the matrix \(X\), which is the observed signal
from different microphones. With this interpretation in mind, if the
interest is strictly expression patterns similar that represent the
hidden expression programs we see that genes-by-samples matrix is
transposed to a samples-by-genes matrix, so that the columns of \(S\)
represent these expression patterns , here refered to as ``metagenes'',
hopefully representing distinct expression programs (Figure
\ref{fig:ICAcartoon} ).

\begin{figure}

{\centering \includegraphics[width=26.24in]{images/ICAcartoon} 

}

\caption{Independent Component Analysis (ICA)}\label{fig:ICAcartoon}
\end{figure}

ICA requires that the columns of \(S\) matrix, the ``metagenes'' in our
example above to be statistical independent. This is a stronger
constraint than uncorrelatedness. In this case, there should be no
relationship between non-linear transformation of the data either. There
are different ways of ensuring this statistical indepedence and this is
the main constraint when finding the optimal \(A\) and \(S\) matrices.
The various ICA algorithms use different proxies for statistical
independence, and the definition of that proxy is the main difference
between many ICA algorithms. The algorithm we are going to use requires
that metagenes or sources in the \(S\) matrix are non-gaussian as
possible. Non-gaussianity is shown to be related to statistical
independence {[}REF{]}. Below, we are using \texttt{fastICA::fastICA()}
function to extract 2 components and plot the rows of matrix \(A\) which
represents metagenes. This way, we can visualize samples in a 2D plot.
If we wanted to plot the relationship between genes we would use the the
columns of matrix \(S\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(fastICA)}
\NormalTok{ica.res=}\KeywordTok{fastICA}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{n.comp=}\DecValTok{2}\NormalTok{) }\CommentTok{# apply ICA}

\CommentTok{# plot reduced dimensions}
\KeywordTok{plot}\NormalTok{(ica.res}\OperatorTok{$}\NormalTok{S[,}\DecValTok{1}\NormalTok{],ica.res}\OperatorTok{$}\NormalTok{S[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-157-1} \end{center}

\hypertarget{non-negative-matrix-factorization-nmf}{%
\subsubsection{Non-negative matrix factorization
(NMF)}\label{non-negative-matrix-factorization-nmf}}

Non-negative matrix factorization algorithms are series of algorithms
that aim to decompose the matrix \(X\) into the product or matrices
\(W\) and \(H\), \(X=WH\) (Figure \ref{fig:NMFcartoon}). The constraint
is that \(W\) and \(H\) must contain non-negative values, so must \(X\).
This is well suited for data sets that can not contain negative values
such as gene expression. This also implies addivity of components, in
our example expression of a gene across samples are addition of multiple
metagenes. Unlike ICA and SVD/PCA, the metagenes can never be combined
in subtractive way. In this sense, expression programs potentially
captured by metagenes are combined additively.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/NMFcartoon} 

}

\caption{Non-negative matrix factorization}\label{fig:NMFcartoon}
\end{figure}

The algorithms that compute NMF tries to minimize the cost function
\(D(X,WH)\), which is the distance between \(X\) and \(WH\). The early
algorithms just use the euclidean distance which translates to
\(\sum(X-WH)^2\), this is also known as Frobenious norm and you will see
in the literature it is written as :\(\||V-WH||_{F}\) However this is
not the only distance metric, other distance metrics are also used in
NMF algorithms. In addition, there could be other parameters to optimize
that relates to sparseness of the \(W\) and \(H\) matrices. With sparse
\(W\) and \(H\), each entry in the \(X\) matrix is expressed as the sum
of a small number of components. This makes the interpretation easier,
if the weights are 0 than there is not contribution from the
corresponding factors.

Below, we are plotting the values of metagenes (rows of \(H\)) for
component 1 and 3. In this context, these values can also be interpreted
as relationship between samples. If we wanted to plot the relationship
between genes we would plot the columns of \(W\) matrix.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(NMF)}
\NormalTok{res=}\KeywordTok{nmf}\NormalTok{(mat,}\DataTypeTok{rank=}\DecValTok{3}\NormalTok{,}\DataTypeTok{seed=}\DecValTok{123}\NormalTok{) }\CommentTok{# nmf with 3 components/factors}
\NormalTok{w <-}\StringTok{ }\KeywordTok{basis}\NormalTok{(res) }\CommentTok{# get W}
\NormalTok{h <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(res)  }\CommentTok{# get H}

\CommentTok{# plot 1st factor against 3rd factor}
\KeywordTok{plot}\NormalTok{(h[}\DecValTok{1}\NormalTok{,],h[}\DecValTok{3}\NormalTok{,],}\DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/nmfCode-1} \end{center}

We should add the note that due to random starting points of the
optimization algorithm, NMF is usually run multiple times and a
consensus clustering approach is used when clustering samples. This
simply means that samples are clustered together if they cluster
together in multiple runs of the NMF. The NMF package we used above has
built-in ways to achieve this. In addition, NMF is a family of
algorithms the choice of cost function to optimize the difference
between \(X\) and \(WH\) and the methods used for optimization creates
multiple variants of NMF. The ``method'' parameter in the above
\texttt{nmf()} function controls the which algorithm for NMF.

\hypertarget{chosing-the-number-of-components-and-ranking-components-in-importance}{%
\subsubsection{chosing the number of components and ranking components
in
importance}\label{chosing-the-number-of-components-and-ranking-components-in-importance}}

In both ICA and NMF, there is no well-defined way to rank components or
to select the number of components. There are couple of approaches that
might suit to both ICA and NMF for ranking components. One can use the
norms of columns/rows in mixing matrices. This could simply mean take
the sum of absolute values in mixing matrices. In our examples above,
For our ICA example above, ICA we would take the sum of the absolute
values of the rows of \(A\) since we transposed the input matrix \(X\)
before ICA. And for the NMF, we would use the columns of \(W\). These
ideas assume that the larger coefficients in the weight or mixing
matrices indicate more important components.

For selecting the optimal number of components, NMF package provides
different strategies. One way is to calculate RSS for each \(k\), number
of components, and take the \(k\) where the RSS curve starts to
stabilize.However, these strategies require that you run the algorithm
with multiple possible component numbers. \texttt{nmf} function will run
these automatically when the \texttt{rank} argument is a vector of
numbers. For ICA there is no straightforward way to choose the right
number of components, a common strategy is to start with as many
components as variables and try to rank them by their usefullness.

\BeginKnitrBlock{rmdtip}
\textbf{Want to know more ?}

NMF package vignette has extensive information on how to run NMF to get
stable resuts and getting an estimate of components
\url{https://cran.r-project.org/web/packages/NMF/vignettes/NMF-vignette.pdf}
\EndKnitrBlock{rmdtip}

\hypertarget{multi-dimensional-scaling}{%
\subsection{Multi-dimensional scaling}\label{multi-dimensional-scaling}}

MDS is a set of data analysis techniques that display the structure of
distance data in a high dimensional space into a lower dimensional space
without much loss of information. The overall goal of MDS is to
faithfully represent these distances with the lowest possible
dimensions. So called ``classical multi-dimensional scaling'' algorithm,
tries to minimize the following function:

\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}d_{ij}-\|z_{i}-z_{j}\|{\bigr )}^{2}}{\sum _{i,j}d_{ij}^{2}}}{\Biggr )}^{1/2}}\)

Here the function compares the new data points on lower dimension
\((z_{1},z_{2},...,z_{N})\) to the input distances between data points
or distance between samples in our gene expression example. It turns
out, this problem can be efficiently solved with SVD/PCA on the scaled
distance matrix, the projection on eigenvectors will be the most optimal
solution for the equation above. Therefore, classical MDS is sometimes
called Principal Coordinates Analysis in the litereuature. However,
later variants improve on classical MDS this by using this as a starting
point and optimize a slightly different cost function that again
measures how well the low-dimensional distances correspond to
high-dimensional distances. This variant is called non-metric MDS and
due to the nature of the cost function, it assumes a less stringent
relationship between the low-dimensional distances
\$\textbar{}z\_\{i\}-z\_\{j\}\textbar{} and input distances \(d_{ij}\).
Formally, this procedure tries to optimize the following function.

\({\displaystyle Stress_{D}(z_{1},z_{2},...,z_{N})={\Biggl (}{\frac {\sum _{i,j}{\bigl (}\|z_{i}-z_{j}\|-\theta(d_{ij}){\bigr )}^{2}}{\sum _{i,j}\|z_{i}-z_{j}\|^{2}}}{\Biggr )}^{1/2}}\)

The core of a non-metric MDS algorithm is a twofold optimization
process. First the optimal monotonic transformation of the distances has
to be found, this is shown in the above formula as \(\theta(d_{ij})\).
Secondly, the points on a low dimension configuration have to be
optimally arranged, so that their distances match the scaled distances
as closely as possible. This two steps are repeated until some
convergence criteria is reached. This usually means that the cost
function does not improve much after certain number of iterations. The
basic steps in a non-metric MDS algorithm are: 1. Find a random low
dimensional configuration of points, or in the variant we will be using
below we start with the configuration returned by classical MDS 2.
Calculate the distances between the points in the low dimension
\$\textbar{}z\_\{i\}-z\_\{j\}\textbar{}, \(z_{i}\) and \(z_{j}\) are
vector of positions for sample \(i\) and \(j\). 3. Find the optimal
monotonic transformation of the input distance,
\({\textstyle \theta(d_{ij})}\), to approximate input distances to
low-dimensional distances. This is achieved by isotonic regression,
where a monotonically increasing free-form function is fit. This step
practically ensures that ranking of low-dimensional distances are
similar to rankings of input distances. 4. Minimize the stress function
by re-configuring low-dimensional space and keeping \(\theta\) function
constant. 5. repeat from step 2 until convergence.

We will now demonstrate both classical MDS and Kruskal's isometric MDS.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mds=}\KeywordTok{cmdscale}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat)))}
\NormalTok{isomds=MASS}\OperatorTok{::}\KeywordTok{isoMDS}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## initial  value 15.907414 
## final  value 13.462986 
## converged
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the patients in the 2D space}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(mds,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType,}
     \DataTypeTok{main=}\StringTok{"classical MDS"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(isomds}\OperatorTok{$}\NormalTok{points,}\DataTypeTok{pch=}\DecValTok{19}\NormalTok{,}\DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType,}
     \DataTypeTok{main=}\StringTok{"isotonic MDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/mds2-1} \end{center}

In this example, there is not much difference between isotonic MDS and
classical MDS. However, there might be cases where different MDS methods
provides visiable changes in the scatter plots.

\hypertarget{t-distributed-stochastic-neighbor-embedding-t-sne}{%
\subsection{t-Distributed Stochastic Neighbor Embedding
(t-SNE)}\label{t-distributed-stochastic-neighbor-embedding-t-sne}}

t-SNE maps the distances in high-dimensional space to lower dimensions
and it is similar to MDS method in this respect. But the benefit of this
particular method is that it tries to preserve the local structure of
the data so the distances and grouping of the points we observe in a
lower dimensions such as a 2D scatter plot is as close as possible to
the distances we observe in the high-dimensional space. As with other
dimension reduction methods, you can choose how many lower dimensions
you need. The main difference of t-SNE is that it tries to preserve the
local structure of the data. This kind of local structure embedding is
missing in the MDS algorithm which also has a similar goal. MDS tries to
optimize the distances as a whole, whereas t-SNE optimizes the distances
with the local structure in mind. This is defined by the ``perplexity''
parameter in the arguments. This parameter controls how much the local
structure influences the distance calculation. The lower the value the
more the local structure is take into account. Similar to MDS, the
process is an optimization algorithm. Here, we also try to minimize the
divergence between observed distances and lower dimension distances.
However, in the case of t-SNE, the observed distances and lower
dimensional distances are transformed using a probabilistic framework
with their local variance in mind.

From here on, we will provide a bit more detail on how the algorithm
works in case conceptual description above is too shallow. In t-SNE the
euclidiean distances between data points are transformed into a
conditional similarity between points. This is done by assuming a normal
distribution on each data point with a variance calculated ultimately by
the use of ``perplexity'' parameter. The perplexity paramater is, in a
sense, a guess about the number of the closest neighbors each point has.
Setting it to higher values gives more weight to global structure. Given
\(d_{ij}\) is the euclidean distance between point \(i\) and \(j\), the
similarity score \(p_{ij}\) is calculated as shown below.

\(p_{j | i} = \frac{\exp(-\|d_{ij}\|^2 / 2 _i^2)}{_{k \neq i} \exp(-\|d_{ik}\|^2 / 2 _i^2)}\)

This distance is symmetrized by incorparating \$p\_\{i \textbar{} j\} as
shown below.

\(p_{i j}=\frac{p_{j|i} + p_{i|j}}{2n}\)

For the distances in the reduced dimension, we use t-distribution with
one degree of freedom. In the formula below, \(| y_i-y_j\|^2\) is
euclidean distance between points \(i\) and \(j\) in the reduced
dimensions.

\[
q_{i j} = \frac{(1+ \| y_i-y_j\|^2)^{-1}}{(_{k \neq l} 1+ \| y_k-y_l\|^2)^{-1} }
\]

As most of the algorithms we have seen in this section, t-SNE is an
optimization process in essence. In every iteration the points along
lower dimensions are re-arranged to minimize the formulated difference
between the the observed joint probabilities (\(p_{i j}\)) and
low-dimensional joint probabilities (\(q_{i j}\)). Here we are trying to
compare probability distributions. In this case, this is done using a
method called Kullback-Leibler divergence, or KL-divergence. In the
formula below, since the \(p_{i j}\) is pre-defined using original
distances, only way to optimize is to play with \(q_{i j}\)) because it
depends on the configuration of points in the lower dimensional space.
This configuration is optimized to minimize the KL-divergence between
\(p_{i j}\) and \(q_{i j}\).

\[
KL(P||Q) = \sum_{i, j} p_{ij} \, \log \frac{p_{ij}}{q_{ij}}.
\] Strictly speaking, KL-divergence measures how well the distribution
\(P\) which is observed using the original data points can be
aproximated by distribution \(Q\), which is modeled using points on the
lower dimension. If the distributions are identical KL-divergence would
be 0. Naturally, the more divergent the distributions are the higher the
KL-divergence will be.

We will now show how to use t-SNE on our gene expression data set. We
are setting the random seed because again t-SNE optimization algorithm
have random starting points and this might create non-identical results
in every run. After calculating the t-SNE lower dimension embeddings we
will plot the points in a 2D scatter plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"Rtsne"}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{) }\CommentTok{# Set a seed if you want reproducible results}
\NormalTok{tsne_out <-}\StringTok{ }\KeywordTok{Rtsne}\NormalTok{(}\KeywordTok{t}\NormalTok{(mat),}\DataTypeTok{perplexity =} \DecValTok{10}\NormalTok{) }\CommentTok{# Run TSNE}
 \KeywordTok{image}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{dist}\NormalTok{(tsne_out}\OperatorTok{$}\NormalTok{Y))))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/tsne-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Show the objects in the 2D tsne representation}
\KeywordTok{plot}\NormalTok{(tsne_out}\OperatorTok{$}\NormalTok{Y,}\DataTypeTok{col=}\NormalTok{annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType,}
     \DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}

\CommentTok{# create the legend for the Leukemia types}
\KeywordTok{legend}\NormalTok{(}\StringTok{"bottomleft"}\NormalTok{,}
       \DataTypeTok{legend=}\KeywordTok{unique}\NormalTok{(annotation_col}\OperatorTok{$}\NormalTok{LeukemiaType),}
       \DataTypeTok{fill =}\KeywordTok{palette}\NormalTok{(}\StringTok{"default"}\NormalTok{),}
       \DataTypeTok{border=}\OtherTok{NA}\NormalTok{,}\DataTypeTok{box.col=}\OtherTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/tsne-2} \end{center}

As you might have noticed, we set again a random seed with
\texttt{set.seed()} function. The optimization algorithm starts with
random configuration of points in the lower dimension space, and each
iteration it tries to improve on the previous lower dimension
confugration, that is why starting points can result in different final
outcomes.

\BeginKnitrBlock{rmdtip}
\textbf{Want to know more ?}

\begin{itemize}
\tightlist
\item
  How perplexity effects t-sne, interactive examples
  \url{https://distill.pub/2016/misread-tsne/}
\item
  more on perplexity:
  \url{https://blog.paperspace.com/dimension-reduction-with-t-sne/}
\item
  Intro to t-SNE
  \url{https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm}
\end{itemize}
\EndKnitrBlock{rmdtip}

\#\#Exercises

\hypertarget{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1}{%
\subsection{How to summarize collection of data points: The idea behind
statistical
distributions}\label{how-to-summarize-collection-of-data-points-the-idea-behind-statistical-distributions-1}}

\hypertarget{section}{%
\subsubsection{}\label{section}}

Calculate the means and variances of the rows of the following simulated
data set, plot the distributions of means and variances using
\texttt{hist()} and \texttt{boxplot()} functions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{#sample data matrix from normal distribution}
\NormalTok{gset=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{600}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{200}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{70}\NormalTok{)}
\NormalTok{data=}\KeywordTok{matrix}\NormalTok{(gset,}\DataTypeTok{ncol=}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{section-1}{%
\subsubsection{}\label{section-1}}

Using the data generated above, calculate the standard deviation of the
distribution of the means using \texttt{sd()} function. Compare that to
the expected standard error obtained from central limit theorem keeping
in mind the population parameters were \(\sigma=70\) and \(n=6\). How
does the estimate from the random samples change if we simulate more
data with \texttt{data=matrix(rnorm(6000,mean=200,sd=70),ncol=6)}

\hypertarget{section-2}{%
\subsubsection{}\label{section-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\tightlist
\item
  simulate 30 random variables using \texttt{rpois()} function, do this
  1000 times and calculate means of sample. Plot the sampling
  distributions of the means using a histogram. Get the 2.5th and 97.5th
  percentiles of the distribution.
\item
  Use \texttt{t.test} function to calculate confidence intervals of the
  first random sample \texttt{pois1} simulated from\texttt{rpois()}
  function below.
\item
  Use bootstrap confidence interval for the mean on \texttt{pois1}
\item
  compare all the estimates
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{#sample 30 values from poisson dist with lamda paramater =30}
\NormalTok{pois1=}\KeywordTok{rpois}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{lambda=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{section-3}{%
\subsubsection{}\label{section-3}}

Optional exercise: Try to recreate the following figure, which
demonstrates the CLT concept.

\begin{center}\includegraphics{compgenomrReloaded_files/figure-latex/unnamed-chunk-162-1} \end{center}

\hypertarget{how-to-test-for-differences-in-samples}{%
\subsection{How to test for differences in
samples}\label{how-to-test-for-differences-in-samples}}

\hypertarget{section-4}{%
\subsubsection{}\label{section-4}}

Test the difference of means of the following simulated genes using the
randomization, t-test and \texttt{wilcox.test()} functions. Plot the
distributions using histograms and boxplots.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{3}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{3}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{section-5}{%
\subsubsection{}\label{section-5}}

Test the difference of means of the following simulated genes using the
randomization, t-test and \texttt{wilcox.test()} functions. Plot the
distributions using histograms and boxplots.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{gene1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{4}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\NormalTok{gene2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DataTypeTok{mean=}\DecValTok{2}\NormalTok{,}\DataTypeTok{sd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{section-6}{%
\subsubsection{}\label{section-6}}

read the gene expression data set with
\texttt{data=readRDS("StatisticsForGenomics/geneExpMat.rds")}. The data
has 100 differentially expressed genes.First 3 columns are the test
samples, and the last 3 are the control samples. Do a t-test for each
gene (each row is a gene), record the p-values. Then, do a moderated
t-test, as shown in the lecture notes and record the p-values. Do a
p-value histogram and compare two approaches in terms of the number of
significant tests with 0.05 threshold. On the p-values use FDR (BH),
bonferroni and q-value adjustment methods. Calculate how many adjusted
p-values are below 0.05 for each approach.

\hypertarget{relationship-between-variables-linear-models-and-correlation-1}{%
\subsection{Relationship between variables: linear models and
correlation}\label{relationship-between-variables-linear-models-and-correlation-1}}

\hypertarget{section-7}{%
\subsubsection{}\label{section-7}}

Below we are going to simulate X and Y values.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run the code then fit a line to predict Y based on X.
\item
  Plot the scatter plot and the fitted line.
\item
  Calculate correlation and R\^{}2.
\item
  Run the \texttt{summary()} function and try to extract P-values for
  the model from the object returned by \texttt{summary}. see
  \texttt{?summary.lm}
\item
  Plot the residuals vs fitted values plot, by calling \texttt{plot}
  function with \texttt{which=1} as the second argument. First argument
  is the model returned by \texttt{lm}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random number seed, so that the random numbers from the text}
\CommentTok{# is the same when you run the code.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{32}\NormalTok{)}

\CommentTok{# get 50 X values between 1 and 100}
\NormalTok{x =}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{)}

\CommentTok{# set b0,b1 and varience (sigma)}
\NormalTok{b0 =}\StringTok{ }\DecValTok{10}
\NormalTok{b1 =}\StringTok{ }\DecValTok{2}
\NormalTok{sigma =}\StringTok{ }\DecValTok{20}
\CommentTok{# simulate error terms from normal distribution}
\NormalTok{eps =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{0}\NormalTok{,sigma)}
\CommentTok{# get y values from the linear equation and addition of error terms}
\NormalTok{y =}\StringTok{ }\NormalTok{b0 }\OperatorTok{+}\StringTok{ }\NormalTok{b1}\OperatorTok{*}\NormalTok{x}\OperatorTok{+}\StringTok{ }\NormalTok{eps}
\end{Highlighting}
\end{Shaded}

\hypertarget{section-8}{%
\subsubsection{}\label{section-8}}

Read the data set histone modification data set with using a variation
of:
\texttt{df=readRDS("StatisticsForGenomics\_data/HistoneModeVSgeneExp.rds")}.
There are 3 columns in the data set these are measured levels of
H3K4me3, H3K27me3 and gene expression per gene.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  plot the scatter plot for H3K4me3 vs expression
\item
  plot the scatter plot for H3K27me3 vs expression
\item
  fit the model model for prediction of expression data using:

  \begin{itemize}
  \tightlist
  \item
    only H3K4me3 as explanatory variable
  \item
    only H3K27me3 as explanatory variable
  \item
    using both H3K4me3 and H3K27me3 as explanatory variables
  \end{itemize}
\item
  inspect summary() function output in each case, which terms are
  significant
\item
  Is using H3K4me3 and H3K27me3 better than the model with only H3K4me3.
\item
  Plot H3k4me3 vs H3k27me3. Inspect the points that does not follow a
  linear trend. Are they clustered at certain segments of the plot.
  Bonus: Is there any biological or technical interpretation for those
  points ?
\end{enumerate}

\hypertarget{genomicIntervals}{%
\chapter{Operations on Genomic Intervals and Genome
Arithmetic}\label{genomicIntervals}}

A considerable time in computational genomics is spent on overlapping
different features of the genome. Each feature can be represented with a
genomic interval

\bibliography{book.bib}

\backmatter
\printindex

\end{document}
